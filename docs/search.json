[
  {
    "objectID": "slides/survey_example.html#an-example-survey",
    "href": "slides/survey_example.html#an-example-survey",
    "title": "An example survey",
    "section": "An example survey",
    "text": "An example survey\n\n\\[\n\\newcommand{\\mc}[1]{\\mathcal{#1}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\E}{\\mathbb{E}}\n\\renewcommand{\\P}{\\mathbb{P}}\n\\newcommand{\\var}{{\\rm Var}} % Variance\n\\newcommand{\\mse}{{\\rm MSE}} % MSE\n\\newcommand{\\bias}{{\\rm Bias}} % MSE\n\\newcommand{\\cov}{{\\rm Cov}} % Covariance\n\\newcommand{\\iid}{\\stackrel{\\rm iid}{\\sim}}\n\\newcommand{\\ind}{\\stackrel{\\rm ind}{\\sim}}\n\\renewcommand{\\choose}[2]{\\binom{#1}{#2}}  % Choose\n\\newcommand{\\chooses}[2]{{}_{#1}C_{#2}}  % Small choose\n\\newcommand{\\cd}{\\stackrel{d}{\\rightarrow}}\n\\newcommand{\\cas}{\\stackrel{a.s.}{\\rightarrow}}\n\\newcommand{\\cp}{\\stackrel{p}{\\rightarrow}}\n\\newcommand{\\bin}{{\\rm Bin}}\n\\newcommand{\\ber}{{\\rm Ber}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\]\n\n\nThere are 6 surveys and each survey has two questions:\n\nprefer A or B, or neither?\nprefer A’ or B, or neither?\n\nNotations:\n\n\\(\\hat{p}_1\\): proportion of people who prefer A over B.\n\\(\\hat{p}_2\\): proportion of people who prefer B over A.\n\\(\\hat{q}_1\\): proportion of people who prefer A’ over B.\n\\(\\hat{q}_2\\): proportion of people who prefer B over A’.\n\\(n\\) is the sample size in each survey"
  },
  {
    "objectID": "slides/survey_example.html#survey-results",
    "href": "slides/survey_example.html#survey-results",
    "title": "An example survey",
    "section": "Survey Results",
    "text": "Survey Results\n\n\n\n\n\n\nstudy\np_1\np_2\nq_1\nq_2\nn\n\n\n\n\n1\n0.4830\n0.3920\n0.4610\n0.4160\n2046\n\n\n2\n0.4100\n0.3500\n0.4200\n0.3600\n1149\n\n\n3\n0.4660\n0.3310\n0.4650\n0.3490\n1112\n\n\n4\n0.4601\n0.3222\n0.4082\n0.3586\n1112\n\n\n5\n0.4400\n0.3200\n0.3970\n0.3300\n1082\n\n\n6\n0.3880\n0.2930\n0.3820\n0.3060\n1484\n\n\n\n\n\n\n\n\n\nTwo possible statistics:\n\ndifference in proportion: \\(T_1 = \\hat{p}_1 - \\hat{q}_1\\).\ndifference in difference: \\(T_2 = (\\hat{p}_1 - \\hat{p}_2) - (\\hat{q}_1 - \\hat{q}_2)\\)"
  },
  {
    "objectID": "slides/survey_example.html#assumptions",
    "href": "slides/survey_example.html#assumptions",
    "title": "An example survey",
    "section": "Assumptions",
    "text": "Assumptions\n\nThe surveys are independent.\nThe responses to the two questions in each survey are also independent.\nThe surveys are conducted based on simple random samples."
  },
  {
    "objectID": "slides/survey_example.html#parametric-bootstrap",
    "href": "slides/survey_example.html#parametric-bootstrap",
    "title": "An example survey",
    "section": "Parametric Bootstrap",
    "text": "Parametric Bootstrap\n\nB &lt;- 10^4\nT1_boot &lt;- matrix(0, ncol = B, nrow = 6)\nT2_boot &lt;- matrix(0, ncol = B, nrow = 6)\n\nset.seed(2023)\n\nfor (i in 1:B) {\n  for (j in 1:6) {\n    prop_boot_p &lt;- rmultinom(1, n[j], \n        prob = c(p_1[j], p_2[j], 1-p_1[j]-p_2[j]))/n[j]\n    prop_boot_q &lt;- rmultinom(1, n[j], \n        prob = c(q_1[j], q_2[j], 1-q_1[j]-q_2[j]))/n[j]\n    T1_boot[j, i] &lt;- prop_boot_p[1] - prop_boot_q[1]\n    T2_boot[j, i] &lt;- (prop_boot_p[1] - prop_boot_p[2]) - \n        (prop_boot_q[1] - prop_boot_q[2])\n  }\n}"
  },
  {
    "objectID": "slides/survey_example.html#difference-in-proportions",
    "href": "slides/survey_example.html#difference-in-proportions",
    "title": "An example survey",
    "section": "Difference in Proportions",
    "text": "Difference in Proportions\n\nsurvey_dp_boot &lt;- tibble(study = 1:6, \n                      DiP = 100 * rowMeans(T1_boot),\n                      se = 100 * apply(T1_boot, 1, sd),\n                      CI.lower = 100 * apply(T1_boot, 1, quantile, \n                                       probs = 0.025),\n                      CI.upper = 100 * apply(T1_boot, 1, quantile, \n                                       probs = 0.975))\nsurvey_dp_boot |&gt;  kable(digits = 2)\n\n\n\n\nstudy\nDiP\nse\nCI.lower\nCI.upper\n\n\n\n\n1\n2.18\n1.54\n-0.83\n5.23\n\n\n2\n-0.97\n2.05\n-4.96\n3.05\n\n\n3\n0.11\n2.12\n-4.05\n4.23\n\n\n4\n5.14\n2.09\n0.99\n9.26\n\n\n5\n4.30\n2.14\n0.09\n8.50\n\n\n6\n0.63\n1.79\n-2.90\n4.11"
  },
  {
    "objectID": "slides/survey_example.html#difference-in-differences",
    "href": "slides/survey_example.html#difference-in-differences",
    "title": "An example survey",
    "section": "Difference in Differences",
    "text": "Difference in Differences\n\nsurvey_did_boot &lt;- tibble(study = 1:6, \n                      DiD = 100 * rowMeans(T2_boot),\n                      se = 100 * apply(T2_boot, 1, sd),\n                      CI.lower = 100 * apply(T2_boot, 1, quantile, \n                                       probs = 0.025),\n                      CI.upper = 100 * apply(T2_boot, 1, quantile, \n                                       probs = 0.975))\nsurvey_did_boot |&gt;  kable(digits = 2)\n\n\n\n\nstudy\nDiD\nse\nCI.lower\nCI.upper\n\n\n\n\n1\n4.57\n2.88\n-1.03\n10.22\n\n\n2\n0.05\n3.63\n-6.96\n7.14\n\n\n3\n1.89\n3.78\n-5.58\n9.35\n\n\n4\n8.74\n3.69\n1.44\n15.92\n\n\n5\n5.29\n3.70\n-1.85\n12.66\n\n\n6\n1.95\n3.03\n-4.04\n7.88"
  },
  {
    "objectID": "slides/survey_example.html#joint-distribution-of-the-two-questions",
    "href": "slides/survey_example.html#joint-distribution-of-the-two-questions",
    "title": "An example survey",
    "section": "Joint distribution of the two questions",
    "text": "Joint distribution of the two questions\n\nApparently, the responses to the two question are correlated.\nHowever we only the marginal distributions for each question.\nWe need to model the joint distribution directly.\nLet Q1 be the first question (A vs B) and Q2 be the second question (A’ vs B).\nSuppose the joint distribution is\n\n\n\nQ1\\Q2\nA’\nB\nno response\n\n\n\n\nA\n\\(\\rho_1\\)\n\\(\\rho_{12}\\)\n\\(\\gamma\\)\n\n\nB\n\\(\\rho_{21}\\)\n\\(\\rho_2\\)\n\\(\\gamma\\)\n\n\nno response\n\\(\\gamma\\)\n\\(\\gamma\\)\n\\(\\rho_3\\)\n\n\n\nThe parameter is \\(\\theta = (\\rho_1, \\rho_2, \\rho_3, \\rho_{12}, \\rho_{21}, \\gamma)\\), where \\(\\rho_1 + \\rho_2 + \\rho_3 + \\rho_{12} + \\rho_{21} + 4\\gamma = 1\\).\nDenote the counts by \\(n_{ij}\\), where \\(i,j \\in \\{1,2,3\\} = \\{A, B, \\text{no response}\\}\\)."
  },
  {
    "objectID": "slides/survey_example.html#interpretation-of-the-parameters",
    "href": "slides/survey_example.html#interpretation-of-the-parameters",
    "title": "An example survey",
    "section": "Interpretation of the parameters",
    "text": "Interpretation of the parameters\n\n\\(\\rho_1\\): the proportion of people that always prefer A type.\n\\(\\rho_2\\): the proportion of people that always prefer B type.\n\\(\\rho_3\\): the proportion of people that always give no response.\n\\(\\rho_{12}\\): the proportion of people that prefer \\(A &gt; B &gt; A'\\).\n\\(\\rho_{21}\\): the proportion of people that prefer \\(A' &gt; B &gt; A\\).\n\\(\\gamma\\): the proportion of people that answer only one of the two questions."
  },
  {
    "objectID": "slides/survey_example.html#missing-data",
    "href": "slides/survey_example.html#missing-data",
    "title": "An example survey",
    "section": "Missing Data",
    "text": "Missing Data\n\nWe have observed \\(Y = [n_{1\\cdot}, n_{2\\cdot}, n_{3\\cdot},n_{\\cdot 1}, n_{\\cdot 2}]^T\\), where \\(n_{i\\cdot} = \\sum_j n_{ij}\\) and \\(n_{\\cdot j} = \\sum_i n_{ij}\\).\nLet \\(U = [n_{11}, n_{12}, n_{13}, n_{21}, n_{22}, n_{23}, n_{31}, n_{32}, n_{33}]^T\\) be the complete data.\nIf we also observe \\(Z = [n_{11}, n_{12}, n_{21}, n_{32}]^T\\), then \\(U\\) can be recovered from \\(Y\\) and \\(Z\\).\n\\(Z\\) is called the missing data.\nLet \\(M\\) be a matrix such that \\[\nU = M\\left[\\begin{array}{c}\nY \\\\\nZ\n\\end{array}\\right].\n\\]\nThe distribution of \\(U\\) multinomial with parameter \\((\\rho_1, \\rho_{12}, \\gamma, \\rho_{21}, \\rho_2, \\gamma, \\gamma, \\gamma, \\rho_3)\\)."
  },
  {
    "objectID": "slides/survey_example.html#intuition",
    "href": "slides/survey_example.html#intuition",
    "title": "An example survey",
    "section": "Intuition",
    "text": "Intuition\n\nIf we observe \\(U\\), we can find the posterior \\(\\pi(\\theta \\mid U)\\).\nObserving \\(U\\) is equivalent to observing \\(Y\\) and \\(Z\\) and hence the posterior can be written as \\(\\pi(\\theta \\mid Y, Z)\\).\nHowever, \\(Z\\) is missing, so we can not condition on \\(Z\\).\nThe most we can do is \\(\\pi(\\theta, Z \\mid Y)\\).\nBy Bayes Theorem, \\[\n\\pi(\\theta, Z \\mid Y) = \\frac{p(Y, Z \\mid \\theta)\\pi(\\theta)}{p(Y)}.\n\\]\nThe distribution \\(p(Y, Z \\mid \\theta)\\) is available since \\([Y^T, Z^T]^T = M^{-1}U\\) and \\(U\\) is multinomial.\nWe can also consider the marginal posterior \\(\\pi(\\theta \\mid Y) = \\sum_{Z} \\pi(\\theta, Z \\mid Y)\\)."
  },
  {
    "objectID": "slides/survey_example.html#data-survey-1",
    "href": "slides/survey_example.html#data-survey-1",
    "title": "An example survey",
    "section": "Data (Survey 1)",
    "text": "Data (Survey 1)\n\n\n\n\n\n\n\n\n\n\nQ1\\Q2\nA’\nB\nNo response\nTotal\n\n\n\n\nA\n\\(\\textcolor{red}{437}\\)\n\\(\\textcolor{red}{409}\\)\n\\(\\textcolor{red}{120}\\)\n988\n\n\nB\n\\(\\textcolor{red}{358}\\)\n\\(\\textcolor{red}{327}\\)\n\\(\\textcolor{red}{99}\\)\n802\n\n\nNo response\n\\(\\textcolor{red}{127}\\)\n\\(\\textcolor{red}{96}\\)\n\\(\\textcolor{red}{27}\\)\n256\n\n\nTotal\n943\n851\n\\(\\textcolor{blue}{252}\\)\n\\(\\textcolor{blue}{2046}\\)\n\n\n\n\n\nThe numbers in black are observed.\nThe numbers in red are my random guesses.\nThe numbers in blue are redundant as they can be computed from the others."
  },
  {
    "objectID": "slides/survey_example.html#sampling-algorithm",
    "href": "slides/survey_example.html#sampling-algorithm",
    "title": "An example survey",
    "section": "Sampling Algorithm",
    "text": "Sampling Algorithm\n\nWe need to sample from \\(\\pi(\\theta, Z \\mid Y)\\).\nThe sampling is achieved by Gibbs sampler:\n\nsample \\(Z\\) from \\(\\pi(Z \\mid \\theta, Y)\\)\nsample \\(\\theta\\) from \\(\\pi(\\theta \\mid Z, Y)\\).\n\nIn our case, \\(\\pi(Z \\mid \\theta, Y)\\) is independent of \\(\\theta\\) and the full conditionals \\(\\pi(z_i \\mid z_{-i}, Y)\\) are hypergeometric distributions.\nYou fill find that the derivation is the same as Fisher’s exact test.\nThe distribution \\(\\pi(\\theta \\mid Z, Y)\\) is exactly \\(\\pi(\\theta \\mid U)\\), i.e., the posterior given the complete data."
  },
  {
    "objectID": "slides/survey_example.html#prior-and-posterior",
    "href": "slides/survey_example.html#prior-and-posterior",
    "title": "An example survey",
    "section": "Prior and posterior",
    "text": "Prior and posterior\n\nWe need to specify a prior for \\(\\theta = (\\rho_1, \\rho_2, \\rho_3, \\rho_{12}, \\rho_{21}, 4\\gamma)\\).\nFor convenience, we choose \\((\\rho_1, \\rho_2, \\rho_3, \\rho_{12}, \\rho_{21}, 4\\gamma) \\sim \\text{Dir}(\\alpha_1, \\ldots, \\alpha_6)\\).\nThe posterior given the complete data is \\[\\begin{multline*}\n(\\rho_1, \\rho_2, \\rho_3, \\rho_{12}, \\rho_{21}, 4\\gamma) \\mid U \\sim \\text{Dir}(\\alpha_1 + n_{11}, \\alpha_2 + n_{22}, \\alpha_3 + n_{33}, \\alpha_4 + n_{12},\\\\\n\\alpha_5 + n_{21}, \\alpha_6 + n_{13} + n_{23} + n_{31} + n_{32}).\n\\end{multline*}\\]"
  },
  {
    "objectID": "slides/survey_example.html#analysis-for-survey-1",
    "href": "slides/survey_example.html#analysis-for-survey-1",
    "title": "An example survey",
    "section": "Analysis for Survey 1",
    "text": "Analysis for Survey 1\n\nsource(\"../dataset/survey/survey_Missing.R\")\n\nY &lt;- c(988, 802, 256, 943, 851)\ntheta_sample &lt;- single_survey_sampler(3000, Y)\n\n\n\n\n\n\n\nQ_2.A'\nQ_2.B\nQ_2.No response\ntotal\n\n\n\n\nQ_1.A\n22.23\n20.07\n5.43\n47.73\n\n\nQ_1.B\n18.09\n16.30\n5.43\n39.82\n\n\nQ_1.No response\n5.43\n5.43\n1.59\n12.45\n\n\ntotal\n45.75\n41.80\n12.45\n100.00"
  },
  {
    "objectID": "slides/survey_example.html#analysis-for-survey-2",
    "href": "slides/survey_example.html#analysis-for-survey-2",
    "title": "An example survey",
    "section": "Analysis for Survey 2",
    "text": "Analysis for Survey 2\n\nY &lt;- c(471, 402, 276, 483, 414)\ntheta_sample &lt;- single_survey_sampler(3000, Y)\n\n\n\n\n\n\n\nQ_2.A'\nQ_2.B\nQ_2.No response\ntotal\n\n\n\n\nQ_1.A\n17.19\n14.81\n8.83\n40.83\n\n\nQ_1.B\n14.75\n12.58\n8.83\n36.17\n\n\nQ_1.No response\n8.83\n8.83\n5.33\n23.00\n\n\ntotal\n40.77\n36.23\n23.00\n100.00"
  },
  {
    "objectID": "slides/survey_example.html#meta-analysis",
    "href": "slides/survey_example.html#meta-analysis",
    "title": "An example survey",
    "section": "Meta analysis",
    "text": "Meta analysis\n\nWe can also perform meta analysis by combining the 6 surveys.\nLet \\(U_i\\) be the complete data for survey \\(i\\).\nA simple hierarchical model is \\[\\begin{align*}\nU_i \\mid \\theta_i &\\ind \\text{Multinomial}(\\theta_i),\\\\\n\\theta_i \\mid \\alpha, \\nu &\\iid \\text{Dir}(\\alpha\\nu),\\\\\n\\nu &\\sim \\text{Dir}(1, 1, 1, 1, 1, 1),\\\\\n\\alpha &\\sim \\text{Exp}(0.001)\n\\end{align*}\\]\nThe complete data \\(U_i\\) is obtained by generating the missing data \\(Z_i\\) from \\(\\pi(Z_i \\mid Y_i)\\) and then combining with the observed data \\(Y_i\\).\nI wrote a function multi_survey_sampler to perform the meta analysis."
  },
  {
    "objectID": "slides/survey_example.html#what-is-the-question",
    "href": "slides/survey_example.html#what-is-the-question",
    "title": "An example survey",
    "section": "What is the question?",
    "text": "What is the question?\n\nSuppose you have obtained posterior samples of \\(\\theta_i = (\\rho_{1,i}, \\rho_{2,i}, \\rho_{3,i}, \\rho_{12,i}, \\rho_{21,i}, \\gamma_i)\\) for \\(i = 1, \\ldots, 6\\).\nWhat do you want to know?\nProbably, you want to compare \\(\\rho_{12,i}\\) and \\(\\rho_{21,i}\\) for each \\(i = 1, \\ldots, 6\\).\nYou have several choices:\n\nuse \\(\\eta_{1, i} = \\rho_{12,i} - \\rho_{21,i} \\in [-1, 1]\\);\nuse \\(\\eta_{2, i} = \\log\\frac{\\rho_{12,i}}{\\rho_{21,i}} \\in (-\\infty, \\infty)\\);\n\nSince we are using the joint distribution, it is okay to subtract probabilities (although it is not recommended).\n\nYou cannot arbitrarily subtract conditional probabilities or marginal probabilities.\nFor example, it is meaningless to compute \\[\\begin{align*}\n& \\P(X = 1) - \\P(Y = 0) \\quad \\text{or}\\\\\n& \\P(X = 1 \\mid Y = 1) - \\P(X = 1 \\mid Y = 0).\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/survey_example.html#analysis",
    "href": "slides/survey_example.html#analysis",
    "title": "An example survey",
    "section": "Analysis",
    "text": "Analysis\n\nsource(\"../dataset/survey/survey_Missing.R\")\nsurvey_data &lt;- read.csv(\"../dataset/survey/survey_data.csv\",header = TRUE)\nattach(survey_data)\nN1 &lt;- cbind(round(n*p_1), round(n*p_2))\nN1 &lt;- cbind(N1, n - N1[,1] - N1[,2])\nN2 &lt;- cbind(round(n*q_1), round(n*q_2))\nN2 &lt;- cbind(N2, n - N2[,1] - N2[,2])\ndetach(survey_data)\nY &lt;- cbind(N1, N2[,1:2])\nY |&gt; `colnames&lt;-`(c(\"n_1.\", \"n_2.\", \"n_3.\", \"n_.1\", \"n_.2\")) |&gt; kable()\n\n\n\n\nn_1.\nn_2.\nn_3.\nn_.1\nn_.2\n\n\n\n\n988\n802\n256\n943\n851\n\n\n471\n402\n276\n483\n414\n\n\n518\n368\n226\n517\n388\n\n\n512\n358\n242\n454\n399\n\n\n476\n346\n260\n430\n357\n\n\n576\n435\n473\n567\n454"
  },
  {
    "objectID": "slides/survey_example.html#analysis-1",
    "href": "slides/survey_example.html#analysis-1",
    "title": "An example survey",
    "section": "Analysis",
    "text": "Analysis\n\nmodel &lt;- stan_model(\"../dataset/survey/multi_survey.stan\")\nsamples &lt;- multi_survey_sampler(Y, model)\n\n\neta1 &lt;- samples$theta[,4,] - samples$theta[,5,]\neta2 &lt;- log(samples$theta[,4,]) - log(samples$theta[,5,])\neta1_overall &lt;- samples$nu[,4] - samples$nu[,5]\neta2_overall &lt;- log(samples$nu[,4]) - log(samples$nu[,5])"
  },
  {
    "objectID": "slides/survey_example.html#results",
    "href": "slides/survey_example.html#results",
    "title": "An example survey",
    "section": "Results",
    "text": "Results\n\n\n\\(\\eta_1 = \\rho_{12} - \\rho_{21} =\\) (prefer A &gt; B &gt; A’) - (prefer A’ &gt; B &gt; A)\n\\(\\eta_2 = \\log\\frac{\\rho_{12}}{\\rho_{21}} =\\) log(prefer A &gt; B &gt; A’) - log(prefer A’ &gt; B &gt; A)\nThe red dashed line is the overall estimate.\n\n\n\n\n\nHome"
  },
  {
    "objectID": "slides/11-bayes_np.html#non--semi--and-parametric-models",
    "href": "slides/11-bayes_np.html#non--semi--and-parametric-models",
    "title": "Lecture 11: Gaussian Process",
    "section": "Non-, Semi-, and Parametric Models",
    "text": "Non-, Semi-, and Parametric Models\n\n\\[\n\\newcommand{\\mc}[1]{\\mathcal{#1}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\E}{\\mathbb{E}}\n\\renewcommand{\\P}{\\mathbb{P}}\n\\newcommand{\\var}{{\\rm Var}} % Variance\n\\newcommand{\\mse}{{\\rm MSE}} % MSE\n\\newcommand{\\bias}{{\\rm Bias}} % MSE\n\\newcommand{\\cov}{{\\rm Cov}} % Covariance\n\\newcommand{\\iid}{\\stackrel{\\rm iid}{\\sim}}\n\\newcommand{\\ind}{\\stackrel{\\rm ind}{\\sim}}\n\\renewcommand{\\choose}[2]{\\binom{#1}{#2}}  % Choose\n\\newcommand{\\chooses}[2]{{}_{#1}C_{#2}}  % Small choose\n\\newcommand{\\cd}{\\stackrel{d}{\\rightarrow}}\n\\newcommand{\\cas}{\\stackrel{a.s.}{\\rightarrow}}\n\\newcommand{\\cp}{\\stackrel{p}{\\rightarrow}}\n\\newcommand{\\bin}{{\\rm Bin}}\n\\newcommand{\\ber}{{\\rm Ber}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\]\n\n\nWhat we have learned so far are called parametric models, i.e., models that are determined by a finite number of parameters.\nNonparametric models are models whose ``parameter’’ is infinite dimensional.\nTwo common types of nonparametric models:\n\nNonparametric regression: \\(Y = f(X) + \\epsilon\\), where \\(f\\) is an unknown function.\nNonparametric density estimation: \\(X_1, \\ldots, X_n \\iid F\\), where \\(F\\) is an unknown distribution (without assuming any parametric family).\n\nSemiparametric models are models that have both parametric and nonparametric components, e.g., \\(Y = X^T\\beta + f(Z) + \\epsilon\\)."
  },
  {
    "objectID": "slides/11-bayes_np.html#bayesian-nonparametrics",
    "href": "slides/11-bayes_np.html#bayesian-nonparametrics",
    "title": "Lecture 11: Gaussian Process",
    "section": "Bayesian Nonparametrics",
    "text": "Bayesian Nonparametrics\n\nFor a nonparametric model, a Bayesian treats the unknown function/distribution as the parameter.\nTherefore, we need prior distributions for these parameters.\nWe will introduce two “distributions” that are useful for nonparametric models:\n\nGaussian process (GP, the “distribution” of random functions)\nDirichlet process (DP, the “distribution” of random distributions)\n\nThat is,\n\n\\(Y = f(x) + \\epsilon\\), \\(f \\sim \\text{GP}\\), \\(\\epsilon \\sim N(0, \\sigma^2)\\).\n\\(X_1, \\ldots, X_n \\sim F\\), \\(F \\sim \\text{DP}\\)."
  },
  {
    "objectID": "slides/11-bayes_np.html#why-nonparametrics",
    "href": "slides/11-bayes_np.html#why-nonparametrics",
    "title": "Lecture 11: Gaussian Process",
    "section": "Why nonparametrics?",
    "text": "Why nonparametrics?"
  },
  {
    "objectID": "slides/11-bayes_np.html#parametric-nonlinear-model",
    "href": "slides/11-bayes_np.html#parametric-nonlinear-model",
    "title": "Lecture 11: Gaussian Process",
    "section": "Parametric nonlinear model",
    "text": "Parametric nonlinear model"
  },
  {
    "objectID": "slides/11-bayes_np.html#definition",
    "href": "slides/11-bayes_np.html#definition",
    "title": "Lecture 11: Gaussian Process",
    "section": "Definition",
    "text": "Definition\nA ``random function’’ \\(f\\) is said to follow a Gaussian process, denoted by \\[\nf \\sim \\mc{GP}(\\mu, K),\n\\] if for any \\(x_1, \\ldots, x_n\\), the random vector \\((f(x_1), \\ldots, f(x_n))\\) has a multivariate normal distribution, i.e., \\[\n\\left[\\begin{array}{c}\nf(x_1)\\\\\n\\vdots\\\\\nf(x_n)\n\\end{array}\\right] \\sim N\\left(\\left[\\begin{array}{c}\n\\mu(x_1)\\\\\n\\vdots\\\\\n\\mu(x_n),\n\\end{array}\\right], \\left[\\begin{array}{ccc}\nK(x_1, x_1) & \\cdots & K(x_1, x_n)\\\\\n\\vdots & \\ddots & \\vdots\\\\\nK(x_n, x_1) & \\cdots & K(x_n, x_n)\n\\end{array}\\right]\\right).\n\\]\n\nThe parameter \\(\\mu: \\R \\to \\R\\) is called the mean function.\nThe parameter \\(K: \\R \\times \\R \\to \\R\\) is called the covariance function/operator or kernel.\nThe kernel \\(K\\) needs to be symmetric and positive definite, i.e, for any \\(x_1, \\ldots, x_n \\in \\R\\), the matrix above is symmetric and positive definite."
  },
  {
    "objectID": "slides/11-bayes_np.html#commonly-used-kernels",
    "href": "slides/11-bayes_np.html#commonly-used-kernels",
    "title": "Lecture 11: Gaussian Process",
    "section": "Commonly used kernels",
    "text": "Commonly used kernels\n\nNotation: for \\(x, x^{\\prime} \\in \\R^n\\), \\(K(x, x^{\\prime})\\) is an \\(n \\times n\\) matrix whose \\((i,j)\\)th entry is \\(K(x_i, x^{\\prime}_j)\\).\nLinear kernel: \\(K(x, x^{\\prime}) = x^Tx^{\\prime}\\).\nPolynomial kernel: \\(K_{c,d}(x, x^{\\prime}) = (x^Tx^{\\prime} + c)^d\\).\nGaussian kernel: \\(K_{\\sigma}(x, x^{\\prime}) = \\exp\\left(-\\frac{\\|x-x^{\\prime}\\|^2}{2\\sigma^2}\\right)\\)."
  },
  {
    "objectID": "slides/11-bayes_np.html#realizations-of-gaussian-processes",
    "href": "slides/11-bayes_np.html#realizations-of-gaussian-processes",
    "title": "Lecture 11: Bayesian Nonparametrics",
    "section": "Realizations of Gaussian processes",
    "text": "Realizations of Gaussian processes\n\nlibrary(mvtnorm)\nGP_sim &lt;- function(from = 0, to = 1, mean_func = function(x){0},\n                   cov_func = function(x1, x2){exp(-16*(x1-x2)^2)}, \n                   m = 100){\n    x &lt;- seq(from, to, length.out = m)\n    mu &lt;- sapply(x, mean_func)\n    Sigma &lt;- outer(x, x, Vectorize(cov_func))\n    y &lt;- rmvnorm(1, mu, Sigma)\n    return(list(x = x, y = y))\n}"
  },
  {
    "objectID": "slides/11-bayes_np.html#gaussian-process-regression",
    "href": "slides/11-bayes_np.html#gaussian-process-regression",
    "title": "Lecture 11: Gaussian Process",
    "section": "Gaussian Process Regression",
    "text": "Gaussian Process Regression\n\nSuppose we observe \\(\\{x_i, y_i\\}_{i=1}^n\\), \\(x_i, y_i \\in \\R\\).\nLet \\(\\mathbf{y} = [y_1, \\ldots, y_n]^T\\) and \\(\\mathbf{x} = [x_1, \\ldots, x_n]^T\\).\nConsider the model: \\[\\begin{align*}\n\\mathbf{Y} \\mid f, \\mathbf{x}, \\sigma^2 &\\sim N(f(\\mathbf{x}), \\sigma^2I_n)\\\\\nf \\mid \\mu, K &\\sim \\mc{GP}(\\mu, K).\n\\end{align*}\\]\nThe GP prior is equivalent to \\(f(\\mathbf{x}) \\mid \\mathbf{x}, \\mu, K \\sim N(\\mu(\\mathbf{x}), K(\\mathbf{x}, \\mathbf{x}))\\).\nWe need to compute two distributions:\n\nThe posterior distribution of \\(f\\) given \\(\\mathbf{y}\\) and \\(\\mathbf{x}\\).\nThe posterior predictive distribution of \\(f(\\mathbf{x}^{\\prime})\\) given \\(\\mathbf{y}\\) and \\(\\mathbf{x}\\)."
  },
  {
    "objectID": "slides/11-bayes_np.html#posterior-predictive-distribution",
    "href": "slides/11-bayes_np.html#posterior-predictive-distribution",
    "title": "Lecture 11: Gaussian Process",
    "section": "Posterior Predictive Distribution",
    "text": "Posterior Predictive Distribution\n\nThe posterior distribution of \\(f\\) is \\[\nf \\mid\n\\]"
  },
  {
    "objectID": "slides/11-bayes_np.html#derivation",
    "href": "slides/11-bayes_np.html#derivation",
    "title": "Lecture 11: Gaussian Process",
    "section": "Derivation",
    "text": "Derivation\n\nFor any \\(m\\) and \\(\\mathbf{x}^{\\prime} \\in \\R^m\\), \\[\n\\left[\\begin{array}{c}\nf(\\mathbf{x})\\\\\nf(\\mathbf{x}^{\\prime})\n\\end{array}\n\\right] \\sim N\\left(\\left[\\begin{array}{c}\n\\mu(\\mathbf{x})\\\\\n\\mu(\\mathbf{x}^{\\prime})\n\\end{array}\\right], \\left[\\begin{array}{cc}\nK(\\mathbf{x}, \\mathbf{x}) & K(\\mathbf{x}, \\mathbf{x}^{\\prime})\\\\\nK(\\mathbf{x}^{\\prime}, \\mathbf{x}) & K(\\mathbf{x}^{\\prime}, \\mathbf{x}^{\\prime})\n\\end{array}\\right]\\right).\n\\]"
  },
  {
    "objectID": "slides/11-bayes_np.html#gaussian-process-regression-1",
    "href": "slides/11-bayes_np.html#gaussian-process-regression-1",
    "title": "Lecture 11: Gaussian Process",
    "section": "Gaussian Process Regression",
    "text": "Gaussian Process Regression"
  },
  {
    "objectID": "slides/11-bayes_np.html#posterior",
    "href": "slides/11-bayes_np.html#posterior",
    "title": "Lecture 11: Gaussian Process",
    "section": "Posterior",
    "text": "Posterior"
  },
  {
    "objectID": "slides/11-bayes_np.html#prediction",
    "href": "slides/11-bayes_np.html#prediction",
    "title": "Lecture 11: Gaussian Process",
    "section": "Prediction",
    "text": "Prediction"
  },
  {
    "objectID": "slides/11-bayes_np.html#kernel",
    "href": "slides/11-bayes_np.html#kernel",
    "title": "Lecture 11: Gaussian Process",
    "section": "Kernel",
    "text": "Kernel\n\n\nHome"
  },
  {
    "objectID": "slides/11-bayes_np.html#dirichlet-distribution",
    "href": "slides/11-bayes_np.html#dirichlet-distribution",
    "title": "Lecture 11: Gaussian Process",
    "section": "Dirichlet distribution",
    "text": "Dirichlet distribution\n\nBeta distribution \\(p \\sim \\text{Beta}(\\alpha, \\beta)\\):\n\\[\nf(p \\mid \\alpha, \\beta) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}p^{\\alpha-1}(1-p)^{\\beta-1}, \\quad p \\in [0,1].\n\\]\nAlternative parameterization: \\(\\alpha^{\\prime} = \\alpha + \\beta\\), \\(\\nu = \\frac{\\alpha}{\\alpha + \\beta} = \\E(p)\\).\nDirichlet distribution \\((p_1, p_2, \\ldots, p_k) \\sim \\text{Dir}(\\alpha_1, \\ldots, \\alpha_k)\\): \\[\nf(p_1, \\ldots, p_k \\mid \\alpha_1, \\ldots, \\alpha_k) =\n\\frac{\\Gamma(\\alpha_1 + \\cdots + \\alpha_k)}{\\Gamma(\\alpha_1)\\cdots\\Gamma(\\alpha_k)} p_1^{\\alpha_1-1}\\cdots p_k^{\\alpha_k-1}\n\\] where \\(p_i \\in [0,1]\\) and \\(\\sum_{i=1}^k p_i = 1\\).\nAlternative: \\(\\alpha = \\sum_{i=1}^k\\alpha_i\\) and \\(\\nu = \\left(\\alpha_1/\\alpha, \\ldots \\alpha_k/\\alpha\\right) = \\E[(p_1, \\ldots, p_k)]\\)."
  },
  {
    "objectID": "slides/11-bayes_np.html#definition-1",
    "href": "slides/11-bayes_np.html#definition-1",
    "title": "Lecture 11: Gaussian Process",
    "section": "Definition",
    "text": "Definition\nA ``random distribution’’ \\(F\\) is said to follow a Dirichlet process, denoted by \\[\nF \\sim \\mc{DP}(\\alpha, F_0),\n\\] if for any measurable partition \\(B_1, \\ldots, B_n\\) of the sample space of \\(F_0\\), the random vector \\((F(B_1), \\ldots, F(B_n))\\) has a Dirichlet distribution, i.e., \\[\n(F(B_1), \\ldots, F(B_n)) \\sim \\text{Dir}(\\alpha F_0(B_1), \\ldots, \\alpha F_0(B_n)).\n\\]\n\nThe parameter \\(\\alpha &gt; 0\\) is called the concentration parameter.\nThe parameter \\(F_0\\) is called the mean distribution."
  },
  {
    "objectID": "slides/11-bayes_np.html#example",
    "href": "slides/11-bayes_np.html#example",
    "title": "Lecture 11: Gaussian Process",
    "section": "Example",
    "text": "Example\n\nn &lt;- 10\nx &lt;- rcauchy(n)\nalpha &lt;- 10\ncurve(pnorm(x), -5, 5, lty = 2, lwd = 2, \n      ylab = TeX(\"$P(X \\\\leq x)$\"), xlab = \"x\")\nlines(ecdf(x), col = \"red\", lwd = 2)\n\nF_bar &lt;- function(m, alpha, n, F0, x){\n    u &lt;- runif(m)\n    out &lt;- rep(NA, m)\n    out[u &lt; alpha/(alpha + n)] &lt;- F0(sum(u &lt; alpha/(alpha + n)))\n    out[u &gt;= alpha/(alpha + n)] &lt;- sample(x, sum(u &gt;= alpha/(alpha + n)), replace = TRUE)\n    return(out)\n}"
  },
  {
    "objectID": "slides/09-regression.html#linear-regression",
    "href": "slides/09-regression.html#linear-regression",
    "title": "Lecture 09: Bayesian Regression",
    "section": "Linear regression",
    "text": "Linear regression\n\nSuppose we have samples \\(\\{x_i, y_i\\}_{i=1}^n\\) where \\(y_i \\in \\R\\) and \\(x_i \\in \\R^{p-1}\\). We want to model the relationship between \\(x_i\\) and \\(y_i\\).\nThe classical linear regression model is \\(Y = X^T\\beta\\).\nWe can fit the model by the method of ordinary least-square (OLS), i.e., minimizing \\(\\sum_{i=1}^n (y_i - x_i^T\\beta)^2\\).\nThe solution is \\(\\hat{\\beta} = (X^TX)^{-1}X^TY\\), where \\(Y = [y_1,\\ldots, y_n]^T\\) and \\[\nX = \\left[\\begin{array}{cc}\n1 & x_1^T \\\\\n\\vdots & \\vdots \\\\\n1 & x_n^T\n\\end{array}\n\\right]_{n\\times p}.\n\\]\nIf we further assume \\(Y = X^T\\beta + \\varepsilon\\), \\(\\varepsilon \\sim N(0, \\sigma^2)\\), then the OLS solution \\(\\hat{\\beta} = (X^TX)^{-1}X^TY\\) is the MLE of \\(\\beta\\)."
  },
  {
    "objectID": "slides/09-regression.html#normal-linear-model",
    "href": "slides/09-regression.html#normal-linear-model",
    "title": "Lecture 09: Bayesian Regression",
    "section": "Normal linear model",
    "text": "Normal linear model\n\nThe simple linear regression model is a normal linear model \\[\nY \\mid X, \\beta, \\sigma^2 \\sim N(X\\beta, \\sigma^2I_n)\n\\] where \\(I_n\\) is the \\(n \\times n\\) identity matrix, \\(X \\in \\R^{n \\times p}\\), and \\(\\beta \\in \\R^p\\).\nBoth \\(\\beta\\) and \\(\\sigma^2\\) are unknown parameters.\nIn practice, it is very likely that the normality assumption is violated. In such cases, we can apply some transformations to \\(Y\\), e.g., Box-Cox transformation, to make it normal.\nIn the next lecture, we will discuss generalized linear models (GLM), i.e., \\[\nY \\mid X, \\beta \\sim F_{\\eta}\n\\] where \\(F_\\eta\\) is an exponential family and \\(\\eta = X^T\\beta\\)."
  },
  {
    "objectID": "slides/09-regression.html#standard-noninformative-prior",
    "href": "slides/09-regression.html#standard-noninformative-prior",
    "title": "Lecture 09: Bayesian Regression",
    "section": "Standard noninformative prior",
    "text": "Standard noninformative prior\n\nA convenient noninformative prior is the uniform prior on \\((\\beta, \\log \\sigma)\\) or, equivalently, \\[\n\\pi\\left(\\beta, \\sigma^2 \\mid X\\right) \\propto \\sigma^{-2}\n\\]\nThis prior is useful when there are many samples and only a few parameters, i.e., \\(n \\gg p\\)."
  },
  {
    "objectID": "slides/09-regression.html#posterior-distributions",
    "href": "slides/09-regression.html#posterior-distributions",
    "title": "Lecture 09: Bayesian Regression",
    "section": "Posterior distributions",
    "text": "Posterior distributions\n\nSimilar to the normal model with unknown mean and variance, the joint posterior can be factorized: \\(\\pi(\\beta, \\sigma^2 \\mid X, Y) = \\pi(\\beta \\mid \\sigma^2, X, Y)\\pi(\\sigma^2 \\mid X, Y)\\)\nConditional posterior distribution of \\(\\beta\\), given \\(\\sigma\\): \\[\n\\beta \\mid \\sigma^2, y \\sim N\\left(\\hat{\\beta}, V_\\beta \\sigma^2\\right)\n\\] where \\(\\hat{\\beta} = (X^TX)^{-1}X^TY\\) and \\(V_\\beta = (X^TX)^{-1}\\).\nMarginal posterior distribution of \\(\\sigma^2\\): \\[\n\\sigma^2 \\mid y \\sim \\text{Inv}-\\chi^2\\left(n-p, s^2\\right)\n\\] where \\(s^2 = \\frac{1}{n-p}(Y - X\\hat{\\beta})^T(Y - X\\hat{\\beta})\\).\n\\(\\text{Inv}-\\chi^2(\\nu, s)\\) is the scaled inverse \\(\\chi^2\\) distribution.\nHence the posterior means of \\(\\beta\\) and \\(\\sigma^2\\) are the same as the OLS estimates."
  },
  {
    "objectID": "slides/09-regression.html#posterior-predictive-distribution",
    "href": "slides/09-regression.html#posterior-predictive-distribution",
    "title": "Lecture 09: Bayesian Regression",
    "section": "Posterior predictive distribution",
    "text": "Posterior predictive distribution\n\nSuppose we observe a new sample \\(X^*\\) and we want to predict its corresponding response \\(Y^*\\).\nThe posterior predictive distribution is \\[\np(Y^* \\mid X^*, X, Y) = \\int p(Y^* \\mid \\beta, \\sigma^2, X^*) \\pi(\\beta, \\sigma^2 \\mid X, Y) \\; d\\beta\\; d\\sigma^2.\n\\]\nFor a normal linear model with the noninformative prior \\(\\pi(\\beta, \\sigma^2) \\propto \\sigma^{-2}\\), we can compute the analytic form of the predictive distribution.\nHowever, there is no need to do so, since once we have the posterior samples of \\(\\beta\\) and \\(\\sigma^2\\), we can easily draw samples from the posterior predictive distribution.\nThat is, with \\((\\beta^{(i)}, \\sigma^{2,{(i)}}) \\iid \\pi(\\beta, \\sigma^2 \\mid X, Y)\\), we can draw \\[\nY^{*(i)} \\mid X^*, X, Y \\sim N(X^{*T}\\beta^{(i)}, \\sigma^{2,(i)}).\n\\]"
  },
  {
    "objectID": "slides/09-regression.html#computation",
    "href": "slides/09-regression.html#computation",
    "title": "Lecture 09: Bayesian Regression",
    "section": "Computation",
    "text": "Computation\nThere are many R packages for fitting Bayesian regression models:\n\n\nrstanarm: pre-compiled Stan regression models; easier to use; faster; less flexible\nbrms: compile models on the fly; slower; more flexible\nbayesplot: visualization of Bayesian models\n\nMCMC diagnostics (using functions mcmc_*)\nPosterior prediction checking (PPC) (using functions ppc_*)\nPosterior prediction distribution (PPD) (using functions ppd_*)\n\nloo: efficient approximate leave-one-out cross-validation for fitted Bayesian models\nshinystan: Interactive diagnostic tools for assessing Bayesian models"
  },
  {
    "objectID": "slides/09-regression.html#example-kids-test-scoreskidiq",
    "href": "slides/09-regression.html#example-kids-test-scoreskidiq",
    "title": "Lecture 09: Bayesian Regression",
    "section": "Example: Kids’ Test Scores1",
    "text": "Example: Kids’ Test Scores1\nData from a survey of adult American women and their children (a subsample from the National Longitudinal Survey of Youth).\n\n\nSource: Gelman and Hill (2007)\n434 obs. of 4 variables\n\nkid_score: Child’s IQ score\nmom_hs: Indicator for whether the mother has a high school degree\nmom_iq: Mother’s IQ score\nmom_age: Mother’s age\n\n\n\nhttps://avehtari.github.io/ROS-Examples/KidIQ/kidiq.html"
  },
  {
    "objectID": "slides/09-regression.html#fit-a-bayesian-linear-regression-model",
    "href": "slides/09-regression.html#fit-a-bayesian-linear-regression-model",
    "title": "Lecture 09: Bayesian Regression",
    "section": "Fit a Bayesian linear regression model",
    "text": "Fit a Bayesian linear regression model\n\nlibrary(rstanarm)\nlibrary(bayesplot)\nlibrary(ggplot2)\n\ndata(kidiq)\n\n# fit a linear regression model\nfit &lt;- stan_glm(kid_score ~ mom_hs + mom_iq, data=kidiq, \n                family = gaussian(),\n                prior = NULL,\n                prior_intercept = NULL,\n                prior_aux = NULL,\n                chains = 4, iter = 2000, seed = 2023,\n                refresh = 0)"
  },
  {
    "objectID": "slides/09-regression.html#summary-of-the-fitted-model",
    "href": "slides/09-regression.html#summary-of-the-fitted-model",
    "title": "Lecture 09: Bayesian Regression",
    "section": "Summary of the fitted model",
    "text": "Summary of the fitted model\n\n\nstan_glm\n family:       gaussian [identity]\n formula:      kid_score ~ mom_hs + mom_iq\n observations: 434\n predictors:   3\n------\n            Median MAD_SD\n(Intercept) 25.8    5.5  \nmom_hs       6.0    2.2  \nmom_iq       0.6    0.1  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 18.2    0.6  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg"
  },
  {
    "objectID": "slides/09-regression.html#prior-summary",
    "href": "slides/09-regression.html#prior-summary",
    "title": "Lecture 09: Bayesian Regression",
    "section": "Prior Summary",
    "text": "Prior Summary\n\nprior_summary(fit)\n\nPriors for model 'fit' \n------\nIntercept (after predictors centered)\n ~ flat\n\nCoefficients\n ~ flat\n\nAuxiliary (sigma)\n ~ flat\n------\nSee help('prior_summary.stanreg') for more details"
  },
  {
    "objectID": "slides/09-regression.html#mcmc-diagnostics",
    "href": "slides/09-regression.html#mcmc-diagnostics",
    "title": "Lecture 09: Bayesian Regression",
    "section": "MCMC Diagnostics",
    "text": "MCMC Diagnostics\n\ncolor_scheme_set(\"brightblue\")\nmcmc_trace(fit, pars = c(\"(Intercept)\", \"mom_iq\", \"mom_hs\"))"
  },
  {
    "objectID": "slides/09-regression.html#mcmc-diagnostic",
    "href": "slides/09-regression.html#mcmc-diagnostic",
    "title": "Lecture 09: Bayesian Regression",
    "section": "MCMC Diagnostic",
    "text": "MCMC Diagnostic\n\nmcmc_dens_overlay(fit, pars = c(\"mom_iq\", \"mom_hs\")) + \n    labs(title = \"Posterior distributions\")"
  },
  {
    "objectID": "slides/09-regression.html#posterior-prediction-checks-in-sample-checking",
    "href": "slides/09-regression.html#posterior-prediction-checks-in-sample-checking",
    "title": "Lecture 09: Bayesian Regression",
    "section": "Posterior Prediction Checks (In-sample checking)",
    "text": "Posterior Prediction Checks (In-sample checking)\nCompare observed data to draws from the posterior predictive distribution.\n\n# predicted vs. observed\nY_rep &lt;- posterior_predict(fit, seed = 2023)\n\nppc_dens_overlay(kidiq$kid_score, Y_rep[1:10,], size = 1) + \n  xlab(\"Kids' IQ Score\") +\n  ylab(\"Density\") +\n  labs(title = \"Posterior predictive distribution for observed samples\")"
  },
  {
    "objectID": "slides/09-regression.html#posterior-prediction-checks-in-sample-checking-output",
    "href": "slides/09-regression.html#posterior-prediction-checks-in-sample-checking-output",
    "title": "Lecture 09: Bayesian Regression",
    "section": "Posterior Prediction Checks (In-sample checking)",
    "text": "Posterior Prediction Checks (In-sample checking)"
  },
  {
    "objectID": "slides/09-regression.html#posterior-prediction-distributions-out-of-sample-prediction",
    "href": "slides/09-regression.html#posterior-prediction-distributions-out-of-sample-prediction",
    "title": "Lecture 09: Bayesian Regression",
    "section": "Posterior Prediction Distributions (Out-of-sample prediction)",
    "text": "Posterior Prediction Distributions (Out-of-sample prediction)\nCompute the posterior predictive distribution for a new sample.\n\nX_new &lt;- data.frame(mom_hs = 0, mom_iq = seq(70, 140, by = 5))\nY_new_pred &lt;- posterior_predict(fit, newdata = X_new) \n\nppd_intervals(Y_new_pred[1:500,], prob = 0.75, linewidth = 2) + \n  ylab(\"Kids' IQ Score\") +\n  ggtitle(\"Posterior predictive intervals for new samples\")"
  },
  {
    "objectID": "slides/09-regression.html#posterior-prediction-distributions-out-of-sample-prediction-output",
    "href": "slides/09-regression.html#posterior-prediction-distributions-out-of-sample-prediction-output",
    "title": "Lecture 09: Bayesian Regression",
    "section": "Posterior Prediction Distributions (Out-of-sample prediction)",
    "text": "Posterior Prediction Distributions (Out-of-sample prediction)"
  },
  {
    "objectID": "slides/09-regression.html#interactive-tool-shinystan",
    "href": "slides/09-regression.html#interactive-tool-shinystan",
    "title": "Lecture 09: Bayesian Regression",
    "section": "Interactive tool: shinystan",
    "text": "Interactive tool: shinystan\n\nlibrary(shinystan)\nlaunch_shinystan(fit)"
  },
  {
    "objectID": "slides/09-regression.html#regularized-linear-regression",
    "href": "slides/09-regression.html#regularized-linear-regression",
    "title": "Lecture 09: Bayesian Regression",
    "section": "Regularized linear regression",
    "text": "Regularized linear regression\n\nThe OLS solution \\(\\hat{\\beta} = (X^TX)^{-1}X^TY\\) is obtained by minimizing the residual sum of squares (RSS), i.e., \\(\\arg\\min_{\\beta\\in\\R^p} \\|Y - X\\beta\\|^2\\).\nIn some cases, the matrix \\(X^TX\\) is ill-conditioned, i.e., the inverse \\((X^TX)^{-1}\\) is numerically unstable.\nAlso when \\(p &gt; n\\), the matrix \\(X^TX\\) is singular and the OLS solution does not exist.\nWe can consider the following penalized least-square problem: \\[\n\\arg\\min_{\\beta\\in\\R^p}  \\|Y - X\\beta\\|^2 + \\lambda \\cdot\\text{Penalty}(\\beta) \\quad \\text{for some } \\lambda &gt; 0.\n\\]\nThere are three commonly used penalties:\n\nRidge regression: \\(\\text{Penalty}(\\beta) = \\|\\beta\\|^2 = \\sum_{j=1}^p \\beta_j^2\\)\nLASSO: \\(\\text{Penalty}(\\beta) = \\|\\beta\\|_1 = \\sum_{j=1}^p |\\beta_j|\\)\nElastic net: \\(\\text{Penalty}(\\beta) = \\alpha \\|\\beta\\|^2 + (1-\\alpha) \\|\\beta\\|_1\\) for some \\(\\alpha \\in [0, 1]\\)."
  },
  {
    "objectID": "slides/09-regression.html#bayesian-interpretation-of-regularization",
    "href": "slides/09-regression.html#bayesian-interpretation-of-regularization",
    "title": "Lecture 09: Bayesian Regression",
    "section": "Bayesian interpretation of regularization",
    "text": "Bayesian interpretation of regularization\n\nAny penalized likelihood estimator has a Bayesian interpretation, i.e., it is the posterior mode under the prior \\(\\pi(\\beta) \\propto \\exp(-\\lambda\\cdot\\text{Penalty}(\\beta))\\).\nFor example,\n\nRidge regression: normal prior \\(\\beta_j \\sim N(0, \\lambda^{-1})\\)\nLASSO: Laplace prior \\(\\beta_j \\sim \\text{Laplace}(0, \\lambda^{-1})\\)\nElastic net: \\(\\pi(\\boldsymbol{\\beta}) \\propto \\exp \\left\\{-\\lambda_1\\|\\boldsymbol{\\beta}\\|_1-\\lambda_2\\|\\boldsymbol{\\beta}\\|_2^2\\right\\}\\)\n\nUsually, the tuning parameter \\(\\lambda\\) is chosen by cross-validation.\nUnder the Bayesian framework, we can\n\nput a prior on \\(\\lambda\\) (fully Bayesian)\nput a hierarchical prior on \\(\\lambda\\) (hierarchical Bayes)\nestimate it from the data (empirical Bayes)"
  },
  {
    "objectID": "slides/09-regression.html#bayesian-lasso",
    "href": "slides/09-regression.html#bayesian-lasso",
    "title": "Lecture 09: Bayesian Regression",
    "section": "Bayesian LASSO",
    "text": "Bayesian LASSO\n\nLASSO = “Least Absolute Shrinkage and Selection Operator”\nModel (this is how stan_glm implements LASSO prior): \\[\\begin{align*}\nY \\mid X, \\beta, \\sigma^2 & \\sim N(X\\beta, \\sigma^2 I_n) \\\\\n\\beta_j & \\iid \\text{Laplace}(0, \\lambda^{-1}) \\\\\n\\lambda & \\sim \\chi^2_{\\nu}\n\\end{align*}\\] where \\(\\nu\\) is the degrees of freedom (the default is \\(\\nu = 1\\)).\nBias increases as \\(\\lambda\\) increases (producing more zeroes).\nVariance decreases as \\(\\lambda\\) increases.\nHence larger value of \\(\\nu\\) produces more shrinkage."
  },
  {
    "objectID": "slides/09-regression.html#bayesian-lasso-1",
    "href": "slides/09-regression.html#bayesian-lasso-1",
    "title": "Lecture 09: Bayesian Regression",
    "section": "Bayesian LASSO",
    "text": "Bayesian LASSO\n\ndata &lt;- read.csv(\"dataset/car_price/CarPrice_Assignment.csv\")\nfit_lasso &lt;- stan_glm(price ~ highwaympg + citympg + horsepower + \n                          peakrpm + compressionratio, \n                      data = data, \n                      prior = lasso(df = 1, location = 0, \n                                    scale = 0.5, autoscale = TRUE),\n                      prior_intercept = normal(location = 0, scale = 1, \n                                               autoscale = TRUE),\n                      prior_aux = exponential(rate = 1, autoscale = TRUE),\n                      chains = 4, iter = 2000, seed = 2023,\n                      refresh = 0)\n\n\n\nData source: https://www.kaggle.com/datasets/hellbuoy/car-price-prediction"
  },
  {
    "objectID": "slides/09-regression.html#summary-of-the-fitted-model-1",
    "href": "slides/09-regression.html#summary-of-the-fitted-model-1",
    "title": "Lecture 09: Bayesian Regression",
    "section": "Summary of the fitted model",
    "text": "Summary of the fitted model\n\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      price ~ highwaympg + citympg + horsepower + peakrpm + compressionratio\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 205\n predictors:   6\n\nEstimates:\n                   mean    sd      10%     50%     90%  \n(Intercept)       9493.2  4656.5  3681.0  9573.9 15322.0\nhighwaympg        -328.0   153.9  -528.2  -320.4  -140.2\ncitympg             71.7   171.2  -131.5    57.3   296.4\nhorsepower         139.0    12.4   122.9   139.2   154.7\npeakrpm             -1.4     0.7    -2.3    -1.4    -0.5\ncompressionratio   451.8    88.6   338.3   450.3   564.1\nsigma             4123.4   212.5  3858.6  4113.1  4398.4\n\nFit Diagnostics:\n           mean    sd      10%     50%     90%  \nmean_PPD 13262.4   398.9 12755.5 13256.9 13779.3\n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n                 mcse Rhat n_eff\n(Intercept)      72.0  1.0 4179 \nhighwaympg        2.6  1.0 3374 \ncitympg           2.9  1.0 3555 \nhorsepower        0.2  1.0 4133 \npeakrpm           0.0  1.0 4251 \ncompressionratio  1.4  1.0 4222 \nsigma             3.2  1.0 4291 \nmean_PPD          6.4  1.0 3869 \nlog-posterior     0.1  1.0  801 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1)."
  },
  {
    "objectID": "slides/09-regression.html#high-dimensional-linear-regression-1",
    "href": "slides/09-regression.html#high-dimensional-linear-regression-1",
    "title": "Lecture 09: Bayesian Regression",
    "section": "High-dimensional linear regression",
    "text": "High-dimensional linear regression\n\nIn some applications, we might have more predictors than samples, i.e., \\(p \\gg n\\).\nIn such cases, the sparsity assumption is often used, i.e., only a few predictors are relevant.\nLASSO or Bayesian Lasso can be used to select the relevant predictors.\nHowever LASSO or Bayesian LASSO have some problems:\n\nit does not handle correlated variables well\nit shrinks all coefficients with the same intensity\nit often overshrinks large coefficients.\n\nWe will now introduce two priors for high-dimensional sparse linear regression that address these problems."
  },
  {
    "objectID": "slides/09-regression.html#spike-and-slab-prior",
    "href": "slides/09-regression.html#spike-and-slab-prior",
    "title": "Lecture 09: Bayesian Regression",
    "section": "Spike-and-slab prior",
    "text": "Spike-and-slab prior\n\nThe spike-and-slab prior1 is \\[\n\\beta_j \\mid w, \\tau^2 \\sim (1 - w)\\delta_0 + w \\cdot N(0, \\tau^2).\n\\]\nEquivalently, \\[\\begin{align*}\n\\beta_j \\mid \\gamma_j = 0 & \\sim \\delta_0 \\\\\n\\beta_j \\mid \\gamma_j = 1 & \\sim N(0, \\tau^2)\\\\\n\\gamma_j & \\iid \\text{Ber}(w)\n\\end{align*}\\]\n\\(w\\) is the parameter that controls the sparsity, and \\(\\gamma_j\\) is the indicator of the \\(j\\)-th predictor being relevant.\nThis prior is considered the ``gold standard’’ for sparse regression.\nHowever, it is computationally expensive to update the discrete variables \\(\\gamma_j\\).\n\nMitchell, T. J., & Beauchamp, J. J. (1988). Bayesian variable selection in linear regression. Journal of the American Statistical Association, 83(404), 1023-1032."
  },
  {
    "objectID": "slides/09-regression.html#horseshoe-prior",
    "href": "slides/09-regression.html#horseshoe-prior",
    "title": "Lecture 09: Bayesian Regression",
    "section": "Horseshoe prior",
    "text": "Horseshoe prior\n\nThe horseshoe prior1 is \\[\\begin{align*}\n\\beta_j \\mid \\lambda_j, \\tau & \\sim N\\left(0, \\tau^2 \\lambda_j^2\\right) \\\\\n\\lambda_j & \\sim C^{+}(0,1), \\quad j=1, \\ldots, p\n\\end{align*}\\] where \\(C^{+}(0, 1)\\) is the half-Cauchy distribution.\nThis is an example of global-local shrinkage prior, where \\(\\lambda_j\\) is the local shrinkage parameter and \\(\\tau\\) is the global shrinkage parameter.\n\nCarvalho, C. M., Polson, N. G., & Scott, J. G. (2010). The horseshoe estimator for sparse signals. Biometrika, 97(2), 465-480."
  },
  {
    "objectID": "slides/09-regression.html#available-priors-for-stan_glm",
    "href": "slides/09-regression.html#available-priors-for-stan_glm",
    "title": "Lecture 09: Bayesian Regression",
    "section": "Available priors for stan_glm",
    "text": "Available priors for stan_glm\n\n\nStudent-\\(t\\) family: cauchy, \\(t\\), normal\nHierarchical shrinkage family: horseshoe, horseshoe+\nLaplace family: laplace, lasso\nR2 prior: This prior hinges on prior beliefs about the location of \\(R^2\\), the proportion of variance in the outcome attributable to the predictors.\nCheck https://mc-stan.org/rstanarm/reference/priors.html for more details."
  },
  {
    "objectID": "slides/09-regression.html#compare-different-models",
    "href": "slides/09-regression.html#compare-different-models",
    "title": "Lecture 09: Bayesian Regression",
    "section": "Compare different models",
    "text": "Compare different models\n\nSuppose we have two regression models: \\[\\begin{align*}\n& Y = \\beta_0 + \\beta_1X_1\\\\\n& Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2.\n\\end{align*}\\]\nWe can compare these two models using the Bayes factor.\nHowever, since we are fitting linear models, we can also use the \\(R^2\\) to compare the models.\nThe \\(R^2\\) statistic is defined as \\[\\begin{align*}\nR^2 & = \\frac{\\text{Explained variance}}{\\text{Total variance}} = 1 - \\frac{\\text{Residual variance}}{\\text{Total variance}}\\\\\n& = 1 - \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}.\n\\end{align*}\\]\nTherefore, a model with a larger \\(R^2\\) is preferred."
  },
  {
    "objectID": "slides/09-regression.html#bayesian-r2",
    "href": "slides/09-regression.html#bayesian-r2",
    "title": "Lecture 09: Bayesian Regression",
    "section": "Bayesian \\(R^2\\)",
    "text": "Bayesian \\(R^2\\)\n\nHowever, the \\(R^2\\) always increases even if we add irrelevant predictors.\nAdjusted \\(R^2\\) is a modification of \\(R^2\\) that penalizes the addition of irrelevant predictors \\[\nR^2_{\\text{adj}} =1-\\left(1-R^2\\right) \\frac{n-1}{n-p-1}.\n\\]\nInstead of comparing the \\(R^2\\) or the adjusted \\(R^2\\) of the two models, we can compare the posterior distributions of the \\(R^2\\) of the two models.\n\n\n\nhttps://avehtari.github.io/ROS-Examples/KidIQ/kidiq_R2.html"
  },
  {
    "objectID": "slides/09-regression.html#example",
    "href": "slides/09-regression.html#example",
    "title": "Lecture 09: Bayesian Regression",
    "section": "Example",
    "text": "Example\n\nfit1 &lt;- stan_glm(kid_score ~ mom_hs, data=kidiq, \n                 seed=2023, refresh=0)\nfit2 &lt;- stan_glm(kid_score ~ mom_hs + mom_iq, data=kidiq, \n                 seed=2023, refresh=0)\n\n## Adding random noise as predictors\nset.seed(2023)\nn &lt;- nrow(kidiq)\nkidiqr &lt;- kidiq\nkidiqr$noise &lt;- array(rnorm(5*n), c(n,5))\nfit3 &lt;- stan_glm(kid_score ~ mom_hs + mom_iq + noise, data=kidiqr,\n                 seed=2023, refresh=0)\n\nr2_fit1 &lt;- bayes_R2(fit1)\nr2_fit2 &lt;- bayes_R2(fit2)\nr2_fit3 &lt;- bayes_R2(fit3)"
  },
  {
    "objectID": "slides/09-regression.html#posterior-distribution-of-r2",
    "href": "slides/09-regression.html#posterior-distribution-of-r2",
    "title": "Lecture 09: Bayesian Regression",
    "section": "Posterior distribution of \\(R^2\\)",
    "text": "Posterior distribution of \\(R^2\\)\nAlthough adding noise predictors does increase the \\(R^2\\), the change is negligible compared to the uncertainty of \\(R^2\\).\n\n\n\nHome"
  },
  {
    "objectID": "slides/07-mcmc.html#motivation",
    "href": "slides/07-mcmc.html#motivation",
    "title": "Lecture 07: Markov Chain Monte Carlo",
    "section": "Motivation",
    "text": "Motivation\n\n\\[\n\\newcommand{\\mc}[1]{\\mathcal{#1}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\E}{\\mathbb{E}}\n\\renewcommand{\\P}{\\mathbb{P}}\n\\newcommand{\\var}{{\\rm Var}} % Variance\n\\newcommand{\\mse}{{\\rm MSE}} % MSE\n\\newcommand{\\bias}{{\\rm Bias}} % MSE\n\\newcommand{\\cov}{{\\rm Cov}} % Covariance\n\\newcommand{\\iid}{\\stackrel{\\rm iid}{\\sim}}\n\\newcommand{\\ind}{\\stackrel{\\rm ind}{\\sim}}\n\\renewcommand{\\choose}[2]{\\binom{#1}{#2}}  % Choose\n\\newcommand{\\chooses}[2]{{}_{#1}C_{#2}}  % Small choose\n\\newcommand{\\cd}{\\stackrel{d}{\\rightarrow}}\n\\newcommand{\\cas}{\\stackrel{a.s.}{\\rightarrow}}\n\\newcommand{\\cp}{\\stackrel{p}{\\rightarrow}}\n\\newcommand{\\bin}{{\\rm Bin}}\n\\newcommand{\\ber}{{\\rm Ber}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\]\n\n\nSuppose now we have the posterior distribution \\(\\pi(\\theta \\mid x)\\) for our problem.\nWe need to derive some quantities from the posterior for statistical inference, for example\n\nposterior mean, variance, quantiles, and\nposterior predictive distribution.\n\nWe have seen that in many cases, the posterior is only available upto a normalizing constant.\nThere are three types of methods:\n\nnumerical integration,\ndistributional approximations, and\nsampling-based methods (Monte Carlo methods)."
  },
  {
    "objectID": "slides/07-mcmc.html#monte-carlo-methods",
    "href": "slides/07-mcmc.html#monte-carlo-methods",
    "title": "Lecture 07: Markov Chain Monte Carlo",
    "section": "Monte Carlo Methods",
    "text": "Monte Carlo Methods\n\nSuppose we need to compute \\(\\E(h(\\theta) \\mid x)\\).\nThe Monte Carlo approximation is \\[\n\\E(h(\\theta) \\mid x) \\approx \\frac{1}{m}\\sum_{i=1}^m h(\\theta^{(i)}),\n\\] where \\(\\theta^{(1)}, \\ldots, \\theta^{(m)} \\iid \\pi(\\theta \\mid x)\\).\nHow to generate iid random samples from an arbitrary distribution?\nHow to do it without knowing the normalizing constant?"
  },
  {
    "objectID": "slides/07-mcmc.html#discretization",
    "href": "slides/07-mcmc.html#discretization",
    "title": "Lecture 07: Markov Chain Monte Carlo",
    "section": "Discretization",
    "text": "Discretization\n\nFix a set of grid values \\(\\theta_1, \\ldots, \\theta_N\\).\nCompute \\(\\pi(\\theta_1 \\mid x), \\ldots \\pi(\\theta_N \\mid x)\\) and normalize them such that \\[\n\\sum_{i=1}^N \\pi(\\theta_i \\mid x) = 1.\n\\]\nGenerate \\(m\\) samples from the multinomial distribution with probabilities \\(\\pi(\\theta_1 \\mid x), \\ldots, \\pi(\\theta_N \\mid x)\\).\nThat is, we approximate \\(\\pi(\\theta \\mid x)\\) on \\(\\Theta\\) with a multinomial distribution on \\(\\{\\theta_1, \\ldots, \\theta_N\\}\\)."
  },
  {
    "objectID": "slides/07-mcmc.html#rejection-sampling",
    "href": "slides/07-mcmc.html#rejection-sampling",
    "title": "Lecture 07: Markov Chain Monte Carlo",
    "section": "Rejection sampling",
    "text": "Rejection sampling\n\nTarget distribution: \\(\\pi(\\theta \\mid x)\\) (possibly unnormalized)\nWe need a proposal distribution \\(q(\\theta)\\) such that\n\nit is easy to sample from \\(q(\\theta)\\), and\nthe importance ratio \\(\\pi(\\theta \\mid x) / q(\\theta)\\) is bounded, i.e., \\[\n\\frac{\\pi(\\theta \\mid x)}{q(\\theta)} \\leq M &lt; \\infty, \\quad \\text{for all } \\theta \\in \\Theta.\n\\]\n\nAlgorithm:\n\nSample \\(\\theta \\sim q(\\theta)\\).\nWith probability \\(\\frac{\\pi(\\theta \\mid x)}{M q(\\theta)}\\), accept \\(\\theta\\) as a draw from \\(\\pi(\\theta \\mid x)\\). If the drawn \\(\\theta\\) is rejected, return to step 1.\nRepeat steps 1 and 2 until \\(m\\) samples are obtained.\n\n\n\n\nSee Theorem 5.6.8 in Statistical Inference by Casella & Berger (2002) for the proof."
  },
  {
    "objectID": "slides/07-mcmc.html#example",
    "href": "slides/07-mcmc.html#example",
    "title": "Lecture 07: Markov Chain Monte Carlo",
    "section": "Example",
    "text": "Example\nGenerate random samples from \\(\\text{Beta}(\\alpha, \\beta)\\), \\(\\alpha = 3\\) and \\(\\beta = 4\\).\n\n\nProposal distribution: \\(\\text{Unif}(0, 1)\\).\n\\(M = 3\\)."
  },
  {
    "objectID": "slides/07-mcmc.html#example-1",
    "href": "slides/07-mcmc.html#example-1",
    "title": "Lecture 07: Markov Chain Monte Carlo",
    "section": "Example",
    "text": "Example\n\nset.seed(2023)\nm &lt;- 10000\nx &lt;- c()\nfor(i in 1:m){\n    v &lt;- runif(1) # proposal\n    \n    # rejection step\n    u &lt;- runif(1)\n    if(u &lt;= dbeta(v, alpha, beta) / (3 * dunif(v))){\n        x &lt;- c(x, v)\n    }\n}\n\nhist(x, freq = FALSE, xlim = c(0,1))\ncurve(dbeta(x, alpha, beta), col = \"red\", lwd = 2, add = TRUE)\ntext(0.1, 2, substitute(\"accept. rate\" == a, \n                        list(a = round(length(x)/m, 2))))"
  },
  {
    "objectID": "slides/07-mcmc.html#example-1-output",
    "href": "slides/07-mcmc.html#example-1-output",
    "title": "Lecture 07: Markov Chain Monte Carlo",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "slides/07-mcmc.html#problems-with-rejection-sampling",
    "href": "slides/07-mcmc.html#problems-with-rejection-sampling",
    "title": "Lecture 07: Markov Chain Monte Carlo",
    "section": "Problems with rejection sampling",
    "text": "Problems with rejection sampling\n\nIf \\(M\\) is large, then the acceptance rate is low, and hence the algorithm is inefficient.\nYou have to know \\(\\pi(\\theta \\mid x)\\) well to find a good proposal distribution, with a reasonable \\(M\\).\nFor multivariate distributions, it is virtually impossible to find a good proposal distribution.\nHowever, it can be used for simple univariate distributions or truncated distributions.\nAlthough rejection sampling produces iid samples from the target distribution, it is inefficient."
  },
  {
    "objectID": "slides/07-mcmc.html#markov-chain-monte-carlo-mcmc",
    "href": "slides/07-mcmc.html#markov-chain-monte-carlo-mcmc",
    "title": "Lecture 07: Markov Chain Monte Carlo",
    "section": "Markov Chain Monte Carlo (MCMC)",
    "text": "Markov Chain Monte Carlo (MCMC)\n\nMCMC uses samples from a Markov chain whose stationary distribution is the target distribution.\nHence the samples are not independent and not from the target distribution.\nThey are only iid from the target distribution asymptotically.\nThe problem is now how to construct a Markov chain whose stationary distribution is the posterior.\nWe will introduce three algorithms:\n\nGibbs sampler\nMetropolis-Hastings algorithm\nHamiltonian Monte Carlo (HMC)"
  },
  {
    "objectID": "slides/07-mcmc.html#gibbs-sampler",
    "href": "slides/07-mcmc.html#gibbs-sampler",
    "title": "Lecture 07: Markov Chain Monte Carlo",
    "section": "Gibbs Sampler",
    "text": "Gibbs Sampler\n\nConsider a parameter vector \\(\\theta = (\\theta_1, \\theta_2, \\ldots, \\theta_p)\\) with posterior \\(\\pi(\\theta|y)\\).\nIf we can sample from the full conditional distributions \\(\\pi(\\theta_i|\\theta_{-i}, y)\\), then we can construct a Markov chain whose stationary distribution is the posterior.\nAlgorithm:\n\nInitialize \\(\\theta^{(0)} = (\\theta_1^{(0)}, \\theta_2^{(0)}, \\ldots, \\theta_p^{(0)})\\).\nFor \\(t = 1, 2, \\ldots, T\\):\n\nSample \\(\\theta_1^{(t)} \\sim \\pi(\\theta_1|\\theta_2^{(t-1)}, \\theta_3^{(t-1)}, \\ldots, \\theta_p^{(t-1)}, y)\\).\nSample \\(\\theta_j^{(t)} \\sim \\pi(\\theta_2|\\theta_1^{(t)}, \\ldots, \\theta_{j-1}^{(t)}, \\theta_{j+1}^{(t-1)}, \\ldots, \\theta_p^{(t-1)}, y)\\).\n\nReturn \\(\\theta^{(1)}, \\theta^{(2)}, \\ldots, \\theta^{(T)}\\)."
  },
  {
    "objectID": "slides/07-mcmc.html#example-bivariate-normal",
    "href": "slides/07-mcmc.html#example-bivariate-normal",
    "title": "Lecture 07: Markov Chain Monte Carlo",
    "section": "Example: Bivariate Normal",
    "text": "Example: Bivariate Normal\n\nSuppose we want to generate samples from a bivariate normal with mean \\(\\mu = (\\mu_1, \\mu_2)\\) and covariance matrix \\(\\Sigma\\).\nLet \\([X_1, X_2]^T \\sim N(\\mu, \\Sigma)\\). The full conditionals are:\n\n\\(X_1|X_2 \\sim N(\\mu_1 + \\Sigma_{12}\\Sigma_{22}^{-1}(X_2 - \\mu_2), \\Sigma_{11} - \\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21})\\).\n\\(X_2|X_1 \\sim N(\\mu_2 + \\Sigma_{21}\\Sigma_{11}^{-1}(X_1 - \\mu_1), \\Sigma_{22} - \\Sigma_{21}\\Sigma_{11}^{-1}\\Sigma_{12})\\).\n\n\n\n\n# initialize\nmu &lt;- c(0, 0)\nSigma &lt;- matrix(c(1, 0.5, 0.5, 1), nrow = 2)\nX &lt;- matrix(0, nrow = 2, ncol = 1000)\n\n# Gibbs sampler\nfor (t in 2:1000) {\n    X[1, t] &lt;- rnorm(1, mu[1] + Sigma[1, 2] / Sigma[2, 2] * (X[2, t-1] - mu[2]), \n                     sqrt(Sigma[1, 1] - Sigma[1, 2] / Sigma[2, 2] * Sigma[2, 1]))\n    X[2, t] &lt;- rnorm(1, mu[2] + Sigma[2, 1] / Sigma[1, 1] * (X[1, t] - mu[1]), \n                     sqrt(Sigma[2, 2] - Sigma[2, 1] / Sigma[1, 1] * Sigma[1, 2]))\n}"
  },
  {
    "objectID": "slides/07-mcmc.html#checking-convergence",
    "href": "slides/07-mcmc.html#checking-convergence",
    "title": "Lecture 07: Markov Chain Monte Carlo",
    "section": "Checking Convergence",
    "text": "Checking Convergence\n\n# trace plot\npar(mfrow = c(1,2))\nplot(X[1, ], type = \"l\", ylab = expression(X[1]), xlab = \"Iteration\")\nplot(X[2, ], type = \"l\", ylab = expression(X[2]), xlab = \"Iteration\")"
  },
  {
    "objectID": "slides/07-mcmc.html#checking-convergence-1",
    "href": "slides/07-mcmc.html#checking-convergence-1",
    "title": "Lecture 07: Markov Chain Monte Carlo",
    "section": "Checking Convergence",
    "text": "Checking Convergence\n\nX %&lt;&gt;% t() %&gt;% as.data.frame() %&gt;% rename(X1 = V1, X2 = V2) \n\nggplot(X, aes(x = X1, y = X2)) + \n    geom_point() + \n    geom_density_2d() + \n    coord_fixed(ratio = 1) + \n    labs(title = TeX(\"Joint distribution of $X_1$ and $X_2$\"))"
  },
  {
    "objectID": "slides/07-mcmc.html#when-to-use-gibbs-sampler",
    "href": "slides/07-mcmc.html#when-to-use-gibbs-sampler",
    "title": "Lecture 07: Markov Chain Monte Carlo",
    "section": "When to use Gibbs sampler?",
    "text": "When to use Gibbs sampler?\n\nIt is useful for multidimensional distributions, especially when the full conditionals are easy to sample from.\nEven if the full conditionals are not easy to sample from, we can use other algorithms to sample from the conditionals."
  },
  {
    "objectID": "slides/07-mcmc.html#metropolis-algorithm",
    "href": "slides/07-mcmc.html#metropolis-algorithm",
    "title": "Lecture 07: Markov Chain Monte Carlo",
    "section": "Metropolis Algorithm",
    "text": "Metropolis Algorithm\n\nTarget distribution: \\(\\pi(\\theta \\mid x)\\).\nProposal distribution at \\(t\\)th iteration: \\(J_t(\\theta^* \\mid \\theta^{(t-1)})\\)\n\n\\(J_t\\) needs to be symmetric, i.e., \\(J_t(\\theta_1 \\mid \\theta_2) = J_t(\\theta_2\\mid\\theta_1)\\).\n\nAlgorithm:\n\nAt \\(t\\)th iteration, sample \\(\\theta^*\\) from \\(J_t(\\theta^* \\mid \\theta^{(t-1)})\\).\nCompute the acceptance ratio \\(\\rho(\\theta^{(t-1)}, \\theta^{*})=\\min\\left\\{\\frac{\\pi(\\theta^* \\mid x)}{\\pi(\\theta^{(t-1)} \\mid x)}, 1\\right\\}\\).\nSet \\[\n\\theta^{(t)}= \\begin{cases}\\theta^* & \\text { with prob. } \\rho(\\theta^{t-1}, \\theta^{*}) \\\\ \\theta^{(t-1)} & \\text { with prob. } 1-\\rho(\\theta^{(t-1)}, \\theta^{*}).\\end{cases}\n\\]\n\nIntuition: if \\(\\theta^*\\) is more likely than \\(\\theta^{(t-1)}\\), then accept \\(\\theta^*\\); otherwise, accept \\(\\theta^*\\) with probability \\(\\rho(\\theta^{(t-1)}, \\theta^{*})\\)."
  },
  {
    "objectID": "slides/07-mcmc.html#example-2",
    "href": "slides/07-mcmc.html#example-2",
    "title": "Lecture 07: Markov Chain Monte Carlo",
    "section": "Example",
    "text": "Example\n\nSuppose \\(X \\mid \\theta \\sim N(\\theta, 1)\\) and \\(\\theta \\sim \\text{Cauchy}(0, 1)\\).\nThe posterior is \\(\\pi(\\theta \\mid x) \\propto \\frac{1}{1+\\theta^2}\\exp\\left(-\\frac{1}{2}(x-\\theta)^2\\right)\\).\nA commonly used proposal is \\(J_t(\\cdot \\mid\\theta^{(t-1)}) = N(\\theta^{(t-1)}, \\sigma^2)\\), where \\(\\sigma^2\\) is a tuning parameter.\n\n\n\n# generate sample\nset.seed(2023)\ntheta_0 &lt;- rcauchy(1, 0, 1)\nX &lt;- rnorm(1, theta_0, 1)\n\n# log posterior\nlog_post &lt;- function(theta, x) {\n    -log(1 + theta^2) - 0.5 * (x - theta)^2\n}\ncurve(exp(log_post(x, X)), 5, 15, \n      ylab = TeX(\"unnormalized $\\\\pi(\\\\theta | x)$\"), \n      xlab = TeX(\"$\\\\theta$\"), col = \"blue\")"
  },
  {
    "objectID": "slides/07-mcmc.html#example-2-output",
    "href": "slides/07-mcmc.html#example-2-output",
    "title": "Lecture 07: Markov Chain Monte Carlo",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "slides/07-mcmc.html#example-3",
    "href": "slides/07-mcmc.html#example-3",
    "title": "Lecture 07: Markov Chain Monte Carlo",
    "section": "Example",
    "text": "Example\n\n# Metropolis algorithm\nT &lt;- 1000\ntheta &lt;- rep(0, T)\ntheta[1] &lt;- rnorm(1, 0, 1)\naccept &lt;- 0\nfor (t in 2:T) {\n    theta_star &lt;- rnorm(1, theta[t-1], 1)\n    r &lt;- exp(log_post(theta_star, X) - log_post(theta[t-1], X))\n    theta[t] &lt;- ifelse(runif(1) &lt; r, \n                       {accept &lt;- accept + 1; theta_star},\n                       theta[t-1])\n}\n\n\nAcceptance rate: 0.702.\nAutocorrelation (lag 1): 0.823, i.e., the correlation between \\(\\theta^{(t-1)}\\) and \\(\\theta^{(t)}\\).\nPosterior mean: 8.635."
  },
  {
    "objectID": "slides/07-mcmc.html#example-4",
    "href": "slides/07-mcmc.html#example-4",
    "title": "Lecture 07: Markov Chain Monte Carlo",
    "section": "Example",
    "text": "Example\n\n# normalizing constant\nZ &lt;- integrate(function(theta) exp(log_post(theta, X)), -Inf, Inf)$value\n\npar(mfrow = c(1,2))\n\nhist(theta, freq = FALSE, ylim = c(0, 0.4))\ncurve(exp(log_post(x, X))/Z, 5, 15, add = TRUE, col = \"red\")\n\nplot(theta, type = \"l\")"
  },
  {
    "objectID": "slides/07-mcmc.html#advantages-of-metropolis-algorithm",
    "href": "slides/07-mcmc.html#advantages-of-metropolis-algorithm",
    "title": "Lecture 07: Markov Chain Monte Carlo",
    "section": "Advantages of Metropolis Algorithm",
    "text": "Advantages of Metropolis Algorithm\n\nIt can be seen as a improvement of rejection sampling.\nWe don’t need to know the target distribution well to choose a proposal distribution.\nThe trade-off is that the generated samples are not longer iid.\nWe can tune the proposal distribution to improve the acceptance rate making the algorithm more efficient."
  },
  {
    "objectID": "slides/07-mcmc.html#metropolis-hastings-algorithm",
    "href": "slides/07-mcmc.html#metropolis-hastings-algorithm",
    "title": "Lecture 07: Markov Chain Monte Carlo",
    "section": "Metropolis-Hastings Algorithm",
    "text": "Metropolis-Hastings Algorithm\n\nMetropolis-Hastings algorithm is a generalization of Metropolis algorithm, in that it does not require symmetric proposals.\nFor a proposal distribution \\(J_t(\\theta_a \\mid \\theta_b)\\), the unconditional probability from \\(\\theta_a\\) to \\(\\theta_b\\) is \\[\np(\\theta^{t-1} = \\theta_a, \\theta^{t} = \\theta_b) = J_t(\\theta_b \\mid \\theta_a)\\pi(\\theta_a \\mid x).\n\\]\nHence the acceptance ratio becomes \\[\\begin{align*}\n\\rho(\\theta^{t-1}, \\theta^{*}) & = \\min\\left\\{\\frac{p(\\theta^{t-1} = \\theta^{t-1}, \\theta^{t} = \\theta^{*})}{p(\\theta^{t-1} = \\theta^{*}, \\theta^{t} = \\theta^{t-1})},1\\right\\}\\\\\n& = \\min\\left\\{\\frac{J_t(\\theta^{t-1} \\mid \\theta^{*})\\pi(\\theta^{*} \\mid x)}{J_t(\\theta^{*} \\mid \\theta^{t-1})\\pi(\\theta^{t-1} \\mid x)},1\\right\\}.\n\\end{align*}\\]\nThis modification allows us to use a wider range of proposal distributions and hence improve the efficiency of the algorithm."
  },
  {
    "objectID": "slides/07-mcmc.html#why-it-works",
    "href": "slides/07-mcmc.html#why-it-works",
    "title": "Lecture 07: Markov Chain Monte Carlo",
    "section": "Why it works?",
    "text": "Why it works?\n\nIt is obvious that MH generates a Markov chain. Why does it converge to the target distribution?\nDetailed balance condition1: A Markov chain has a stationary distribution \\(\\pi\\) if it satisfies the equation \\[\n\\pi(\\theta_a)\\P(X_t = \\theta_b \\mid X_{t-1} = \\theta_a) = \\pi(\\theta_b)\\P(X_t = \\theta_a \\mid X_{t-1} = \\theta_b).\n\\]\nProof (sketch): \\[\\begin{align*}\n& \\quad \\pi(\\theta_a \\mid x)\\P(X_t = \\theta_b \\mid X_{t-1} = \\theta_a)\\\\\n&= \\pi(\\theta_a \\mid x)J_t(\\theta_b \\mid \\theta_a) \\rho(\\theta_a, \\theta_b) \\\\\n& = \\pi(\\theta_a \\mid x)J_t(\\theta_b \\mid \\theta_a)\n\\min\\left\\{\\frac{J_t(\\theta_a \\mid \\theta_b)\\pi(\\theta_b \\mid x)}{J_t(\\theta_b \\mid \\theta_a)\\pi(\\theta_a \\mid x)},1\\right\\}\\\\\n& = \\min\\left\\{J_t(\\theta_a \\mid \\theta_b)\\pi(\\theta_b \\mid x), J_t(\\theta_b \\mid \\theta_a)\\pi(\\theta_a \\mid x)\\right\\}\n\\end{align*}\\]\n\nThe detailed balance condition implies the existence of a stationary distribution, but not the other way around."
  },
  {
    "objectID": "slides/07-mcmc.html#ergodic-theorems",
    "href": "slides/07-mcmc.html#ergodic-theorems",
    "title": "Lecture 07: Markov Chain Monte Carlo",
    "section": "Ergodic Theorems",
    "text": "Ergodic Theorems\n\nNow we know that the MH algorithm generates a Markov chain \\(\\theta_1, \\theta_2, \\ldots\\) with a stationary distribution \\(\\pi(\\theta \\mid x)\\).\nWe need to use these MCMC samples to estimate posterior quantities, for example, \\[\n\\frac{1}{M}\\sum_{i=1}^M h(\\theta_i).\n\\]\nIf the \\(\\theta_i\\)’s are iid, then we can use the CLT to obtain the consistency and the asymptotic distribution of the above estimator.\nFor Markov chains, we have ergodic theorems, which state that if \\(\\theta_1,\\theta_2,\\ldots\\) is ergodic, we have the CLT.\nSee Ch. 6 & 7 of Monte Carlo Statistical Methods by Robert & Casella (2004) for details."
  },
  {
    "objectID": "slides/07-mcmc.html#guidelines-for-choosing-proposal-distribution",
    "href": "slides/07-mcmc.html#guidelines-for-choosing-proposal-distribution",
    "title": "Lecture 07: Markov Chain Monte Carlo",
    "section": "Guidelines for choosing proposal distribution",
    "text": "Guidelines for choosing proposal distribution\n\nFor any \\(\\theta\\), it is easy to sample from \\(J\\left(\\theta^* \\mid \\theta\\right)\\).\nIt is easy to compute the ratio \\(\\rho\\).\nEach jump goes a reasonable distance in the parameter space (otherwise the random walk moves too slowly).\nThe jumps are not rejected too frequently (otherwise the random walk wastes too much time standing still)."
  },
  {
    "objectID": "slides/07-mcmc.html#combining-gibbs-and-mh",
    "href": "slides/07-mcmc.html#combining-gibbs-and-mh",
    "title": "Lecture 07: Markov Chain Monte Carlo",
    "section": "Combining Gibbs and MH",
    "text": "Combining Gibbs and MH\n\nIn fact, Gibbs sampler is a special case of MH, with the proposal being the conditional distribution: \\[\nJ_{j, t}^{\\text {Gibbs }}\\left(\\theta^* \\mid \\theta^{(t-1)}\\right)= \\begin{cases}\\pi\\left(\\theta_j^* \\mid \\theta_{-j}^{(t-1)}, x\\right) & \\text { if } \\theta_{-j}^*=\\theta_{-j}^{(t-1)} \\\\ 0 & \\text { otherwise. }\\end{cases}\n\\]\nWe can use MH to sample from \\(\\pi\\left(\\theta_j^* \\mid \\theta_{-j}^{(t-1)}, x\\right)\\). This is called Metropolis-within-Gibbs.\nIn cases where some conditionals are easy to sample from but others are not, Metropolis-within-Gibbs will be more efficient than a simple MH."
  },
  {
    "objectID": "slides/07-mcmc.html#diagnostic-of-mcmc-samples",
    "href": "slides/07-mcmc.html#diagnostic-of-mcmc-samples",
    "title": "Lecture 07: Markov Chain Monte Carlo",
    "section": "Diagnostic of MCMC samples",
    "text": "Diagnostic of MCMC samples\nSuppose now we have a sequence of MCMC samples: \\(\\theta_1, \\ldots, \\theta_m\\).\n\nDid I run the chain long enough so that the samples are roughly from the target distribution?\nHow strong is the dependence between the samples?\nHow to fix these problems?"
  },
  {
    "objectID": "slides/07-mcmc.html#mixing-and-convergence",
    "href": "slides/07-mcmc.html#mixing-and-convergence",
    "title": "Lecture 07: Markov Chain Monte Carlo",
    "section": "Mixing and convergence",
    "text": "Mixing and convergence\n\n\nConvergence: whether the chain has converged to the target distribution.\nMixing: Multiple chains should mix together.\n\n\n\nFig 11.3 of BDA"
  },
  {
    "objectID": "slides/07-mcmc.html#autocorrelation",
    "href": "slides/07-mcmc.html#autocorrelation",
    "title": "Lecture 07: Markov Chain Monte Carlo",
    "section": "Autocorrelation",
    "text": "Autocorrelation\nLag-\\(k\\) Autocorrelation: the correlation between \\(\\theta^{(t)}\\) and \\(\\theta^{(t+k)}\\) for \\(k &gt; 0\\).\n\nacf(theta, lag.max = 20)"
  },
  {
    "objectID": "slides/07-mcmc.html#effective-sample-size",
    "href": "slides/07-mcmc.html#effective-sample-size",
    "title": "Lecture 07: Markov Chain Monte Carlo",
    "section": "Effective sample size",
    "text": "Effective sample size\n\n\nWe can compute an approximate ``effective number of independent simulation draws’’ for any estimand of interest \\(\\psi\\).\nThe effective sample size is defined as \\[\n\\text{ESS} = \\frac{m}{1 + 2\\sum_{k=1}^\\infty \\rho_k},\n\\] where \\(m\\) is the length of a chain and \\(\\rho_k\\) is the lag-\\(k\\) autocorrelation.\nDifferent estimates of \\(\\rho_k\\)’s give different ESS’s.\n\n\n\nlibrary(coda)\nlibrary(mcmcse)\n\nprint(paste(\"ESS:\", round(coda::effectiveSize(theta), 2)))\n\n[1] \"ESS: 96.85\"\n\nprint(paste(\"ESS:\", round(mcmcse::ess(theta), 2)))\n\n[1] \"ESS: 113.83\""
  },
  {
    "objectID": "slides/07-mcmc.html#practical-tricks",
    "href": "slides/07-mcmc.html#practical-tricks",
    "title": "Lecture 07: Markov Chain Monte Carlo",
    "section": "Practical tricks",
    "text": "Practical tricks\n\nBurn-in: discard the first few samples.\nThinning: keep only every \\(k\\)-th sample.\nSimulated annealing: start with a large variance (of proposal) and gradually reduce it.\nParallel tempering: run multiple chains with different temperatures and swap chains occasionally: for \\(1 &lt; T_1 &lt; T_2 &lt; \\cdots &lt; T_n\\), the \\(i\\)-th chain has target distribution \\[\n\\pi_i(\\theta) \\propto \\pi(\\theta)^{1/T_i} = \\exp\\left\\{\\frac{1}{T_i}\\log\\pi(\\theta)\\right\\}.\n\\]\nData augmentation: introduce latent variables to make the target distribution more tractable, i.e., find \\(\\nu\\) such that \\[\n\\pi(\\theta \\mid x) = \\int \\pi(\\theta \\mid \\xi, x) \\nu(\\xi)d\\xi.\n\\]"
  },
  {
    "objectID": "slides/07-mcmc.html#example-5",
    "href": "slides/07-mcmc.html#example-5",
    "title": "Lecture 07: Markov Chain Monte Carlo",
    "section": "Example",
    "text": "Example\nSuppose the target distribution is \\(\\pi(\\theta) = \\frac{1}{2}N(10,1) + \\frac{1}{2}N(-10, 1)\\).\n\nlog_post &lt;- function(x){\n    log(dnorm(x, 10, 1)/2 + dnorm(x, -10, 1)/2)\n}\ncurve(exp(log_post(x)), -15, 15, col = \"red\", lwd = 2)"
  },
  {
    "objectID": "slides/07-mcmc.html#vanilla-metropolis",
    "href": "slides/07-mcmc.html#vanilla-metropolis",
    "title": "Lecture 07: Markov Chain Monte Carlo",
    "section": "Vanilla Metropolis",
    "text": "Vanilla Metropolis\n\nset.seed(2023)\nM &lt;- 1000; n_chain &lt;- 4\ntheta &lt;- matrix(0, ncol = 4, nrow = M)\n\nfor (j in 1:n_chain){\n    for (i in 2:M){\n        theta.star &lt;- rnorm(1, theta[i-1,j], 1)\n        rho &lt;- exp(log_post(theta.star) - log_post(theta[i-1,j]))\n        if (runif(1) &lt; rho){\n            theta[i,j] &lt;- theta.star\n        } else {\n            theta[i,j] &lt;- theta[i-1,j]\n        }\n    }\n}"
  },
  {
    "objectID": "slides/07-mcmc.html#vanilla-metropolis-1",
    "href": "slides/07-mcmc.html#vanilla-metropolis-1",
    "title": "Lecture 07: Markov Chain Monte Carlo",
    "section": "Vanilla Metropolis",
    "text": "Vanilla Metropolis\n\npar(mfrow = c(2,2))\nfor (j in 1:n_chain){\n    plot(theta[,j], type = \"l\", col = \"blue\", lwd = 2, \n         ylim = c(-15, 15), main = paste(\"Chain\", j))\n}"
  },
  {
    "objectID": "slides/07-mcmc.html#parallel-tempering",
    "href": "slides/07-mcmc.html#parallel-tempering",
    "title": "Lecture 07: Markov Chain Monte Carlo",
    "section": "Parallel tempering",
    "text": "Parallel tempering\n\nset.seed(2023)\nM &lt;- 1000; n_chain &lt;- 4\ntheta &lt;- matrix(0, ncol = 4, nrow = M)\ntemp &lt;- c(1, 10, 20, 40); swap_int &lt;- 50\n\nfor (i in 2:M){\n    for (j in 1:n_chain){\n        theta.star &lt;- rnorm(1, theta[i-1,j], 1)\n        rho &lt;- exp(log_post(theta.star)/temp[j] -\n                       log_post(theta[i-1,j])/temp[j])\n        if (runif(1) &lt; rho){\n            theta[i,j] &lt;- theta.star\n        } else {\n            theta[i,j] &lt;- theta[i-1,j]\n        }\n    }\n    # Swap chains\n    if(i %% swap_int == 0){\n        for (j in 1:(n_chain-1)){\n            rho &lt;- exp(log_post(theta[i,j])/temp[j+1] +\n                           log_post(theta[i,j+1])/temp[j] - \n                           log_post(theta[i,j])/temp[j]-\n                           log_post(theta[i,j+1])/temp[j+1])\n            if (runif(1) &lt; rho){\n                theta[i,] &lt;- theta[i, c(j+1, j)]\n            }\n        }\n    }\n}"
  },
  {
    "objectID": "slides/07-mcmc.html#parallel-tempering-1",
    "href": "slides/07-mcmc.html#parallel-tempering-1",
    "title": "Lecture 07: Markov Chain Monte Carlo",
    "section": "Parallel tempering",
    "text": "Parallel tempering\n\npar(mfrow = c(1,2))\nplot(theta[,1], type = \"l\", col = \"blue\", lwd = 2, \n         ylim = c(-15, 15), main = paste(\"Chain\", 1))\nhist(theta[,1], freq = F, xlim = c(-15, 15), \n     ylim = c(0, 0.2), main = \"Chain 1\")\ncurve(exp(log_post(x)), -15, 15, col = \"red\", lwd = 2, add = TRUE)\n\n\n\n\nHome"
  },
  {
    "objectID": "slides/05-hierarchical_model.html#introduction",
    "href": "slides/05-hierarchical_model.html#introduction",
    "title": "Lecture 05: Connections to non-Bayesian Analysis and Hierarchical Models",
    "section": "Introduction",
    "text": "Introduction\n\nIn the previous lecture, we saw that MLE and MAP are related.\nWe will discuss this relationship in more details and explore how this relationship benefits us.\nMLE and MAP are not the only connection between Bayesian and non-Bayesian analysis.\nWe will discuss two other examples: bootstrap and random effect model."
  },
  {
    "objectID": "slides/05-hierarchical_model.html#mle-vs-map",
    "href": "slides/05-hierarchical_model.html#mle-vs-map",
    "title": "Lecture 05: Connections to non-Bayesian Analysis and Hierarchical Models",
    "section": "MLE vs MAP",
    "text": "MLE vs MAP\n\n\nGiven \\(X_1, \\ldots, X_n \\iid f(x\\mid\\theta)\\), the likelihood function is\n\n\\[\\ell(\\theta\\mid x_1,\\ldots,x_n) = \\prod_{i=1}^n f(x_i\\mid\\theta) = f(x_1,\\ldots,x_n\\mid\\theta).\\]\n\n\n\nSuppose we choose \\(\\pi(\\theta) \\propto 1\\). The posterior is1\n\n\\[\n    \\pi(\\theta \\mid x_1, \\ldots, x_n) \\propto f(x_1,\\ldots,x_n\\mid\\theta)\\pi(\\theta) = f(x_1,\\ldots,x_n\\mid\\theta).\n\\]\n\n\n\nTherefore\n\n\\[\n\\hat{\\theta}_{\\text{MAP}} = \\argmax_{\\theta}\\pi(\\theta \\mid x_1, \\ldots, x_n)\n    = \\argmax_{\\theta}\\ell(\\theta\\mid x_1,\\ldots,x_n) = \\hat{\\theta}_{\\text{MLE}}.\n\\]\n\nWe need to make sure that the posterior is proper; otherwise, this equation is meaningless."
  },
  {
    "objectID": "slides/05-hierarchical_model.html#asymptotics-of-mle",
    "href": "slides/05-hierarchical_model.html#asymptotics-of-mle",
    "title": "Lecture 05: Connections to non-Bayesian Analysis and Hierarchical Models",
    "section": "Asymptotics of MLE",
    "text": "Asymptotics of MLE\n\nLet \\(X_1, \\ldots, X_n \\iid f(x\\mid \\theta)\\) and \\(\\hat{\\theta}_n\\) be the MLE of \\(\\theta\\).\nUnder some regularity conditions on \\(f(x\\mid\\theta)\\), the MLE is asymptotically normally distributed and efficient (having the smallest variance).\nThat is, \\[\n\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta\\right) \\stackrel{d}{\\rightarrow} N\\left(0, I(\\theta)^{-1}\\right)\n\\] where \\(I(\\theta)\\) is the Fisher information of \\(f(x\\mid\\theta)\\).\nAn important application of this result is the approximate confidence interval for \\(\\theta\\) \\[\n\\P\\left(\\hat{\\theta}_n - z_{\\alpha/2}[nI(\\hat{\\theta}_n)]^{-1/2} \\leq \\theta \\leq \\hat{\\theta}_n + z_{\\alpha/2}[nI(\\hat{\\theta}_n)]^{-1/2} \\right) \\approx \\alpha.\n\\]"
  },
  {
    "objectID": "slides/05-hierarchical_model.html#asymptotics-of-bayesian-inference",
    "href": "slides/05-hierarchical_model.html#asymptotics-of-bayesian-inference",
    "title": "Lecture 05: Connections to non-Bayesian Analysis and Hierarchical Models",
    "section": "Asymptotics of Bayesian inference",
    "text": "Asymptotics of Bayesian inference\n\nThe Bernstein-von Mises Theorem states that under some regularity conditions, the posterior distribution is asymptotically normal centered at the MLE.\nSee Section 10.2 of Van der Vaart (2000) for rigorous arguments.\nThe regularity conditions include:\n\nthe posterior must be proper, and\nthe prior is (strictly) positive in a neighborhood around the true parameter (this is a strong condition in high dimensions), etc.\n\nRoughly speaking, the Bernstein-von Mises theorem works well on a problem with few parameters and large dataset containing iid samples. For complicated problems like nonparametric problems, it might not work as well as you expect1.\n\n\n\nVan der Vaart, A. W. (2000). Asymptotic statistics.\nSee this blog post for an interesting discussion on Bernstein-von Mises Theoerm."
  },
  {
    "objectID": "slides/05-hierarchical_model.html#normal-approximation-to-the-posterior",
    "href": "slides/05-hierarchical_model.html#normal-approximation-to-the-posterior",
    "title": "Lecture 05: Connections to non-Bayesian Analysis and Hierarchical Models",
    "section": "Normal approximation to the posterior",
    "text": "Normal approximation to the posterior\n\nIf the posterior \\(\\pi(\\theta\\mid x)\\) is unimodal and roughly symmetric, it can be convenient to approximate it by a normal distribution.\nThat is, the logarithm of the posterior density is approximated by a quadratic function of \\(\\theta\\).\nConsider Taylor series expansion of \\(\\log \\pi(\\theta\\mid x)\\) centered at the posterior mode (MAP) \\(\\hat{\\theta}\\) \\[\n\\log \\pi(\\theta \\mid x)=\\log \\pi(\\hat{\\theta} \\mid x)+\\frac{1}{2}(\\theta-\\hat{\\theta})^T\\left[\\frac{d^2}{d \\theta^2} \\log \\pi(\\theta \\mid x)\\right]_{\\theta=\\hat{\\theta}}(\\theta-\\hat{\\theta})+\\cdots\n\\]\nThat is, \\[\\begin{align*}\n\\pi(\\theta \\mid x) \\approx N\\left(\\hat{\\theta},[I(\\hat{\\theta})]^{-1}\\right)\n\\end{align*}\\] where \\(I(\\theta)\\) is the observed information, \\(I(\\theta)=-\\frac{d^2}{d \\theta^2} \\log \\pi(\\theta \\mid x)\\)."
  },
  {
    "objectID": "slides/05-hierarchical_model.html#normal-approximation-to-the-posterior-1",
    "href": "slides/05-hierarchical_model.html#normal-approximation-to-the-posterior-1",
    "title": "Lecture 05: Connections to non-Bayesian Analysis and Hierarchical Models",
    "section": "Normal approximation to the posterior",
    "text": "Normal approximation to the posterior\n\nIf the approximation is good, we can construct an approximate credible interval without resorting to sampling techniques.\nIn practice, however, it is almost impossible to check how good or bad the approximation is, especially when the parameter is high-dimensional.\nEven for low-dimensional parameter, if the posterior is multi-modal, the normal approximation will be terrible.\nIn a later lecture, we will introduce a technique, called variational inference, which allows us to approximate the posterior using any parametric family, not just normal."
  },
  {
    "objectID": "slides/05-hierarchical_model.html#bootstrap-and-bayesian-inference2",
    "href": "slides/05-hierarchical_model.html#bootstrap-and-bayesian-inference2",
    "title": "Lecture 05: Connections to non-Bayesian Analysis and Hierarchical Models",
    "section": "Bootstrap and Bayesian Inference1",
    "text": "Bootstrap and Bayesian Inference1\n\nBootstrap is a method proposed by Efron (1979)2 for deriving the sampling distribution of a statistic.\nIt is based on resampling from the empirical distribution, which converges to the true population distribution (Glivenko-Cantelli theorem).\nExample: \\(X_1, \\ldots, X_n \\iid F\\) and \\(\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i\\).\nA bootstrap sample \\(X_1^*, \\ldots, X_n^*\\) is obtained from \\(X_1, \\ldots, X_n\\) using sampling with replacement.\nThe sampling distribution of \\(\\bar{X}_n\\) can be estimated/approximated by the distribution of \\(\\bar{X}^*_n = \\frac{1}{n}\\sum_{i=1}^n X_i^*\\), which is called the bootstrap distribution.\n\nCh. 8.4 in Hastie, T., Tibshirani, R., Friedman, J. H., & Friedman, J. H. (2009). The elements of statistical learning: data mining, inference, and prediction.B. Efron. (1979) Bootstrap Methods: Another Look at the Jackknife. Annals of Statistics 7(1) 1 - 26."
  },
  {
    "objectID": "slides/05-hierarchical_model.html#bootstrap-and-bayesian-inference",
    "href": "slides/05-hierarchical_model.html#bootstrap-and-bayesian-inference",
    "title": "Lecture 05: Connections to non-Bayesian Analysis and Hierarchical Models",
    "section": "Bootstrap and Bayesian Inference",
    "text": "Bootstrap and Bayesian Inference\n\nThere are some similarities between bootstrap and Bayesian inference:\n\nThey both treat the sample as fixed and given.\nBased on the sample, they both construct a distribution: bootstrap distribution vs posterior distribution.\n\nWhen will they be (approximately) the same?\nSuppose \\(X_1, \\ldots, X_n \\mid p \\iid \\text{Ber}(p)\\) and \\(p \\sim \\text{Beta}(\\alpha, \\beta)\\). Then \\[\np \\sim \\text{Beta}\\left(\\alpha + \\sum X_i, \\beta + n - \\sum X_i\\right).\n\\]\nLetting \\(\\alpha \\to 0\\) and \\(\\beta \\to 0\\), we have a noninformative prior and \\[\np \\mid X_1,\\ldots, X_n \\sim \\text{Beta}\\left(\\sum X_i, n-\\sum X_i\\right).\n\\]"
  },
  {
    "objectID": "slides/05-hierarchical_model.html#bootstrap-and-bayesian-inference-1",
    "href": "slides/05-hierarchical_model.html#bootstrap-and-bayesian-inference-1",
    "title": "Lecture 05: Connections to non-Bayesian Analysis and Hierarchical Models",
    "section": "Bootstrap and Bayesian Inference",
    "text": "Bootstrap and Bayesian Inference\n\nConsider a bootstrap sample \\(X_1^*, \\ldots, X_n^*\\) (obtained from \\(X_1,\\ldots,X_n\\) using sampling with replacement).\nActually, \\(X_1^*,\\ldots, X_n^*\\mid X_1,\\ldots, X_n \\iid \\text{Ber}(\\hat{p})\\), where \\(\\hat{p} = \\frac{1}{n}\\sum X_i\\).\nLet \\(\\hat{p}^* = \\frac{1}{n}\\sum X_i^*\\). What is the bootstrap distribution \\(\\hat{p}^* \\mid X_1,\\ldots, X_n\\)?\nLet’s do some experiment!"
  },
  {
    "objectID": "slides/05-hierarchical_model.html#bootstrap-and-bayesian-inference-2",
    "href": "slides/05-hierarchical_model.html#bootstrap-and-bayesian-inference-2",
    "title": "Lecture 05: Connections to non-Bayesian Analysis and Hierarchical Models",
    "section": "Bootstrap and Bayesian Inference",
    "text": "Bootstrap and Bayesian Inference\n\nset.seed(124)\np &lt;- 0.4; n &lt;- 100; X &lt;- rbinom(n, 1, p)\np_hat &lt;- mean(X)\n\n# bootstrap\nB &lt;- 10^3\np_boot &lt;- rep(0, B)\nfor(i in 1:B){\n    X_boot &lt;- sample(X, size = n, replace = TRUE)\n    p_boot[i] &lt;- mean(X_boot)\n}\n\nhist(p_boot, freq = FALSE, main = \"Bootstrap Distribution\", xlab = \"p\")\ncurve(dbeta(x, n*p_hat, n*(1-p_hat)), add = T, col = \"red\")\nabline(v = p, lty = 2, col = \"green\", lwd = 4)\nabline(v = p_hat, lty = 2, col = \"blue\", lwd = 4)"
  },
  {
    "objectID": "slides/05-hierarchical_model.html#bootstrap-and-bayesian-inference-2-output",
    "href": "slides/05-hierarchical_model.html#bootstrap-and-bayesian-inference-2-output",
    "title": "Lecture 05: Connections to non-Bayesian Analysis and Hierarchical Models",
    "section": "Bootstrap and Bayesian Inference",
    "text": "Bootstrap and Bayesian Inference\n\n\n\n\nThe red curve is the posterior distribution; the blue dashed line is the MLE."
  },
  {
    "objectID": "slides/05-hierarchical_model.html#bootstrap-and-bayesian-inference-3",
    "href": "slides/05-hierarchical_model.html#bootstrap-and-bayesian-inference-3",
    "title": "Lecture 05: Connections to non-Bayesian Analysis and Hierarchical Models",
    "section": "Bootstrap and Bayesian Inference",
    "text": "Bootstrap and Bayesian Inference\n\nIn this sense, the bootstrap distribution represents an (approximate) nonparametric posterior distribution under a noninformative prior.\nBut this bootstrap distribution is obtained painlessly – without having to formally specify a prior and without having to sample from the posterior distribution.\nHence we might think of the bootstrap distribution as a “poor man’s” posterior."
  },
  {
    "objectID": "slides/05-hierarchical_model.html#distributional-estimates",
    "href": "slides/05-hierarchical_model.html#distributional-estimates",
    "title": "Lecture 05: Connections to non-Bayesian Analysis and Hierarchical Models",
    "section": "Distributional estimates",
    "text": "Distributional estimates\n\nOne of the advantages of Bayesian inference is that the entire inference is based the posterior distribution, rather than a single point estimate or an interval.\nThe posterior distribution is just one example of distributional estimate, which use a distribution for inference.\nBootstrap distribution is another example.\nThere is a frequentist concept, called the confidence distribution, proposed by Neyman (1937)1.\nThe confidence distribution contains confidence interval of any level.\nFraser (2011)2 also discussed how the Bayesian posterior and the confidence distribution are related.\n\nNeyman, J. (1937). Outline of a theory of statistical estimation based on the classical theory of probability. Phil. Trans. Roy. SocFraser, D. A. S. (2011). Is Bayes Posterior just Quick and Dirty Confidence? Statistical Science, 26(3), 299–316."
  },
  {
    "objectID": "slides/05-hierarchical_model.html#random-effect-models",
    "href": "slides/05-hierarchical_model.html#random-effect-models",
    "title": "Lecture 05: Connections to non-Bayesian Analysis and Hierarchical Models",
    "section": "Random effect models",
    "text": "Random effect models\n\nFixed effect model: \\(Y_i = \\beta_0 + \\beta X_i + \\varepsilon_i\\), \\(\\varepsilon_i \\iid N(0, \\sigma^2)\\)\n\nThe parameter \\(\\beta\\) is the fixed effect of the factors \\(X\\).\nWith a unit increase in \\(X\\), \\(Y\\) is expected to increase by \\(\\beta\\).\nUse (regularized) least squares to obtain an estimate for \\(\\beta\\).\nThe effect is the same for each observation\n\nRandom effect model: \\(Y_i = \\beta_0 + \\gamma_iX_i + \\varepsilon_i\\), \\(\\gamma_i \\sim N(\\beta, \\tau^2)\\), \\(\\varepsilon_i \\iid N(0, \\sigma^2)\\)\n\n\\(\\gamma_i\\) is the random effect in this model\nThe effect is different for each observation and, on average, \\(Y\\) is increase by \\(\\beta\\) with a unit increase in \\(X\\)."
  },
  {
    "objectID": "slides/05-hierarchical_model.html#mixed-effect-model",
    "href": "slides/05-hierarchical_model.html#mixed-effect-model",
    "title": "Lecture 05: Connections to non-Bayesian Analysis and Hierarchical Models",
    "section": "Mixed effect model",
    "text": "Mixed effect model\n\nA model with both fixed effects and random effects is called a mixed effect model.\nThis model is commonly used in biostatistics, psychology, economics, etc.\nA typical mixed effect model: \\[\nY = X\\beta + Z\\gamma + \\varepsilon, \\quad \\varepsilon \\sim N(0, \\sigma^2)  \n\\] where \\(\\beta\\) is the unknown fixed effect and \\(\\gamma \\sim N(\\nu, \\Psi)\\) is the unknown random effect."
  },
  {
    "objectID": "slides/05-hierarchical_model.html#data-description",
    "href": "slides/05-hierarchical_model.html#data-description",
    "title": "Lecture 05: Connections to non-Bayesian Analysis and Hierarchical Models",
    "section": "Data description",
    "text": "Data description\nWe use the sleepstudy dataset in the lme4 R package.\n\nThis dataset contains the average reaction time per day (in milliseconds) for subjects in a sleep deprivation study described in Belenky et al. (2003)1, for the most sleep-deprived group (3 hours time-in-bed; 18 subjects) and for the first 10 days of the study. Days 0-1 were adaptation and training (T1/T2), and day 2 was baseline (B); sleep deprivation started after day 2.\n\nGregory Belenky, Nancy J. Wesensten, David R. Thorne, Maria L. Thomas, Helen C. Sing, Daniel P. Redmond, Michael B. Russo and Thomas J. Balkin (2003) Patterns of performance degradation and restoration during sleep restriction and subsequent recovery: a sleep dose-response study. Journal of Sleep Research 12, 1–12."
  },
  {
    "objectID": "slides/05-hierarchical_model.html#data-description-1",
    "href": "slides/05-hierarchical_model.html#data-description-1",
    "title": "Lecture 05: Connections to non-Bayesian Analysis and Hierarchical Models",
    "section": "Data description",
    "text": "Data description\n\nlibrary(lme4)\nlibrary(tidyverse)\nlibrary(kableExtra)\ndata(\"sleepstudy\")\nsleepstudy |&gt; head() |&gt; kbl(format=\"markdown\")\n\n\n\n\nReaction\nDays\nSubject\n\n\n\n\n249.5600\n0\n308\n\n\n258.7047\n1\n308\n\n\n250.8006\n2\n308\n\n\n321.4398\n3\n308\n\n\n356.8519\n4\n308\n\n\n414.6901\n5\n308"
  },
  {
    "objectID": "slides/05-hierarchical_model.html#first-attempt-simple-linear-regression",
    "href": "slides/05-hierarchical_model.html#first-attempt-simple-linear-regression",
    "title": "Lecture 05: Connections to non-Bayesian Analysis and Hierarchical Models",
    "section": "First attempt: simple linear regression",
    "text": "First attempt: simple linear regression\n\nLet \\(Y\\) be the reaction time and \\(X\\) be the Day variable.\nWe want to see the negative impact of consecutive sleep loss in reaction time.\nA simple linear model: \\(Y = \\beta_0 + \\beta_1 X\\).\n\n\n\nlibrary(sjPlot)\nsleepstudy |&gt;\n    lm(Reaction ~ Days, data = _, subset=Days&gt;=2) |&gt;\n    tab_model()\n\n\n\n\n \nReaction\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n245.10\n223.31 – 266.88\n&lt;0.001\n\n\nDays\n11.44\n7.78 – 15.09\n&lt;0.001\n\n\nObservations\n144\n\n\nR2 / R2 adjusted\n0.212 / 0.207"
  },
  {
    "objectID": "slides/05-hierarchical_model.html#individual-difference",
    "href": "slides/05-hierarchical_model.html#individual-difference",
    "title": "Lecture 05: Connections to non-Bayesian Analysis and Hierarchical Models",
    "section": "Individual difference",
    "text": "Individual difference\nHowever, this model does not take into account the individual difference, i.e., the difference between subjects."
  },
  {
    "objectID": "slides/05-hierarchical_model.html#second-attempt-mixed-effect-model",
    "href": "slides/05-hierarchical_model.html#second-attempt-mixed-effect-model",
    "title": "Lecture 05: Connections to non-Bayesian Analysis and Hierarchical Models",
    "section": "Second attempt: Mixed effect model",
    "text": "Second attempt: Mixed effect model\n\nWe allow each subject to have its own intercept and effect: \\[\n    Y_{ij} = b_{0,i} + b_{1,i} X_{j}, \\quad i = 1,\\ldots, 18, j = 0,\\ldots, 9\n\\] where \\(Y_{ij}\\) is the reaction time of \\(i\\)th subject in Day \\(j\\).\nNote that we are not interested in the effects for each individuals.\nWe just want to identify how much the individual difference contributes to the un-explained variation of the model.\nWe assume that \\(b_{0,i} \\iid N(\\beta_0, \\tau_0^2)\\) and \\(b_{1,i} \\iid N(\\beta_1, \\tau_1^2)\\) are independent.\nThe frequentist interpretation of this assumption is that the subjects are randomly selected, so there is randomness in the individual coefficients.\nThe parameters \\(\\beta_0\\) and \\(\\beta_1\\) are unknown and fixed."
  },
  {
    "objectID": "slides/05-hierarchical_model.html#second-attempt-mixed-effect-model-1",
    "href": "slides/05-hierarchical_model.html#second-attempt-mixed-effect-model-1",
    "title": "Lecture 05: Connections to non-Bayesian Analysis and Hierarchical Models",
    "section": "Second attempt: Mixed effect model",
    "text": "Second attempt: Mixed effect model\n\nsleepstudy |&gt;\n    lmer(Reaction ~ Days + (1|Subject) + (0+Days|Subject), \n         data = _, subset=Days&gt;=2) |&gt;\n    tab_model(show.icc = FALSE,show.obs = FALSE, show.ngroups = FALSE)\n\n\n\n\n \nReaction\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n245.10\n227.71 – 262.49\n&lt;0.001\n\n\nDays\n11.44\n7.97 – 14.90\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n662.92\n\n\n\nτ00 Subject\n831.93\n\n\nτ11 Subject.Days\n39.50\n\n\nρ01\n \n\n\nρ01\n \n\nMarginal R2 / Conditional R2\n0.316 / 0.697"
  },
  {
    "objectID": "slides/05-hierarchical_model.html#bayesian-approach-to-mixed-effect-models",
    "href": "slides/05-hierarchical_model.html#bayesian-approach-to-mixed-effect-models",
    "title": "Lecture 05: Connections to non-Bayesian Analysis and Hierarchical Models",
    "section": "Bayesian approach to mixed effect models",
    "text": "Bayesian approach to mixed effect models\n\nThe random effect assumption looks like a prior distribution on the coefficients.\nA mixed model \\[\nY = X\\beta + Z\\gamma + \\varepsilon, \\quad\\gamma \\sim N(\\nu, \\Psi), \\varepsilon \\sim N(0, \\sigma^2)  \n\\] can be rewritten as \\[\\begin{align*}\n    Y \\mid \\gamma & \\sim N(X\\beta + Z\\gamma, \\sigma^2)\\\\\n    \\gamma & \\sim N(\\nu, \\Psi)\n\\end{align*}\\]\nA Bayesian approach puts priors on the parameters: \\[\\begin{align*}\n    Y \\mid \\gamma, \\beta, \\sigma^2 & \\sim N(X\\beta + Z\\gamma, \\sigma^2)\\\\\n    \\gamma \\mid \\nu, \\Psi & \\sim N(\\nu, \\Psi)\\\\\n    \\beta, \\nu, \\sigma^2, \\Psi & \\sim \\pi(\\beta, \\nu, \\sigma^2, \\Psi)\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/05-hierarchical_model.html#bayesian-approach-to-mixed-effect-models-1",
    "href": "slides/05-hierarchical_model.html#bayesian-approach-to-mixed-effect-models-1",
    "title": "Lecture 05: Connections to non-Bayesian Analysis and Hierarchical Models",
    "section": "Bayesian approach to mixed effect models",
    "text": "Bayesian approach to mixed effect models\nUse stan_lmer to fit a Bayesian mixed effect model\n\nlibrary(rstanarm)\nsleep_lmer_B &lt;- stan_lmer(Reaction ~ Days + (1|Subject) + (0+Days|Subject), \n                          data = sleepstudy)\n\n\nUse summary(sleep_lmer_B) to check the fitted result\nUse prior_summary(sleep_lmer_B) to check the priors used in the model"
  },
  {
    "objectID": "slides/05-hierarchical_model.html#wrap-up",
    "href": "slides/05-hierarchical_model.html#wrap-up",
    "title": "Lecture 05: Connections to non-Bayesian Analysis and Hierarchical Models",
    "section": "Wrap-up",
    "text": "Wrap-up\n\nRandom effects are also called variance components, since we are more interested in the variance of the random effect rather than the effect itself.\nMixed effect model is just one example of hierarchical models (or multi-level models).\nA hierarchical model contains many sub-models that are linked by some hierarchical relations.\nIn the previous example, we can partition the dataset into smaller pieces by subject and hence subject is a hierarchy in this dataset.\nNote that a hierarchical model need not be a Bayesian model. However, Bayesian approach is a natural way to fit a hierarchical model."
  },
  {
    "objectID": "slides/05-hierarchical_model.html#motivation",
    "href": "slides/05-hierarchical_model.html#motivation",
    "title": "Lecture 05: Connections to non-Bayesian Analysis and Hierarchical Models",
    "section": "Motivation",
    "text": "Motivation\n\nSome problems have intrinsic hierarchical structures, e.g., samples are grouped by some covariates (gender, ethnicity, etc.).\nIn practice, simple nonhierarchical models are usually inappropriate for hierarchical data: with few parameters, they generally cannot fit large datasets accurately, whereas with many parameters, they tend to ‘overfit’.\nWhen parameters are only exchangeable rather than independent, a hyperprior is needed (de Finetti’s Theorem). That is, the subscripts of the parameters convey no information."
  },
  {
    "objectID": "slides/05-hierarchical_model.html#meta-analysis",
    "href": "slides/05-hierarchical_model.html#meta-analysis",
    "title": "Lecture 05: Connections to non-Bayesian Analysis and Hierarchical Models",
    "section": "Meta-analysis",
    "text": "Meta-analysis\n\nMeta-analysis is an increasingly popular and important process of summarizing and integrating the findings of research studies in a particular area.\nThe article “Why Science Is Not Necessarily Self-Correcting” by Ioannidis (2012) pointed out that sometimes fundamental fallacies remain unchallenged and are only perpetuated.\nYou can find a detailed introduction to meat-analysis and different approaches to meta-analysis in https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/.\n\n\n\nIoannidis, J. P. (2012). Why science is not necessarily self-correcting. Perspectives on Psychological Science, 7(6), 645-654."
  },
  {
    "objectID": "slides/05-hierarchical_model.html#data-description-2",
    "href": "slides/05-hierarchical_model.html#data-description-2",
    "title": "Lecture 05: Connections to non-Bayesian Analysis and Hierarchical Models",
    "section": "Data description",
    "text": "Data description\n\nBeta-blocker is a family of drugs that affect the central nervous system and can relax the heart muscles.\nThis dataset summarizes mortality after myocardial infarction in 22 clinical trials.\nEach clinical trial consists of two groups of heart attack patients randomly allocated to receive or not receive beta-blockers.\nThe dataset is available at http://www.stat.columbia.edu/~gelman/book/data/meta.asc.\nThe aim of a meta-analysis is to provide a combined analysis of the studies that indicates the overall strength of the evidence for a beneficial effect of the treatment under study."
  },
  {
    "objectID": "slides/05-hierarchical_model.html#data-description-3",
    "href": "slides/05-hierarchical_model.html#data-description-3",
    "title": "Lecture 05: Connections to non-Bayesian Analysis and Hierarchical Models",
    "section": "Data description",
    "text": "Data description\n\nmeta &lt;- read.table(\"http://www.stat.columbia.edu/~gelman/book/data/meta.asc\", \n                  header = TRUE, skip = 4)\nmeta |&gt; head() |&gt; kbl()\n\n\n\n\nstudy\ncontrol.deaths\ncontrol.total\ntreated.deaths\ntreated.total\n\n\n\n\n1\n3\n39\n3\n38\n\n\n2\n14\n116\n7\n114\n\n\n3\n11\n93\n5\n69\n\n\n4\n127\n1520\n102\n1533\n\n\n5\n27\n365\n28\n355\n\n\n6\n6\n52\n4\n59"
  },
  {
    "objectID": "slides/05-hierarchical_model.html#odds-ratio",
    "href": "slides/05-hierarchical_model.html#odds-ratio",
    "title": "Lecture 05: Connections to non-Bayesian Analysis and Hierarchical Models",
    "section": "Odds ratio",
    "text": "Odds ratio\n\nThe odds ratio is the ratio of the odds death in the treatment group and the odds of death in the control group \\[\n\\rho = \\left. \\frac{p_{\\text{treat}}}{1-p_{\\text{treat}}} \\middle/ \\frac{p_{\\text{con}}}{1-p_{\\text{con}}} \\right. .\n\\]\nWe concentrate the inference on the log of the odds ratios \\(\\theta = \\log \\rho\\) for each clinical trial."
  },
  {
    "objectID": "slides/05-hierarchical_model.html#normal-approximation-to-the-likelihood",
    "href": "slides/05-hierarchical_model.html#normal-approximation-to-the-likelihood",
    "title": "Lecture 05: Connections to non-Bayesian Analysis and Hierarchical Models",
    "section": "Normal approximation to the likelihood",
    "text": "Normal approximation to the likelihood\n\nLet \\(y_{ij}\\) and \\(n_{ij}\\) be the number of deaths and subjects of the \\(i\\)th group (\\(1=\\) treated, \\(0 =\\) control) in study \\(j\\).\nFor each study \\(j\\), we estimate \\(\\theta_j\\) by \\[\n\\hat{\\theta}_j=\\log \\left(\\frac{y_{1 j}}{n_{1 j}-y_{1 j}}\\right)-\\log \\left(\\frac{y_{0 j}}{n_{0 j}-y_{0 j}}\\right),\n\\] with approximate sampling variance \\[\n\\sigma_j^2=\\frac{1}{y_{1 j}}+\\frac{1}{n_{1 j}-y_{1 j}}+\\frac{1}{y_{0 j}}+\\frac{1}{n_{0 j}-y_{0 j}}.\n\\]\nThus \\(\\hat{\\theta}_j \\sim N(\\theta_j, \\sigma_j^2)\\) approximately."
  },
  {
    "objectID": "slides/05-hierarchical_model.html#odds-ratio-1",
    "href": "slides/05-hierarchical_model.html#odds-ratio-1",
    "title": "Lecture 05: Connections to non-Bayesian Analysis and Hierarchical Models",
    "section": "Odds ratio",
    "text": "Odds ratio\n\nmeta_summary &lt;- meta |&gt;\n    mutate(treated.odds = treated.deaths/(treated.total - treated.deaths),\n           control.odds = control.deaths/(control.total - control.deaths),\n           log.odds.ratio = log(treated.odds) - log(control.odds), \n           var = 1/treated.deaths + 1/(treated.total - treated.deaths) + \n               1/control.deaths + 1/(control.total - control.deaths)) |&gt;\n    select(c(study, log.odds.ratio, var)) \nmeta_summary |&gt; head() |&gt; kbl(digits = 4)\n\n\n\n\nstudy\nlog.odds.ratio\nvar\n\n\n\n\n1\n0.0282\n0.7230\n\n\n2\n-0.7410\n0.2334\n\n\n3\n-0.5406\n0.3187\n\n\n4\n-0.2461\n0.0191\n\n\n5\n0.0695\n0.0788\n\n\n6\n-0.5842\n0.4566"
  },
  {
    "objectID": "slides/05-hierarchical_model.html#goal-of-the-inference",
    "href": "slides/05-hierarchical_model.html#goal-of-the-inference",
    "title": "Lecture 05: Connections to non-Bayesian Analysis and Hierarchical Models",
    "section": "Goal of the inference",
    "text": "Goal of the inference\n\nNow we have the log odds ratios and their sampling variances from 22 studies.\nHow are these studies related to each other?\n\nAre they identical replications of each other, i.e., the studies are independent samples from a common population?\nAre they completely unrelated?\n\nA more realistic assumption is that they are somewhere between the two extremes.\nExchangeablity assumption: the parameters \\(\\theta_j\\) are exchangeable.\n\nWe cannot distinguish \\(\\theta_j\\)’s before we see that data.\nTherefore \\[\n(\\theta_1, \\ldots, \\theta_{22}) \\sim \\pi \\Leftrightarrow\n\\theta_i \\iid \\pi(\\theta\\mid\\phi), \\phi \\sim \\nu.\n\\]"
  },
  {
    "objectID": "slides/05-hierarchical_model.html#a-hierarchical-model",
    "href": "slides/05-hierarchical_model.html#a-hierarchical-model",
    "title": "Lecture 05: Connections to non-Bayesian Analysis and Hierarchical Models",
    "section": "A hierarchical model",
    "text": "A hierarchical model\n\nLikelihood: \\(\\hat{\\theta}_j \\sim N(\\theta_j, \\sigma^2_j)\\), \\(j = 1,\\ldots, 22\\).\n\nAssume the \\(\\sigma^2_j\\)’s are known (given by the sampling variance computed before).\n\nPrior: we use the exchangeable normal prior for \\(\\theta_j\\), \\[\n\\theta_j \\mid \\mu, \\tau \\iid N(\\mu, \\tau^2)\n\\] where \\(\\mu\\) and \\(\\tau\\) are unknown hyperparameters.\nHyperprior: for \\(\\mu\\) and \\(\\tau\\) \\[\\begin{align*}\n\\mu & \\sim \\pi(\\mu) \\propto 1\\\\\n\\tau & \\sim \\text{Unif}(0, A)\n\\end{align*}\\] for some large \\(A\\).\nComputation is left as homework."
  },
  {
    "objectID": "slides/05-hierarchical_model.html#weakly-informative-priors-for-variance-parameters",
    "href": "slides/05-hierarchical_model.html#weakly-informative-priors-for-variance-parameters",
    "title": "Lecture 05: Connections to non-Bayesian Analysis and Hierarchical Models",
    "section": "Weakly informative priors for variance parameters",
    "text": "Weakly informative priors for variance parameters\n\nFor the variance parameter \\(\\tau\\), we use the uniform prior \\(\\text{Unif}(0, A)\\), which is a proper noninformative prior.\nAnother choice is Inverse-Gamma\\((\\epsilon, \\epsilon)\\), with small \\(\\epsilon\\).\nHow about the Jeffreys prior?\n\nTo compute the Jeffreys prior, you need to first find the marginal distribution \\(f(x\\mid \\mu, \\tau) = \\int f(x \\mid \\theta) \\pi(\\theta \\mid \\mu, \\tau) d\\theta\\).\nThen compute the Fisher information of \\(f(x \\mid \\mu, \\tau)\\).\nBoth steps are difficult!!\n\nA bad news is that the posterior of \\(\\tau\\) is sensitive to the choice of prior. You will see this in your homework.\nRead Section 5.7 of BDA for more details.\n\n\n\nHome"
  },
  {
    "objectID": "slides/03-prior.html#introduction",
    "href": "slides/03-prior.html#introduction",
    "title": "Lecture 03: From Prior Information to Prior Distribution",
    "section": "Introduction",
    "text": "Introduction\n\n\\[\n\\newcommand{\\mc}[1]{\\mathcal{#1}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\E}{\\mathbb{E}}\n\\renewcommand{\\P}{\\mathbb{P}}\n\\newcommand{\\var}{{\\rm Var}} % Variance\n\\newcommand{\\mse}{{\\rm MSE}} % MSE\n\\newcommand{\\bias}{{\\rm Bias}} % MSE\n\\newcommand{\\cov}{{\\rm Cov}} % Covariance\n\\newcommand{\\iid}{\\stackrel{\\rm iid}{\\sim}}\n\\newcommand{\\ind}{\\stackrel{\\rm ind}{\\sim}}\n\\renewcommand{\\choose}[2]{\\binom{#1}{#2}}  % Choose\n\\newcommand{\\chooses}[2]{{}_{#1}C_{#2}}  % Small choose\n\\newcommand{\\cd}{\\stackrel{d}{\\rightarrow}}\n\\newcommand{\\cas}{\\stackrel{a.s.}{\\rightarrow}}\n\\newcommand{\\cp}{\\stackrel{p}{\\rightarrow}}\n\\newcommand{\\bin}{{\\rm Bin}}\n\\newcommand{\\ber}{{\\rm Ber}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\]\n\n\nThe choice of prior distribution is the most critical and difficult step in Bayesian analysis.\nIn practice, the available prior information is often not precise enough to lead to an exact determination of the prior distribution.\nWe need to find a prior distribution that is compatible with the prior information.\nAnother problem is the determination of prior when the prior information is too vague or does not even exist."
  },
  {
    "objectID": "slides/03-prior.html#prior-information",
    "href": "slides/03-prior.html#prior-information",
    "title": "Lecture 03: From Prior Information to Prior Distribution",
    "section": "Prior Information",
    "text": "Prior Information\n\nSuppose the prior information for \\(\\theta\\) is given through some expectations. \\[\n\\E_{\\pi}\\left[g_k(\\theta)\\right]=\\omega_k, \\quad k = 1, \\ldots, K,\n\\]\nFor example, the census records show that the sex ratio at birth is around 1.05 (ranging from 1.03 to 1.07 for most countries).\nThe prior information about the sex ratio \\(\\theta\\) can be expressed as \\[\n    \\E_{\\pi}(\\theta) = 1.05 \\quad \\text{and} \\quad \\var_{\\pi}(\\theta) = 0.01^2.\n\\]\nObviously, there are many distributions compatible with these constraints.\nWe will discuss two approaches for choosing prior distributions compatible with these information."
  },
  {
    "objectID": "slides/03-prior.html#intuition-behind-entropy",
    "href": "slides/03-prior.html#intuition-behind-entropy",
    "title": "Lecture 03: From Prior Information to Prior Distribution",
    "section": "Intuition behind entropy",
    "text": "Intuition behind entropy\n\nLet \\(\\pi\\) be a discrete distribution. The Shannon Entropy1 of \\(\\pi\\) is \\[\n\\mc{E}(\\pi) =  -\\E_{\\pi}[\\log \\pi(\\theta)].\n\\]\nEntropy is a measure of randomness: high entropy = high randomness\nEntropy is the expectation of information: \\(\\mc{E}(\\pi) = \\E_{\\pi}(I(X))\\), where \\(I(x)\\) is the information of the event \\(\\{X = x\\}\\).\nRoughly speaking, information of an event \\(E\\) is the knowledge you obtained after the occurrence of \\(E\\) .\n\nIntroduced by Shannon (1948)."
  },
  {
    "objectID": "slides/03-prior.html#shannons-axioms-for-information",
    "href": "slides/03-prior.html#shannons-axioms-for-information",
    "title": "Lecture 03: From Prior Information to Prior Distribution",
    "section": "Shannon’s Axioms for Information",
    "text": "Shannon’s Axioms for Information\n\nAn event that always happens yields no information.\nThe less probable an event is, the more information it yields.\nIf two independent events are measured separately, the total amount of information is the sum of the information of the individual events.\n\n\nThat is, the information \\(I(A)\\) of an event \\(A\\) satisfies\n\n\n\\(I(A) = 0\\) if \\(\\P(A) = 1\\).\n\\(I(A)\\) is a decreasing function of \\(\\P(A)\\).\n\\(I(A\\cap B) = I(A) + I(B)\\) if \\(A\\) and \\(B\\) are independent.\n\n\n Shannon’s solution: \\(I(A) = -\\log \\P(A)\\) for \\(\\P(A) &gt; 0\\)"
  },
  {
    "objectID": "slides/03-prior.html#shannon-entropy",
    "href": "slides/03-prior.html#shannon-entropy",
    "title": "Lecture 03: From Prior Information to Prior Distribution",
    "section": "Shannon Entropy",
    "text": "Shannon Entropy\n\nLet \\(X\\) be a discrete random variable with probability mass function \\(\\pi(x)\\). The Shannon information of \\(\\pi\\) is \\[\nI(x) \\coloneqq I(\\{X = x\\}) = -\\log \\P(X = x) = -\\log \\pi(x).\n\\]\nThe Shannon entropy of \\(\\pi\\) is \\(\\mc{E}(\\pi) = \\E_{\\pi}(I(X)) = -\\E_{\\pi}(\\log \\pi(X))\\).\nIf the support of \\(\\pi(x)\\) is a finite set, say \\(\\{x_1,\\ldots, x_m\\}\\), then \\(\\mc{E}(\\pi) \\leq \\log m\\) and the equality holds when \\(\\pi(x) = \\frac{1}{m}\\).\nWhat if \\(X\\) is continuous? If \\(\\pi(x)\\) is a probability density, the information \\(-\\log \\pi(x)\\) can be negative."
  },
  {
    "objectID": "slides/03-prior.html#relative-entropy",
    "href": "slides/03-prior.html#relative-entropy",
    "title": "Lecture 03: From Prior Information to Prior Distribution",
    "section": "Relative Entropy",
    "text": "Relative Entropy\n\nIt does not make sense to have a negative information.\nBut ``relative information” can be negative.\nLet \\(\\pi_0\\) be a reference distribution. The relative information of \\(\\pi\\) with respect to \\(\\pi_0\\) is \\[\nI(x; \\pi_0) \\coloneqq \\log \\frac{1}{\\pi_0(x)} - \\log \\frac{1}{\\pi(x)} = \\log \\frac{\\pi(x)}{\\pi_0(x)}\n\\]\nThen by Jensen’s inequality \\[\n\\E_{\\pi_0}[I(X;\\pi_0)] = \\mc{E}(\\pi_0) - \\mc{E}(\\pi, \\pi_0) \\leq 0\n\\] and the equality holds when \\(\\pi = \\pi_0\\)."
  },
  {
    "objectID": "slides/03-prior.html#relative-entropy-1",
    "href": "slides/03-prior.html#relative-entropy-1",
    "title": "Lecture 03: From Prior Information to Prior Distribution",
    "section": "Relative Entropy",
    "text": "Relative Entropy\n\nCross entropy: \\(\\mc{E}(\\pi, \\pi_0) = - \\E_{\\pi_0}(\\log \\pi(X))\\)\nRelative entropy: \\[\n\\mc{E}(\\pi_0\\| \\pi) = - \\E_{\\pi_0}[I(X;\\pi_0)] = \\E_{\\pi_0}\\left[\\log \\frac{\\pi_0(X)}{\\pi(X)}\\right] = D_{KL}(\\pi_0\\| \\pi) \\geq 0\n\\]\nIt is also called the Kullback-Liebler divergence.\nA common choice of the reference distribution \\(\\pi_0\\) is non-informative distributions, e.g., the Lebesgue measure."
  },
  {
    "objectID": "slides/03-prior.html#entropyinformation",
    "href": "slides/03-prior.html#entropyinformation",
    "title": "Lecture 03: From Prior Information to Prior Distribution",
    "section": "Entropy/Information",
    "text": "Entropy/Information\nSome take-home messages:\n\nEntropy \\(\\approx\\) Information \\(\\approx\\) Uncertainty \\(\\approx\\) Energy \\(\\approx\\) Temparature\nThere are different definitions of entropy, e.g. Rényi entropy, and therefore different divergences can be induced.\nEntropy plays the role of utility functions in decision theory: an inference procedure is obtained by maximizing the entropy."
  },
  {
    "objectID": "slides/03-prior.html#maximum-entropy-prior-mep",
    "href": "slides/03-prior.html#maximum-entropy-prior-mep",
    "title": "Lecture 03: From Prior Information to Prior Distribution",
    "section": "Maximum Entropy Prior (MEP)",
    "text": "Maximum Entropy Prior (MEP)\n\nThe maximum entropy prior (MEP) (with respect to a reference distribution \\(\\pi_0\\)) is the solution to the optimization problem \\[\n\\max_{\\pi \\in \\Gamma} \\; \\mc{E}(\\pi_0 \\| \\pi)\n\\qquad s.t. \\quad \\E_{\\pi}\\left[g_k(\\theta)\\right]=\\omega_k,\n\\quad k = 1, \\ldots, K\n\\tag{1}\\] where \\(\\Gamma\\) is a class of candidate priors.\nThe existence of the solution depends on \\(g_k\\)’s, \\(\\Gamma\\), and \\(\\pi_0\\).\nThe prior \\(\\pi\\) maximizing the entropy is, in this information-theoretic sense, minimizing the prior information brought through \\(\\pi\\) about \\(\\theta\\)."
  },
  {
    "objectID": "slides/03-prior.html#discrete-mep",
    "href": "slides/03-prior.html#discrete-mep",
    "title": "Lecture 03: From Prior Information to Prior Distribution",
    "section": "Discrete MEP",
    "text": "Discrete MEP\n\nLet \\(\\Theta=\\{\\theta_1, \\ldots, \\theta_m\\}\\) and \\(\\pi_0(\\theta_i) = \\frac{1}{m}\\) be the reference distribution.\nThe prior informations are \\[\n\\E_{\\pi}\\left[g_k(\\theta)\\right]=\\omega_k, \\quad k = 1, \\ldots, K.\n\\]\nThe MEP for \\(\\theta\\) is \\[\n\\pi^*(\\theta_i)=\\frac{\\exp \\left\\{\\sum_{k=1}^K \\lambda_k g_k\\left(\\theta_i\\right)\\right\\}}{\\sum_j \\exp \\left\\{\\sum_{k=1}^K \\lambda_k g_k\\left(\\theta_j\\right)\\right\\}}\n\\] where the \\(\\lambda_k\\)’s are obtained from Equation 1 as Lagrange multipliers.\nHere \\(\\Gamma\\) is the class of all discrete distributions on \\(\\Theta\\), i.e., \\(\\Gamma = \\{(p_1, \\ldots, p_m): \\sum p_i = 1\\}\\).\nThe proof is a simple application of Lagrange multipliers."
  },
  {
    "objectID": "slides/03-prior.html#continuous-mep",
    "href": "slides/03-prior.html#continuous-mep",
    "title": "Lecture 03: From Prior Information to Prior Distribution",
    "section": "Continuous MEP",
    "text": "Continuous MEP\n\n\nLet \\(\\Theta = \\R\\) and the prior information be\n\n\\[\n\\E_{\\pi}\\left[g_k(\\theta)\\right]=\\omega_k, \\quad k = 1, \\ldots, K.\n\\]\n\n\n\nThe MEP for \\(\\theta\\) is\n\n\\[\n\\pi^*(\\theta)=\\frac{\\exp \\left\\{\\sum_{k=1}^K \\lambda_k g_k(\\theta)\\right\\} \\pi_0(\\theta)}{\\int \\exp \\left\\{\\sum_{k=1}^K \\lambda_k g_k(\\eta)\\right\\} \\pi_0(\\eta)d\\eta}.\n\\]\n\n\nNotice that the above distributions \\(\\pi^{*}\\) are necessarily in an exponential family."
  },
  {
    "objectID": "slides/03-prior.html#examples",
    "href": "slides/03-prior.html#examples",
    "title": "Lecture 03: From Prior Information to Prior Distribution",
    "section": "Examples",
    "text": "Examples\n\nWithout any prior information, the MEP is the uniform distribution.\nWith \\(\\E_{\\pi}(\\theta) = \\mu\\), the MEP is \\(\\pi^*(\\theta) \\propto e^{\\lambda\\theta}\\), which can not be normalized to 1.\nWith \\(\\E_{\\pi}(\\theta) = \\mu\\) and \\(\\var_{\\pi}(\\theta) = \\sigma^2\\), the MEP is \\(N(\\mu, \\sigma^2)\\)."
  },
  {
    "objectID": "slides/03-prior.html#parametric-approximations",
    "href": "slides/03-prior.html#parametric-approximations",
    "title": "Lecture 03: From Prior Information to Prior Distribution",
    "section": "Parametric Approximations",
    "text": "Parametric Approximations\n\nA more practical solution to incorporate the prior information is to use parametric approximations.\nChoose a parametric family, e.g. normal, and find one in the family that matches the prior information (exactly or approximately).\nExample: Suppose the prior informations are: median = 0, lower quartile = -1, and upper quartile = 1.\nThe normal distribution satisfying these constraints is \\(N(0, 2.19)\\).\nThe Cauchy distribution satisfying these constraints is \\(\\text{Cauchy}(0,1)\\).\nBoth are reasonable prior distributions (depending on the likelihood and data)."
  },
  {
    "objectID": "slides/03-prior.html#some-problems",
    "href": "slides/03-prior.html#some-problems",
    "title": "Lecture 03: From Prior Information to Prior Distribution",
    "section": "Some problems",
    "text": "Some problems\n\nMatching moments/quantiles is often impractical and sometimes produces impossible values of the parameters; for instance, it can give negative variances.\nA deeper drawback of most parametric approaches is that the selection of the parametrized family is based on ease in the mathematical treatment, not on a subjective basis such as a preliminary histogram approximating \\(\\pi\\)."
  },
  {
    "objectID": "slides/03-prior.html#conjugate-prior-1",
    "href": "slides/03-prior.html#conjugate-prior-1",
    "title": "Lecture 03: From Prior Information to Prior Distribution",
    "section": "Conjugate prior",
    "text": "Conjugate prior\n\nDefinition 1 A class \\(\\mc{P}\\) of prior distributions for \\(\\theta\\) is called conjugate for a sampling model \\(p(x \\mid \\theta)\\) if \\[\\begin{align*}\n\\pi(\\theta) \\in \\mc{P} \\Rightarrow \\pi(\\theta \\mid x) \\in \\mc{P} .\n\\end{align*}\\]\n\n\nUsually, the class \\(\\mc{P}\\) is a parametric family with a finite number of parameters.\nExample: Beta is conjugate for the binomial model.\nConjugate priors make posterior calculations easy, but might not actually represent our prior information.\nHowever, mixtures of conjugate prior distributions are very flexible and are computationally tractable."
  },
  {
    "objectID": "slides/03-prior.html#exponential-families",
    "href": "slides/03-prior.html#exponential-families",
    "title": "Lecture 03: From Prior Information to Prior Distribution",
    "section": "Exponential families",
    "text": "Exponential families\n\nRecall that a family of distributions is an exponential family if its pdf/pmf can be written as \\[\\begin{align*}\nf(x;\\theta) & = h(x)\\exp\\left(\\sum_{i=1}^k w_i(\\theta)T_i(x) - \\psi(\\theta)\\right)\\\\\n& = h(x)\\exp\\left(\\sum_{i=1}^k \\eta_iT_i(x)-\\tilde{\\psi}(\\eta)\\right), \\quad \\eta_i = w_i(\\theta), \\eta = [\\eta_1, \\ldots, \\eta_k].\n\\end{align*}\\]\n\n\\(\\eta\\) is the natural parameter.\n\\(T(X) = [T_1(X), \\ldots, T_k(X)]\\) is a sufficient statistic; it is complete if the parameter space is an open set in \\(\\R^k\\).\n\nExample: Normal, Gamma, Beta, Binomial (with known \\(n\\)), Poisson, …\nNot an exponential family: \\(\\text{Unif}(\\theta, \\theta+1)\\) (if the support of the distribution depends on the parameter, then it is not an exponential family)."
  },
  {
    "objectID": "slides/03-prior.html#conjugate-priors-for-exponential-families",
    "href": "slides/03-prior.html#conjugate-priors-for-exponential-families",
    "title": "Lecture 03: From Prior Information to Prior Distribution",
    "section": "Conjugate priors for exponential families",
    "text": "Conjugate priors for exponential families\nConsider the exponential family \\(f(x\\mid\\theta) = h(x)\\exp(\\theta^Tx - \\psi(\\theta))\\).\n\nProposition 1 A conjugate family for \\(f(x \\mid \\theta)\\) is given by \\[\\begin{align*}\n\\pi(\\theta \\mid \\mu, \\lambda)=K(\\mu, \\lambda) e^{\\theta \\cdot \\mu-\\lambda \\psi(\\theta)},\n\\end{align*}\\] where \\(K(\\mu, \\lambda)\\) is the normalizing constant of the density. The corresponding posterior distribution is \\(\\pi(\\theta \\mid \\mu+x, \\lambda+1)\\).\n\n\nThe conjugate prior is again an exponential family.\nThe parameter spaces for \\(\\mu, \\lambda\\) are determined by propriety of the posterior."
  },
  {
    "objectID": "slides/03-prior.html#example-logistic-regression",
    "href": "slides/03-prior.html#example-logistic-regression",
    "title": "Lecture 03: From Prior Information to Prior Distribution",
    "section": "Example (Logistic regression)",
    "text": "Example (Logistic regression)\n\n\\(y \\in \\{0, 1\\}\\) and \\(x \\in \\R^k\\)\n\n\n\nThe conditional distribution of \\(Y\\) given \\(X = x\\) is\n\n\\[\n\\P_\\beta(Y=1\\mid X = x)=1-\\P_\\beta(Y=0 \\mid X = x)=\\frac{\\exp \\left(\\beta^t x\\right)}{1+\\exp \\left(\\beta^t x\\right)}.\n\\]\n\n\n\nGiven a sample \\((y_1, x_1), \\ldots, (y_n, x_n)\\), the joint distribution is\n\n\\[\nf\\left(y_1, \\ldots, y_n \\mid x_1, \\ldots, x_n, \\beta\\right)=\\exp \\left(\\beta^t \\sum_{i=1}^n y_i x_i\\right) \\prod_{i=1}^n\\left(1+e^{\\beta^t x_i}\\right)^{-1}.\n\\]"
  },
  {
    "objectID": "slides/03-prior.html#example-logistic-regression-1",
    "href": "slides/03-prior.html#example-logistic-regression-1",
    "title": "Lecture 03: From Prior Information to Prior Distribution",
    "section": "Example (Logistic regression)",
    "text": "Example (Logistic regression)\n\nApplying the previous proposition, we can find the conjugate prior for \\(\\beta\\): \\[\n\\pi\\left(\\beta \\mid y_0, \\lambda\\right) \\propto e^{\\beta^t y_0} \\prod_{i=1}^n\\left(1+e^{\\beta^t x_i}\\right)^{-\\lambda}.\n\\]\nThe normalizing constant for \\(\\pi(\\beta\\mid y_0, \\lambda)\\) is unknown and approximations of posterior quantities such as the posterior mean and posterior median can only be achieved through simulation techniques."
  },
  {
    "objectID": "slides/03-prior.html#mixtures-of-conjugate-priors",
    "href": "slides/03-prior.html#mixtures-of-conjugate-priors",
    "title": "Lecture 03: From Prior Information to Prior Distribution",
    "section": "Mixtures of conjugate priors",
    "text": "Mixtures of conjugate priors\nLet \\(f(x \\mid \\theta)\\) be an exponential family and \\(\\pi(\\theta \\mid \\lambda, \\mu)\\) be its conjugate prior.\n\nConsider \\(\\pi(\\theta)=\\sum_{i=1}^N w_i \\pi\\left(\\theta \\mid \\lambda_i, \\mu_i\\right)\\).\nThen the posterior is \\[\n\\pi(\\theta \\mid x)=\\sum_{i=1}^N w_i^{\\prime}(x) \\pi\\left(\\theta \\mid \\lambda_i+1, \\mu_i+x\\right).\n\\] where \\(w^{\\prime}_i(x)\\) depends on \\(w_i\\)’s and the normalizing constant of \\(\\pi(\\theta\\mid\\lambda,\\mu)\\)1.\nMixtures can then be used as a basis to approximate any prior distribution.\n\nSee Page 126 of Bayesian Choice for details."
  },
  {
    "objectID": "slides/03-prior.html#noninformative-prior-1",
    "href": "slides/03-prior.html#noninformative-prior-1",
    "title": "Lecture 03: From Prior Information to Prior Distribution",
    "section": "Noninformative prior",
    "text": "Noninformative prior\n\nWhen there is no prior information or the prior information is unreliable, should we still use Bayesian techniques?\nThe answer is YES, due to some optimality criteria (we will discuss this in the next lecture).\nIn such cases, the priors should be derived from the sampling distribution, since that is the only information we have.\nThese priors are called noninformative priors, or objective/default priors.\nWe will introduce some principles for deriving noninformative priors."
  },
  {
    "objectID": "slides/03-prior.html#laplaces-prior",
    "href": "slides/03-prior.html#laplaces-prior",
    "title": "Lecture 03: From Prior Information to Prior Distribution",
    "section": "Laplace’s prior",
    "text": "Laplace’s prior\n\nFor the binomial model, Laplace proposed to use the uniform distribution for \\(p\\).\nHis reasoning is called the principle of insufficient reason:\n\n\n\nIf we are ignorant of the ways an event can occur, the event will occur equally likely in any way. 1\n\n\n\nIt is also called the equiprobability principle.\n\nhttps://mathworld.wolfram.com/PrincipleofInsufficientReason.html"
  },
  {
    "objectID": "slides/03-prior.html#criticisms-against-laplaces-prior",
    "href": "slides/03-prior.html#criticisms-against-laplaces-prior",
    "title": "Lecture 03: From Prior Information to Prior Distribution",
    "section": "Criticisms against Laplace’s prior",
    "text": "Criticisms against Laplace’s prior\n\nWhen the parameter space is not compact, this principle leads to an improper prior.\n\nA distribution \\(\\pi\\) is improper if \\(\\pi(\\theta) \\geq 0\\) but \\(\\int \\pi(\\theta)d\\theta = \\infty\\).\nHowever, it is actually possible to work with improper priors, as long as we do not try to interpret them as probability distributions.\n\nLaplace’s prior is not invariant under reparametrization.\n\nSuupose we assign a unifrom prior for \\(\\theta\\): \\(\\pi(\\theta) \\propto 1\\).\nWe consider a reparametrization \\(\\eta = g(\\theta)\\), where \\(g\\) is one-to-one.\nThe corresponding prior is \\[\n\\pi^*(\\eta)=\\left|\\frac{d}{d \\eta} g^{-1}(\\eta)\\right|\n\\] which is not constant."
  },
  {
    "objectID": "slides/03-prior.html#example",
    "href": "slides/03-prior.html#example",
    "title": "Lecture 03: From Prior Information to Prior Distribution",
    "section": "Example",
    "text": "Example\n\nSuppose \\(\\Theta = [0, 1]\\) and \\(\\pi(\\theta) = 1\\) for \\(\\theta \\in \\Theta\\).\nConsider the odds \\(\\eta = \\frac{\\theta}{1-\\theta}\\), which is one-to-one on \\(\\Theta\\).\nThen \\(\\pi^*(\\eta) = \\left|\\frac{d}{d \\eta} g^{-1}(\\eta)\\right| = \\frac{1}{(1+\\eta)^2}\\), that is, the prior prefers small \\(\\eta\\) (corresponding to small \\(\\theta\\))."
  },
  {
    "objectID": "slides/03-prior.html#jeffreys-prior",
    "href": "slides/03-prior.html#jeffreys-prior",
    "title": "Lecture 03: From Prior Information to Prior Distribution",
    "section": "Jeffreys prior",
    "text": "Jeffreys prior\n\n\nRecall that the Fisher information of a pdf/pmf \\(f(x\\mid\\theta)\\) is given by\n\n\\[\nI(\\theta)=\\E_\\theta\\left[\\left(\\frac{\\partial \\log f(X \\mid \\theta)}{\\partial \\theta}\\right)^2\\right] \\stackrel{(*)}{=} -\\E_\\theta\\left[\\frac{\\partial^2 \\log f(X \\mid \\theta)}{\\partial \\theta^2}\\right].\n\\]\n\n\nThe Jeffreys prior distribution is \\(\\pi_J(\\theta) \\propto \\sqrt{I(\\theta)}\\), defined up to a normalization constant when \\(\\pi_J\\) is proper.\n\n\n\nThe equality \\((*)\\) holds under some regularity conditions on \\(f(x\\mid\\theta)\\)."
  },
  {
    "objectID": "slides/03-prior.html#example-1",
    "href": "slides/03-prior.html#example-1",
    "title": "Lecture 03: From Prior Information to Prior Distribution",
    "section": "Example",
    "text": "Example\nIf \\(X \\sim \\bin(n, p)\\),\n\n\\[\n\\begin{aligned}\nf(x \\mid p) & =\\left(\\begin{array}{c}\nn \\\\\nx\n\\end{array}\\right) p^x(1-p)^{n-x}, \\\\\n\\frac{\\partial^2 \\log f(x \\mid p)}{\\partial p^2} & = - \\frac{x}{p^2} - \\frac{n-x}{(1-p)^2}, \\\\\nI(p) & =n\\left[\\frac{1}{p}+\\frac{1}{1-p}\\right]=\\frac{n}{p(1-p)} .\n\\end{aligned}\n\\]\n\n\nTherefore, the Jeffreys prior for this model is \\[\n\\pi_J(p) \\propto[p(1-p)]^{-1 / 2}\n\\] and is thus proper, since it is a \\(\\text{Beta}(1/2, 1/2)\\) distribution."
  },
  {
    "objectID": "slides/03-prior.html#example-2",
    "href": "slides/03-prior.html#example-2",
    "title": "Lecture 03: From Prior Information to Prior Distribution",
    "section": "Example",
    "text": "Example\n\nBeta(1,1) (blue) and Beta(1/2, 1/2) (red)"
  },
  {
    "objectID": "slides/03-prior.html#multidimensional-parameters",
    "href": "slides/03-prior.html#multidimensional-parameters",
    "title": "Lecture 03: From Prior Information to Prior Distribution",
    "section": "Multidimensional parameters",
    "text": "Multidimensional parameters\n\nFor \\(\\theta \\in \\R^k\\), the Fisher information matrix \\(I(\\theta)\\) has the following elements,\n\n\n\\[\nI_{i j}(\\theta)=-\\mathbb{E}_\\theta\\left[\\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_j} \\log f(X \\mid \\theta)\\right] \\quad(i, j=1, \\ldots, k).\n\\]\n\n\nThe Jeffreys noninformative prior is then defined by \\(\\pi_J(\\theta) \\propto \\sqrt{\\det(I(\\theta))}\\).\nIf \\(f(x \\mid \\theta)\\) belongs to an exponential family, \\(f(x \\mid \\theta)=h(x) \\exp (\\theta \\cdot x-\\psi(\\theta))\\), the Fisher information matrix is given by \\(I(\\theta)=\\nabla \\nabla^t \\psi(\\theta)\\) and \\[\n\\pi_J(\\theta) \\propto\\left(\\prod_{i=1}^k \\psi_{i i}^{\\prime \\prime}(\\theta)\\right)^{1 / 2}\n\\] where \\(\\psi_{i i}^{\\prime \\prime}(\\theta)=\\frac{\\partial^2}{\\partial \\theta_i^2} \\psi(\\theta)\\)."
  },
  {
    "objectID": "slides/03-prior.html#jeffreys-prior-1",
    "href": "slides/03-prior.html#jeffreys-prior-1",
    "title": "Lecture 03: From Prior Information to Prior Distribution",
    "section": "Jeffreys prior",
    "text": "Jeffreys prior\nJeffreys prior has several advantages:\n\nIt depends only on the sampling model, through the Fisher information.\nThe Fisher information is an indicator of the amount of information brought by the model (or the observation) about \\(\\theta\\).\nIt is invariant under reparametrization."
  },
  {
    "objectID": "slides/03-prior.html#invariance-of-jeffreys-prior",
    "href": "slides/03-prior.html#invariance-of-jeffreys-prior",
    "title": "Lecture 03: From Prior Information to Prior Distribution",
    "section": "Invariance of Jeffreys prior",
    "text": "Invariance of Jeffreys prior\n\nBy definition, \\(I(\\theta) = I(h(\\theta))[h^{\\prime}(\\theta)]^2\\) for any one-to-one \\(h\\).\nLet \\(\\eta = g(\\theta)\\) and \\(\\theta \\sim \\pi_J(\\theta) \\propto \\sqrt{I(\\theta)}\\).\nThen \\[\\begin{align*}\n\\pi^*(\\eta) & = \\pi_J(g^{-1}(\\eta))\\left|\\frac{d}{d \\eta} g^{-1}(\\eta)\\right|\n= \\sqrt{I(g^{-1}(\\eta))}\\left|\\frac{d}{d \\eta} g^{-1}(\\eta)\\right| \\\\\n& = \\sqrt{I(\\eta)[h^{\\prime}(\\eta)]^{-2}}|h^{\\prime}(\\eta)| \\quad \\text{(Let $h = g^{-1}$)}\\\\\n& = \\sqrt{I(\\eta)} = \\pi_J(\\eta).\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/03-prior.html#limitations-of-jeffreys-prior",
    "href": "slides/03-prior.html#limitations-of-jeffreys-prior",
    "title": "Lecture 03: From Prior Information to Prior Distribution",
    "section": "Limitations of Jeffreys prior",
    "text": "Limitations of Jeffreys prior\n\nJeffreys prior works well for one-parameter models.\nFor multiparameter models however, Jeffreys prior leads to incoherences or even paradoxes.\nThey do not necessarily perform satisfactorily for all inferential purposes, in particular when considering subvectors of interest (see Example 3.5.9 in Bayesian Choice)."
  },
  {
    "objectID": "slides/03-prior.html#example-3",
    "href": "slides/03-prior.html#example-3",
    "title": "Lecture 03: From Prior Information to Prior Distribution",
    "section": "Example",
    "text": "Example\n\nConsider \\(X \\sim N(\\mu, \\sigma^2)\\) with \\(\\theta = (\\mu, \\sigma)\\) unknown.\nIn this case, \\[\nI(\\theta)  =\\left(\\begin{array}{cc}\n1 / \\sigma^2 & 0 \\\\\n0 & 2 / \\sigma^2\n\\end{array}\\right).\n\\]\nThus \\(\\pi_J(\\mu, \\sigma) \\propto \\sigma^{-2}\\).\nIf we assume \\(\\mu\\) and \\(\\sigma^2\\) are a priori independent, then \\[\n\\tilde{\\pi}_J(\\mu, \\sigma) = \\pi_J(\\mu)\\pi_J(\\sigma) \\propto \\sigma^{-1}.\n\\]\nTheoretically, \\(\\tilde{\\pi}_J(\\mu,\\sigma) \\propto \\sigma^{-1}\\) has better convergence properties than \\(\\pi_J(\\mu,\\sigma) \\propto \\sigma^{-2}\\)."
  },
  {
    "objectID": "slides/03-prior.html#example-4",
    "href": "slides/03-prior.html#example-4",
    "title": "Lecture 03: From Prior Information to Prior Distribution",
    "section": "Example",
    "text": "Example\n\nWe have seen that binomial and negative binomial modelings could lead to the same likelihood.\nHowever, if \\(X \\sim\\) \\(\\bin(n, \\theta)\\), the noninformative prior \\(\\pi_1(\\theta)\\) is \\(\\text{Beta}(1 / 2,1 / 2)\\).\nIf \\(N \\sim \\text{NB}(x, \\theta)\\), the Fisher information is \\[\\begin{align*}\nI(\\theta) & =-\\E_\\theta\\left[\\frac{\\partial^2}{\\partial \\theta^2} \\log f(N \\mid \\theta)\\right] \\\\\n& =\\E_\\theta\\left[\\frac{x}{\\theta^2}+\\frac{N-x}{(1-\\theta)^2}\\right]=\\frac{x}{\\theta^2(1-\\theta)},\n\\end{align*}\\]\nThe Jeffreys prior is \\(\\pi_2(\\theta) \\propto \\theta^{-1}(1-\\theta)^{-1 / 2}\\), which is improper and, more importantly, differs from \\(\\pi_1\\)."
  },
  {
    "objectID": "slides/03-prior.html#reference-prior",
    "href": "slides/03-prior.html#reference-prior",
    "title": "Lecture 03: From Prior Information to Prior Distribution",
    "section": "Reference prior",
    "text": "Reference prior\n\nBernardo (1979) proposed a modification of the Jeffreys approach called the reference prior approach.\nA major difference is that this method distinguishes between parameters of interest and nuisance parameters.\nHence the prior depends on the sampling model as well as the inferential problem.\nThe main idea behind the reference approach is to a prior that maximizes the information brought by the data, i.e. the KL divergence between prior and posterior.\nIt has been shown that for one-parameter models, the reference approach gives the Jeffreys prior."
  },
  {
    "objectID": "slides/03-prior.html#the-reference-prior-approach",
    "href": "slides/03-prior.html#the-reference-prior-approach",
    "title": "Lecture 03: From Prior Information to Prior Distribution",
    "section": "The reference prior approach",
    "text": "The reference prior approach\nLet \\(X \\sim f(x \\mid \\theta)\\) and \\(\\theta=\\left(\\theta_1, \\theta_2\\right)\\). Suppose \\(\\theta_1\\) is the parameter of interest, and \\(\\theta_2\\) is the nuisance parameter.\n\nDefine \\(\\pi_J\\left(\\theta_2 \\mid \\theta_1\\right)\\) as the Jeffreys prior associated with \\(f(x \\mid \\theta)\\) when \\(\\theta_1\\) is fixed.\nDerive the marginal distribution \\[\\begin{align*}\n\\tilde{f}\\left(x \\mid \\theta_1\\right)=\\int f\\left(x \\mid \\theta_1, \\theta_2\\right) \\pi\\left(\\theta_2 \\mid \\theta_1\\right) d \\theta_2.\n\\end{align*}\\]\nCompute the Jeffreys prior \\(\\pi_J\\left(\\theta_1\\right)\\) associated with \\(\\tilde{f}\\left(x \\mid \\theta_1\\right)\\).\nThe reference prior for \\(\\theta\\) is \\(\\pi_R(\\theta) \\propto \\pi_J\\left(\\theta_2 \\mid \\theta_1\\right)\\pi_J\\left(\\theta_1\\right)\\)."
  },
  {
    "objectID": "slides/03-prior.html#example-5",
    "href": "slides/03-prior.html#example-5",
    "title": "Lecture 03: From Prior Information to Prior Distribution",
    "section": "Example",
    "text": "Example\nLet \\(X_1, X_2 \\iid N(\\mu, \\sigma^2)\\) where \\(\\sigma\\) is the parameter of interest and \\(\\mu\\) is the unknown nuisance parameter.\n\nFixing \\(\\sigma\\), the Jeffreys prior for \\(\\mu\\) is \\(\\pi_J(\\mu\\mid\\sigma) \\propto 1\\) (computed using the joint distribution of \\(X_1\\) and \\(X_2\\)).\nThe marginal distribution is \\[\\begin{align*}\n\\tilde{f}(x_1, x_2\\mid\\sigma) & = \\int \\frac{1}{2\\pi\\sigma^2}\\exp\\left(-\\frac{(x_1-\\mu)^2+(x_2-\\mu)^2}{2\\sigma^2}\\right) d\\mu\\\\\n& = \\frac{1}{\\sqrt{2 \\pi} 2 \\sigma}e^{-\\left(x_1-x_2\\right)^2 / 4 \\sigma^2}.\n\\end{align*}\\]\nThe Jeffreys prior for \\(\\tilde{f}(x_1, x_2\\mid\\sigma)\\) is \\(\\pi_J(\\sigma) \\propto \\sigma^{-1}\\).\nThe reference prior for \\((\\mu, \\sigma)\\) is \\(\\pi_R(\\mu,\\sigma) \\propto \\pi_J(\\mu\\mid\\sigma)\\pi_J(\\sigma) \\propto \\sigma^{-1}\\)."
  },
  {
    "objectID": "slides/03-prior.html#more-about-reference-priors",
    "href": "slides/03-prior.html#more-about-reference-priors",
    "title": "Lecture 03: From Prior Information to Prior Distribution",
    "section": "More about reference priors",
    "text": "More about reference priors\n\nIn general, the reference prior depends the “importance” of the individual parameters.\nUnder some additional assumptions (e.g., posterior asymptotic normality), the reference prior can be easily computed.\nFor formal definitions and more examples of reference priors, see Bernardo (2005).\n\n\n\nBernardo, J. M. (2005). Reference analysis. Handbook of statistics, 25, 17-90."
  },
  {
    "objectID": "slides/03-prior.html#matching-prior",
    "href": "slides/03-prior.html#matching-prior",
    "title": "Lecture 03: From Prior Information to Prior Distribution",
    "section": "Matching prior",
    "text": "Matching prior\n\nA peculiar, not to say paradoxical, approach to noninformative priors is to look for good frequentist properties, that is, properties that hold on the average (in \\(x\\)), rather than conditional on \\(x\\).\nGoal: posterior probabilities \\(\\approx\\) frequentist coverage.\nFor example, find a prior such that the HPD interval and the frequentist CI are asymptotically the same."
  },
  {
    "objectID": "slides/03-prior.html#posterior-validation-and-robustness",
    "href": "slides/03-prior.html#posterior-validation-and-robustness",
    "title": "Lecture 03: From Prior Information to Prior Distribution",
    "section": "Posterior validation and robustness",
    "text": "Posterior validation and robustness\n\nWhen using an improper prior, you must check the propriety of the posterior.\nYou can perform some numerical analysis to see how the choice of prior affects your result. This is especially important for small sample sizes.\nAn additional level in the prior modeling should increase the robustness of the prior distribution.\nFor example, \\[\\begin{align*}\n\\lambda & \\sim \\pi_2(\\lambda), \\\\\n\\theta \\mid \\lambda & \\sim \\pi_1(\\theta \\mid \\lambda), \\\\\nX \\mid \\theta & \\sim f(x \\mid \\theta) .\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/03-prior.html#some-practical-guides-for-choosing-priors",
    "href": "slides/03-prior.html#some-practical-guides-for-choosing-priors",
    "title": "Lecture 03: From Prior Information to Prior Distribution",
    "section": "Some practical guides for choosing priors",
    "text": "Some practical guides for choosing priors\n\nWith prior information:\n\nFind the MEP\nFind a (conjugate) parametric prior that approximates the prior information\n\nWithout prior information:\n\nUse conjugate priors (if applicable) or mixture of conjugate priors\nUse noninformative priors, e.g., Jeffreys prior, reference priors\n\nFinally, perform some sensitivity analysis to assess priors’ influence on the result.\n\n\n\nHome"
  },
  {
    "objectID": "slides/01-intro.html#course-description",
    "href": "slides/01-intro.html#course-description",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Course Description",
    "text": "Course Description\n\nStatistical analysis is an inversion process: retrieving the causes (models/parameters) from the effects (observations/sample/data)\nBayes’ Theorem: If \\(A\\) and \\(E\\) are events such that \\(\\P(E) \\neq 0\\), \\[\\P(A \\mid E)=\\frac{\\P(E \\mid A) \\P(A)}{\\P(E \\mid A) \\P(A)+\\P\\left(E \\mid\nA^c\\right) \\P\\left(A^c\\right)}.\\]\nLet \\(A\\) be causes and \\(E\\) be effects. Bayes’ Theorem gives the relationship between \\(\\P(A\\mid E)\\) (statistical inference) and \\(\\P(E\\mid A)\\) (phenomenon).\nThis course will focus on statistical analysis under the Bayesian paradigm."
  },
  {
    "objectID": "slides/01-intro.html#course-description-1",
    "href": "slides/01-intro.html#course-description-1",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Course Description",
    "text": "Course Description\n\n\nAdapting Bayes’ Theorem to a parametric problem,\n\n\\[\\pi(\\theta|x) = \\frac{p(x|\\theta)\\pi(\\theta)}{\\int\n    p(x|\\theta^{\\prime})\\pi(\\theta^{\\prime})d\\theta^{\\prime}}.\\]\n\\[\\begin{align*}\n\\theta        & \\longleftrightarrow \\text{parameter}\\\\\nx             & \\longleftrightarrow \\text{data}\\\\\n\\pi(\\theta)   & \\longleftrightarrow \\textcolor{magenta}{\\text{prior distribution}}\\\\\np(x|\\theta)   & \\longleftrightarrow \\text{likelihood/model}\\\\\n\\pi(\\theta|x) & \\longleftrightarrow \\textcolor{magenta}{\\text{posterior distribution}}\\\\\n\\end{align*}\\] \n\nKey components: priors and posteriors"
  },
  {
    "objectID": "slides/01-intro.html#course-description-2",
    "href": "slides/01-intro.html#course-description-2",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Course Description",
    "text": "Course Description\nA general procedure for Bayesian statistical analysis:\n\nChoose a class \\(\\mc{M}\\) of models for the sample \\(x\\);\nChoose a prior distribution over \\(\\mc{M}\\);\nUse Bayes Theorem to compute the posterior distribution of models given the sample;\nPick a “good” model based on the posterior distribution."
  },
  {
    "objectID": "slides/01-intro.html#topics",
    "href": "slides/01-intro.html#topics",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Topics",
    "text": "Topics\n\n\nPrinciples for the choice of priors\nInference based on the posterior\nBayesian analysis for different problems:\n\nestimation\nhypothesis testing\nregression\nprediction\nnonparametric model\nmodel comparison"
  },
  {
    "objectID": "slides/01-intro.html#course-materials",
    "href": "slides/01-intro.html#course-materials",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Course Materials",
    "text": "Course Materials\n\n\nHoff, P. D. (2009). A First Course in Bayesian Statistical Methods.\nRobert, C. P. (2007). The Bayesian Choice, 2nd Edition.\nGelman, A. et al. (2020). Bayesian Data Analysis, 3rd Edition.\nRobert, C. P., & Casella, G. (2004). Monte Carlo Statistical Methods, 2nd Edition.\nProf. Rebecca Steorts at Duke. Lecture notes\nSlides will be posted on NTU Cool."
  },
  {
    "objectID": "slides/01-intro.html#course-schedule",
    "href": "slides/01-intro.html#course-schedule",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Course Schedule",
    "text": "Course Schedule\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopics\nReading\n\n\n\n\n1\n9/5\nIntroduction & History of Bayes Theorem\n\n\n\n2\n9/12\nOne-parameter Models; Conjugate Priors\nHoff Ch. 1-3, BC Ch. 1\n\n\n3\n9/19\nPrior Information and Prior Distribution\nBC Ch. 3\n\n\n4\n9/26\nDecision Theory and Bayesian Estimation\nBC Ch. 2, 4\n\n\n5\n10/3\nConnections to non-Bayesian Analysis; Hierarchical Models\nBDA Ch. 4, 5\n\n\n6\n10/10\nNo class (National Holiday)\n\n\n\n7\n10/17\nTesting and Model Comparison\nBC Ch. 5, 7, BDA Ch. 6, 7\n\n\n8\n10/24\nProject Proposal\n\n\n\n9\n10/31\nMetropolis-Hastings algorithms; Gibbs sampler\nBDA Ch. 10-11\n\n\n10\n11/7\nHamiltonian Monte Carlo; Variational Inference\nBDA Ch. 12-13\n\n\n11\n11/14\nBayesian regression\nBDA Ch. 14\n\n\n12\n11/21\nGeneralized Linear Models; Latent Variable Model\nBDA Ch. 16, 18\n\n\n13\n11/28\nEmpirical Bayes\nBC Ch. 10\n\n\n14\n12/5\nBayesian Nonparametrics\nBDA Ch. 21, 23\n\n\n15\n12/12\nFinal Project Presentation\n\n\n\n16\n12/19\nFinal Project Presentation"
  },
  {
    "objectID": "slides/01-intro.html#grading",
    "href": "slides/01-intro.html#grading",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Grading",
    "text": "Grading\n\n\nHomework (3 - 4 homeworks): 30%\nOral Presentation (project proposal and final presentation): 40%\nWritten Report: 30%\n\n\nContact information\n\n\nOffice: Room 602, Cosmology Hall\nE-mail: chunhaoy@ntu.edu.tw\nOffice Hours: Tue. 3-5pm or by appointment"
  },
  {
    "objectID": "slides/01-intro.html#final-project",
    "href": "slides/01-intro.html#final-project",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Final Project",
    "text": "Final Project\n\n\nA group of 2-3 students\nChoose a dataset and find a problem which you can answer with your dataset.\nFigure out how to perform legitimate statistical analysis based on (but not limited to) what we learn in this course.\nAn oral presentation of your results (20 mins per group)\nA written report (8 pages) including:\n\ndescription of the dataset\nstatistical problem\nmodel and inference\nresults and conclusion\nreference\n\nMore details will be announced after midterm exam."
  },
  {
    "objectID": "slides/01-intro.html#prerequisites",
    "href": "slides/01-intro.html#prerequisites",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Prerequisites",
    "text": "Prerequisites\n\n\nBasic calculus and linear algebra\nMathematical statistics (undergrad level is fine)\nFamiliarity with some data analytic programming language (R/Python/MATLAB)\nExperiences with data analysis is not required but will be helpful"
  },
  {
    "objectID": "slides/01-intro.html#example-8-school-dataset",
    "href": "slides/01-intro.html#example-8-school-dataset",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Example: 8-school dataset",
    "text": "Example: 8-school dataset\nA study was performed for the ETS to analyze the effects of special coaching programs on SAT scores. Eight schools with their own coaching programs were involved in this study.\n\n\n\n\n\nSchool\nEffect\nStd\n\n\n\n\nA\n28.39\n14.9\n\n\nB\n7.94\n10.2\n\n\nC\n-2.75\n16.3\n\n\nD\n6.82\n11.0\n\n\nE\n-0.64\n9.4\n\n\nF\n0.63\n11.4\n\n\nG\n18.01\n10.4\n\n\nH\n12.16\n17.6\n\n\n\n\n\n\n\n\n\nSee Ch 5.5 of BDA3."
  },
  {
    "objectID": "slides/01-intro.html#example-8-school-dataset-1",
    "href": "slides/01-intro.html#example-8-school-dataset-1",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Example: 8-school dataset",
    "text": "Example: 8-school dataset\nWhat can we ask about this dataset?\n\nIs there any difference between the coaching programs?\nWhich program is considered effective?\nWhich program is the most effective?\nHow much can we expect to improve on the SAT scores after taking the coaching programs?"
  },
  {
    "objectID": "slides/01-intro.html#step-1-2-determine-the-likelihood-and-prior",
    "href": "slides/01-intro.html#step-1-2-determine-the-likelihood-and-prior",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Step 1 & 2: Determine the likelihood and prior",
    "text": "Step 1 & 2: Determine the likelihood and prior\n\nConsider \\[\\begin{align*}\nY_{i} \\mid \\theta_i, \\sigma_i^2 & \\ind N(\\theta_i, \\sigma_i^2), i = 1,\\ldots, 8\\\\\n\\theta_i \\mid \\mu, \\tau & \\ind N(\\mu, \\tau^2)\n\\end{align*}\\] where \\(Y_i\\)’s are the observed average treatment effect and \\(\\sigma_i^2\\)’s are the variance of the treatment effects.\nWe are interested in estimating the \\(\\theta_i\\)’s, which are the effects of the coaching programs."
  },
  {
    "objectID": "slides/01-intro.html#step-4-inference-based-on-the-posterior",
    "href": "slides/01-intro.html#step-4-inference-based-on-the-posterior",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Step 4: Inference based on the posterior",
    "text": "Step 4: Inference based on the posterior\n\ndata {\n  int&lt;lower=0&gt; J;         \n  real y[J];              \n  real&lt;lower=0&gt; sigma[J];\n}\nparameters {\n  real mu;                \n  real&lt;lower=0&gt; tau;      \n  real theta[J];\n}\nmodel {\n  theta ~ normal(mu, tau); // prior\n  y ~ normal(theta, sigma); // likelihood\n}"
  },
  {
    "objectID": "slides/01-intro.html#step-4-inference-based-on-the-posterior-1",
    "href": "slides/01-intro.html#step-4-inference-based-on-the-posterior-1",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Step 4: Inference based on the posterior",
    "text": "Step 4: Inference based on the posterior\n\nlibrary(rstan)\nschools_dat &lt;- list(J = 8,\n    y = c(28, 8, -3, 7, -1, 1, 18, 12),\n    sigma = c(15, 10, 16, 11, 9, 11, 10, 18)) \nfit &lt;- rstan::sampling(school_model, data = schools_dat,\n    chains = 4, iter = 11000, warmup = 1000, thin = 10)"
  },
  {
    "objectID": "slides/01-intro.html#step-4-inference-based-on-the-posterior-2",
    "href": "slides/01-intro.html#step-4-inference-based-on-the-posterior-2",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Step 4: Inference based on the posterior",
    "text": "Step 4: Inference based on the posterior"
  },
  {
    "objectID": "slides/01-intro.html#stan-statistical-computing-platform",
    "href": "slides/01-intro.html#stan-statistical-computing-platform",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Stan: Statistical Computing Platform",
    "text": "Stan: Statistical Computing Platform\n\n\n\n\n\n\n\nProvide automated Bayesian computation\nInterface with R/Python/MATLAB/…"
  },
  {
    "objectID": "slides/01-intro.html#who-is-the-father-of-statistics",
    "href": "slides/01-intro.html#who-is-the-father-of-statistics",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Who is the father of statistics?",
    "text": "Who is the father of statistics?\nSome important statisticians:\n\nCarl Friedrich Gauss (1777 - 1855): Gaussian distribution, …\nRonald A. Fisher (1890 - 1962): likelihood based inference, …\nKarl Pearson (1857 - 1936): Pearson’s correlation coefficient, …\nWilliam Sealy Gosset (1876 - 1937): \\(t\\)-test and \\(t\\) distribution, …\nJerzy Neyman (1894 - 1981): Neyman-Pearson Lemma, …\nPierre-Simon Laplace (1749 - 1827): Central Limit Theorem, …\nThomas Bayes (1702 - 1761): Bayes Theorem, …"
  },
  {
    "objectID": "slides/01-intro.html#thomas-bayes",
    "href": "slides/01-intro.html#thomas-bayes",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Thomas Bayes",
    "text": "Thomas Bayes\n\n\n\nBritish Reverend Thomas Bayes (1702 - 1761)\nWork on inverse probability problem\nDiscover a relationship between causes and observations (1746 - 1749), which is later called Bayes Theorem\nWrite “An Essay towards solving a Problem in the Doctrine of Chances”\nHis philosopher friend, Richard Price, edited and published his result."
  },
  {
    "objectID": "slides/01-intro.html#bayes-thought-experiment",
    "href": "slides/01-intro.html#bayes-thought-experiment",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Bayes’ Thought Experiment",
    "text": "Bayes’ Thought Experiment"
  },
  {
    "objectID": "slides/01-intro.html#pierre-simon-laplace",
    "href": "slides/01-intro.html#pierre-simon-laplace",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Pierre-Simon Laplace",
    "text": "Pierre-Simon Laplace\n\n\n\nA French mathematician (1749 - 1827)\nDiscover the same rule in 1774, independent of Bayes\nUsing conditional probability, write down the rule as \\[\\begin{align*}\n\\P(A \\mid E)=\\frac{\\P(E \\mid A) \\P(A)}{\\P(E \\mid A) \\P(A)+\\P\\left(E \\mid\nA^c\\right) \\P\\left(A^c\\right)}.\n\\end{align*}\\]\nPropose to use equi-probability as the prior, i.e., the uniform prior"
  },
  {
    "objectID": "slides/01-intro.html#laplaces-rule-of-succession",
    "href": "slides/01-intro.html#laplaces-rule-of-succession",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Laplace’s Rule of Succession",
    "text": "Laplace’s Rule of Succession\n\nIf we repeat an experiment that we know can result in a success or failure, \\(n\\) times independently, and get \\(s\\) successes, and \\(n-s\\) failures, then what is the probability that the next repetition will succeed?\nLaplace’s Answer: \\[\\P\\left(X_{n+1}=1 \\mid X_1+\\cdots+X_n=s\\right)=\\frac{s+1}{n+2}\\]\nThis answer is obtained from the Beta-Binomial model."
  },
  {
    "objectID": "slides/01-intro.html#pierre-simon-laplace-1",
    "href": "slides/01-intro.html#pierre-simon-laplace-1",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Pierre-Simon Laplace",
    "text": "Pierre-Simon Laplace\n\nLaplace was not satisfied with the uniform prior, neither did other mathematicians.\nHowever, he still applied Bayes Theorem to solve many practical problems:\n\nEstimating the French population\nBirth and census study\nCredibility of witnesses in the court\n\nIn 1810, he announced the Central Limit Theorem.\nAt the age of 62, he turned to frequentist-based approach. Why?"
  },
  {
    "objectID": "slides/01-intro.html#decline-of-bayesian-statistics",
    "href": "slides/01-intro.html#decline-of-bayesian-statistics",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Decline of Bayesian Statistics",
    "text": "Decline of Bayesian Statistics\n\nThe subjective component, prior distribution, is heavily criticized by mathematicians and theorists.\nFor large dataset, Bayesian and frequentist produced almost the same result.\nDataset became more reliable, and frequentist approaches are easier to implement.\nBayesian approaches are computationally challenging, even for simple models.\nResearchers tend to design their own experiments to answer industrial/scientific questions."
  },
  {
    "objectID": "slides/01-intro.html#sir-ronald-a.-fisher",
    "href": "slides/01-intro.html#sir-ronald-a.-fisher",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Sir Ronald A. Fisher",
    "text": "Sir Ronald A. Fisher\n\n\n\nBritish Statistician/Mathematician (1890 - 1962)\nTea and milk experiment\nFather of modern statistics and experimental design\nProblem –&gt; Experiment –&gt; Data –&gt; Analysis\nDevelop a collection of statistical methods, e.g., MLE, ANOVA, Fisher information, sufficient statistics, etc."
  },
  {
    "objectID": "slides/01-intro.html#revival-of-bayesian-statistics",
    "href": "slides/01-intro.html#revival-of-bayesian-statistics",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Revival of Bayesian Statistics",
    "text": "Revival of Bayesian Statistics\n\nClean, reliable data -&gt; Frequentist\nThrough the use of prior information, Bayesian approaches are actually more powerful and flexible when handling complex datasets.\nAdvances in computing technologies\nReadings:\n\nLindley, D. V. (1975). The future of statistics: A Bayesian 21st century. Advances in Applied Probability, 7, 106-115.\nEfron, B. (1986). Why isn’t everyone a Bayesian? The American Statistician, 40(1), 1-5.\nEfron, B. (1998). R. A. Fisher in the 21st century. Statistical Science, 95-114."
  },
  {
    "objectID": "slides/01-intro.html#what-is-probability",
    "href": "slides/01-intro.html#what-is-probability",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "What is probability?",
    "text": "What is probability?\n\nFrequentist: Probability is the limit of relative frequency as the experiment repeats infinitely.\nBayesian: Probability reflects one’s belief.\nHowever, it doesn’t matter how you interpret probability because\n\n\n\n\n\n\n\n\n\n\nImage source: https://www.lacan.upc.edu/admoreWeb/2018/05/all-models-are-wrong-but-some-are-useful-george-e-p-box/"
  },
  {
    "objectID": "slides/01-intro.html#the-theory-that-would-not-die",
    "href": "slides/01-intro.html#the-theory-that-would-not-die",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "The Theory That Would Not Die",
    "text": "The Theory That Would Not Die\n\n\nFor more interesting stories about Bayesian statistics, check\n\n\n\n\n\n\n\nThere is a talk at Google given by the author Sharon McGrayne.\nhttps://www.lesswrong.com/posts/RTt59BtFLqQbsSiqd/a-history-of-bayes-theorem"
  },
  {
    "objectID": "slides/01-intro.html#probability",
    "href": "slides/01-intro.html#probability",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Probability",
    "text": "Probability\n\nThe value \\(\\P(E)\\) is the probability of the occurrence of event \\(E\\).\nLet \\(\\Omega\\) be the sample space, i.e., the space containing all possible outcomes.\nA probability \\(\\P\\) on \\(\\Omega\\) satisfies:\n\n\\(\\P(\\Omega) = 1\\)\n\\(\\P(E) \\geq 0\\) for any \\(E \\subset \\Omega\\)\n(countable additivity) \\(\\P(\\cup_{i=1}^{\\infty} E_i) = \\sum_{i=1}^\\infty \\P(E_i)\\) for pairwise disjoint \\(E_i\\)’s"
  },
  {
    "objectID": "slides/01-intro.html#conditional-probability",
    "href": "slides/01-intro.html#conditional-probability",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Conditional Probability",
    "text": "Conditional Probability\n\n\nThe conditional probability of \\(A\\) given \\(B\\) is\n\n\\[\\P(A \\mid B) \\coloneqq \\frac{\\P(A \\cap B)}{\\P(B)}, \\quad \\text{if}\\;\\;\\P(B) &gt; 0.\\]\n\n\n\nOne-line proof of Bayes Theorem: If \\(\\P(A)\\) and \\(\\P(B)\\) are both non-zero,\n\n\\[\\P(A \\mid B) = \\frac{\\P(A \\cap B)}{\\P(B)} = \\frac{\\P(B \\mid A)\\P(A)}{\\P(B)}.\\]"
  },
  {
    "objectID": "slides/01-intro.html#random-variables",
    "href": "slides/01-intro.html#random-variables",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Random Variables",
    "text": "Random Variables\n\nA random variable is\n\na measurable function\nan outcome from a random experiment\n\nThe information about a random variable is given by it probability density function (pdf) or probability mass function (pmf).\npdf: \\(f(x) \\geq 0\\) and \\(\\int f(x) dx = 1\\)\npmf: \\(f(x) = \\P(X = x)\\) and \\(\\sum f(x) = 1\\)\nExpectation: \\(\\E(X) = \\int xf(x)dx\\) or \\(\\E(X) = \\sum x\\P(X = x)\\)\nVariance: \\(\\var(X) = \\E[(X - \\mu)^2] = \\E(X^2) - \\mu^2\\) where \\(\\mu = \\E(X)\\).\nCovariance: \\(\\cov(X, Y) = \\E[(X - \\mu_X)(Y - \\mu_Y)] = \\E(XY) - \\mu_X\\mu_Y\\)"
  },
  {
    "objectID": "slides/01-intro.html#independence",
    "href": "slides/01-intro.html#independence",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Independence",
    "text": "Independence\n\nIndependence of events: \\(\\P(A \\cap B) = \\P(A)\\P(B)\\)\nIndependent random variables:\n\n\\(\\Leftrightarrow\\) \\(f_{X,Y}(x, y) = f_X(x)f_Y(y)\\)\n\\(\\Rightarrow\\) \\(\\E(XY) = \\E(X) \\E(Y)\\) (the reverse is not true)\n\\(\\Leftrightarrow\\) \\(f_{X|Y}(x|y) = f_X(x)\\)\n\nConditional independence\n\n\\(X \\perp Y \\mid Z\\): given \\(Z\\), \\(X\\) and \\(Y\\) are independent\nEquivalently, \\(X \\mid Z\\) and \\(Y \\mid Z\\) are independent.\n\nRemark: (mutual) independence DOES NOT imply conditional independence\nExample: \\(X, Z \\iid \\text{Ber}(1/2)\\) and \\(Y = I(X \\neq Z)\\). Check that \\(X\\) and \\(Y\\) are independent, but they are not conditionally independent given \\(Z\\)."
  },
  {
    "objectID": "slides/01-intro.html#law-of-total-probabilityexpectationvariance",
    "href": "slides/01-intro.html#law-of-total-probabilityexpectationvariance",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Law of Total Probability/Expectation/Variance",
    "text": "Law of Total Probability/Expectation/Variance\n\n\nLaw of Total Probability: For \\(B_i \\cap B_j = \\emptyset\\) and \\(\\cup_{i=1}^k B_i = \\Omega\\),\n\n\\[\\P(A) = \\sum_{i=1}^k \\P(A \\mid B_i)\\P(B_i).\\]\n\n\n\nLaw of Total Expectation (tower property/double expectation):\n\n\\[\\E(X) = \\E_Y(\\E_{X|Y}(X|Y)).\\]\n\n\n\nLaw of Total Variance:\n\n\\[\\var(X) = \\var(\\E(X|Y)) + \\E(\\var(X|Y)).\\]"
  },
  {
    "objectID": "slides/01-intro.html#important-univariate-distributions",
    "href": "slides/01-intro.html#important-univariate-distributions",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Important univariate distributions",
    "text": "Important univariate distributions\n\n\nContinuous univariate distribution:\n\nNormal distribution (on \\(\\R\\))\nGamma distribution (on \\(\\R_+\\))\nBeta distribution (on \\([0, 1]\\))\n\nDiscrete univariate distribution:\n\nBinomial distribution (on \\(\\{0, 1, \\ldots, n\\}\\))\nPoisson distribution (on \\(\\{0, 1, 2, \\ldots\\}\\))\nNegative Binomial distribution (on \\(\\{0, 1, 2, \\ldots\\}\\))"
  },
  {
    "objectID": "slides/01-intro.html#normal-distribution",
    "href": "slides/01-intro.html#normal-distribution",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Normal Distribution",
    "text": "Normal Distribution\n\n\n\\(N(\\mu, \\sigma^2)\\), \\(\\mu \\in \\R\\), \\(\\sigma &gt; 0\\)\nDensity function:\n\n\\[f(x;\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right), \\quad x \\in \\R\\]\n\n\\(\\E(X) = \\mu\\) and \\(\\var(X) = \\sigma^2\\)"
  },
  {
    "objectID": "slides/01-intro.html#gamma-distribution",
    "href": "slides/01-intro.html#gamma-distribution",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Gamma Distribution",
    "text": "Gamma Distribution\n\n\n\\(\\text{Gamma}(\\alpha, \\beta)\\), \\(\\alpha, \\beta &gt; 0\\)\nDensity function:\n\n\\[f(x;\\alpha, \\beta) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}x^{\\alpha-1}\\exp(-\\beta x), \\quad x &gt; 0\\]\n\n\\(\\E(X) = \\frac{\\alpha}{\\beta}\\) and \\(\\var(X) = \\frac{\\alpha}{\\beta^2}\\).\nSpecial Case: Exponential distribution (\\(\\alpha = 1\\)) and \\(\\chi^2\\) distribution (\\(\\alpha = k/2\\) and \\(\\beta = \\frac{1}{2}\\), where \\(k\\) is the degree of freedom)"
  },
  {
    "objectID": "slides/01-intro.html#beta-distribution",
    "href": "slides/01-intro.html#beta-distribution",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Beta Distribution",
    "text": "Beta Distribution\n\n\n\\(\\text{Beta}(\\alpha, \\beta)\\), \\(\\alpha, \\beta &gt; 0\\)\nDensity function:\n\n\\[f(x;\\alpha, \\beta) = \\frac{1}{B(\\alpha, \\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}, \\quad 0 \\leq x \\leq 1\\]\n\n\\(B(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}\\) is the Beta function\n\\(\\E(X) = \\frac{\\alpha}{\\alpha+\\beta}\\)\nSpecial case: Uniform distribution (\\(\\alpha = \\beta = 1\\))"
  },
  {
    "objectID": "slides/01-intro.html#binomial-distribution",
    "href": "slides/01-intro.html#binomial-distribution",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\n\n\\(\\text{Bin}(n, p)\\), \\(n \\in \\mathbb{N}\\), \\(0 &lt; p &lt; 1\\)\nNumber of positive outcomes out of \\(n\\) binary trials\nMass function:\n\n\\[f(x; n, p) = \\choose{n}{x}p^x(1-p)^{n-x}, \\quad x = 0, 1,\\ldots, n\\]\n\n\\(\\E(X) = np\\) and \\(\\var(X) = np(1-p)\\)\nSpecial case: Bernoulli distribution (\\(n = 1\\))"
  },
  {
    "objectID": "slides/01-intro.html#poisson-distribution",
    "href": "slides/01-intro.html#poisson-distribution",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\n\n\n\\(\\text{Poi}(\\lambda)\\), \\(\\lambda &gt; 0\\)\nMass function:\n\n\\[f(x;\\lambda) = \\frac{e^{-\\lambda}\\lambda^x}{x!}, \\quad x = 0, 1, 2, \\ldots\\]\n\n\\(\\E(X) = \\var(X) = \\lambda\\)."
  },
  {
    "objectID": "slides/01-intro.html#negative-binomial-distribution",
    "href": "slides/01-intro.html#negative-binomial-distribution",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Negative Binomial Distribution",
    "text": "Negative Binomial Distribution\n\n\n\\(\\text{NB}(r, p)\\), \\(r = 1,2,\\ldots\\), \\(0 &lt; p &lt; 1\\)\nNumber of failures before the \\(r\\)th success\nMass function:\n\n\\[f(x;r,p) = \\choose{x+r-1}{x}p^r(1-p)^x, \\quad x = 0, 1, \\ldots\\]\n\n\\(\\E(X) = \\frac{r(1-p)}{p}\\) and \\(\\var(X) = \\frac{r(1-p)}{p^2}\\).\nSpecial case: Geometric distribution (\\(r=1\\), number of failures before the first success)"
  },
  {
    "objectID": "slides/01-intro.html#maximum-likelihood-estimation",
    "href": "slides/01-intro.html#maximum-likelihood-estimation",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Maximum Likelihood Estimation",
    "text": "Maximum Likelihood Estimation\n\nSuppose \\(X_1, \\ldots, X_n \\iid f(x|\\theta)\\).\nThe likelihood function is \\[L(\\theta) = \\prod_{i=1}^n f(x_i|\\theta).\\]\nThe MLE of \\(\\theta\\) is \\(\\hat{\\theta} = \\argmax_{\\theta}L(\\theta)\\).\nUnder some regularity conditions on \\(f(x|\\theta)\\), the MLE is efficient (has the smallest variance) and its distribution is approximately normal (when \\(n\\) is large enough)."
  },
  {
    "objectID": "slides/01-intro.html#law-of-large-numbers-central-limit-theorem",
    "href": "slides/01-intro.html#law-of-large-numbers-central-limit-theorem",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Law of Large Numbers & Central Limit Theorem",
    "text": "Law of Large Numbers & Central Limit Theorem\nSuppose \\(X_1, \\ldots, X_n\\) are iid (independent and identically distributed) from some distribution \\(F\\) with \\(\\E(X) = \\mu\\) and \\(\\var(X) = \\sigma^2 &lt; \\infty\\). Let \\(\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i\\). Then\n\nLaw of Large Numbers (LLN): \\(\\bar{X}_n\\) will be very closed to \\(\\mu\\) for large \\(n\\) \\[ \\bar{X}_n \\cas \\mu\\]\nCentral Limit Theorem (CLT): the distribution of \\(\\bar{X}_n\\) will be approximately normal for large \\(n\\) \\[\\sqrt{n}(\\bar{X}_n - \\mu) \\cd N(0, \\sigma^2)\\]"
  },
  {
    "objectID": "slides/01-intro.html#monte-carlo-approximationestimation",
    "href": "slides/01-intro.html#monte-carlo-approximationestimation",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Monte Carlo Approximation/Estimation",
    "text": "Monte Carlo Approximation/Estimation\n\nSuppose we have function \\(f: [a, b] \\to \\R\\) and we want to compute \\(I = \\int_a^b f(x)dx\\).\nWrite \\[I = (b-a)\\int_a^b f(x)\\frac{1}{b-a}dx = (b-a)\\E(f(X)), \\quad X \\sim \\text{Unif}(a,b).\\]\nMonte Carlo approximation:\n\nGenerate \\(X_1, \\ldots, X_n \\iid \\text{Unif}(a,b)\\).\nCompute \\(\\hat{I}_n = \\frac{b-a}{n}\\sum_{i=1}^n f(X_i)\\).\nBy LLN and CLT, \\[\\hat{I}_n \\cas I \\quad \\text{and} \\quad \\sqrt{n}(\\hat{I}_n - I) \\cd N(0, (b-a)^2\\sigma^2)\\] if \\(\\sigma^2 = \\var(f(X)) &lt; \\infty\\).\n\n\n\n\nHome"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Statistical Methods (Fall 2023)",
    "section": "",
    "text": "Week\nDate\nTopics\nLecture\nReading\n\n\n\n\n1\n9/5\nIntroduction & History of Bayes Theorem\nSlide\n\n\n\n2\n9/12\nOne-parameter Models; Conjugate Priors\nSlide\nHoff Ch. 1-3, BC Ch. 1\n\n\n3\n9/19\nPrior Information and Prior Distribution\nSlide\nBC Ch. 3\n\n\n4\n9/26\nDecision Theory and Bayesian Estimation\nSlide\nBC Ch. 2, 4\n\n\n5\n10/3\nConnections to non-Bayesian Analysis; Hierarchical Models\nSlide\nBDA Ch. 4, 5\n\n\n6\n10/10\nNo class (National Holiday)\n\n\n\n\n7\n10/17\nTesting and Model Comparison\nSlide\nBC Ch. 5, 7, BDA Ch. 6, 7\n\n\n8\n10/24\nProject Proposal\n\n\n\n\n9\n10/31\nMetropolis-Hastings algorithms; Gibbs sampler\nSlide\nBDA Ch. 10-11\n\n\n10\n11/7\nHamiltonian Monte Carlo; Variational Inference\nSlide\nBDA Ch. 12-13\n\n\n11\n11/14\nBayesian regression\nSlide\nBDA Ch. 14\n\n\n12\n11/21\nGeneralized Linear Models; Latent Variable Model\nSlide Survey\nBDA Ch. 16, 18\n\n\n13\n11/28\nBayesian Nonparametrics\n\nBDA Ch. 21, 23\n\n\n14\n12/5\nEmpirical Bayes\n\nBC Ch. 10\n\n\n15\n12/12\nFinal Project Presentation\n\n\n\n\n16\n12/19\nFinal Project Presentation"
  },
  {
    "objectID": "index.html#course-schedule",
    "href": "index.html#course-schedule",
    "title": "Bayesian Statistical Methods (Fall 2023)",
    "section": "",
    "text": "Week\nDate\nTopics\nLecture\nReading\n\n\n\n\n1\n9/5\nIntroduction & History of Bayes Theorem\nSlide\n\n\n\n2\n9/12\nOne-parameter Models; Conjugate Priors\nSlide\nHoff Ch. 1-3, BC Ch. 1\n\n\n3\n9/19\nPrior Information and Prior Distribution\nSlide\nBC Ch. 3\n\n\n4\n9/26\nDecision Theory and Bayesian Estimation\nSlide\nBC Ch. 2, 4\n\n\n5\n10/3\nConnections to non-Bayesian Analysis; Hierarchical Models\nSlide\nBDA Ch. 4, 5\n\n\n6\n10/10\nNo class (National Holiday)\n\n\n\n\n7\n10/17\nTesting and Model Comparison\nSlide\nBC Ch. 5, 7, BDA Ch. 6, 7\n\n\n8\n10/24\nProject Proposal\n\n\n\n\n9\n10/31\nMetropolis-Hastings algorithms; Gibbs sampler\nSlide\nBDA Ch. 10-11\n\n\n10\n11/7\nHamiltonian Monte Carlo; Variational Inference\nSlide\nBDA Ch. 12-13\n\n\n11\n11/14\nBayesian regression\nSlide\nBDA Ch. 14\n\n\n12\n11/21\nGeneralized Linear Models; Latent Variable Model\nSlide Survey\nBDA Ch. 16, 18\n\n\n13\n11/28\nBayesian Nonparametrics\n\nBDA Ch. 21, 23\n\n\n14\n12/5\nEmpirical Bayes\n\nBC Ch. 10\n\n\n15\n12/12\nFinal Project Presentation\n\n\n\n\n16\n12/19\nFinal Project Presentation"
  },
  {
    "objectID": "index.html#important-dates",
    "href": "index.html#important-dates",
    "title": "Bayesian Statistical Methods (Fall 2023)",
    "section": "Important Dates:",
    "text": "Important Dates:\n\n10/10: No class (National Holiday)\n10/24: Project proposal: you need to prepare a 10-min presentation for your project proposal\n12/12,19: Final project presentation: a 20-min presentation for your final project"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "BDA Book Website\nStan\nDoing Meta-Analysis in R\nMixed Models with R"
  },
  {
    "objectID": "resources.html#links",
    "href": "resources.html#links",
    "title": "Resources",
    "section": "",
    "text": "BDA Book Website\nStan\nDoing Meta-Analysis in R\nMixed Models with R"
  },
  {
    "objectID": "resources.html#other-bayesian-courses",
    "href": "resources.html#other-bayesian-courses",
    "title": "Resources",
    "section": "Other Bayesian Courses",
    "text": "Other Bayesian Courses\n\nProf. Aki Vehtari"
  },
  {
    "objectID": "slides/02-intro-bayes.html#beta-binomial-model-1",
    "href": "slides/02-intro-bayes.html#beta-binomial-model-1",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Beta-Binomial model",
    "text": "Beta-Binomial model\n\nLet \\(X_1, \\ldots, X_n \\mid p \\iid \\ber(p)\\).\nConsider the prior \\(p \\sim \\text{Beta}(\\alpha, \\beta)\\) where \\(\\alpha\\) and \\(\\beta\\) are known.\nThe posterior distribution of \\(p\\) given \\(X_1, \\ldots, X_n\\) is \\[\np \\mid X_1, \\ldots, X_n \\sim \\text{Beta}\\left(\\alpha + \\sum_{i=1}^n X_i, \\beta + n - \\sum_{i=1}^n X_i \\right).\n\\]"
  },
  {
    "objectID": "slides/02-intro-bayes.html#derivation",
    "href": "slides/02-intro-bayes.html#derivation",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Derivation",
    "text": "Derivation\n\nBayes Theorem: \\[\\begin{align*}\n\\pi(p \\mid x) & = \\frac{f(x \\mid p) \\pi(p)}{\\int f(x \\mid p) \\pi(p) dp} = \\frac{\\text{likelihood} \\times \\text{prior}}{\\text{marginal}}\\\\\n& \\propto f(x \\mid p) \\pi(p) = \\text{likelihood} \\times \\text{prior}\n\\end{align*}\\]\nThe marginal (and other normalizing constants) can be ignored.\nThe likelihood is \\[\nf(x_1, \\ldots, x_n \\mid p) = \\prod_{i=1}^n p^{x_i}(1-p)^{1-x_i} = p^{\\sum_{i=1}^n x_i}(1-p)^{n - \\sum_{i=1}^n x_i}.\n\\]\nThe prior is \\[\n\\pi(p) = \\frac{1}{B(\\alpha, \\beta)} p^{\\alpha-1} (1-p)^{\\beta-1} \\propto p^{\\alpha-1}(1-p)^{\\beta - 1}.\n\\]"
  },
  {
    "objectID": "slides/02-intro-bayes.html#derivation-1",
    "href": "slides/02-intro-bayes.html#derivation-1",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Derivation",
    "text": "Derivation\n\nHence the posterior is \\[\\begin{align*}\n\\pi(p \\mid X_1, \\ldots, X_n) & \\propto p^{\\sum_{i=1}^n x_i}(1-p)^{n - \\sum_{i=1}^n x_i} \\times p^{\\alpha-1}(1-p)^{\\beta - 1}\\\\\n& = p^{\\alpha + \\sum_{i=1}^n x_i - 1}(1-p)^{\\beta + n - \\sum_{i=1}^n x_i - 1}.\n\\end{align*}\\]\nRecognizing that this is the kernel of a Beta distribution, the posterior is \\[\np \\mid X_1, \\ldots, X_n \\sim \\text{Beta}\\left(\\alpha + \\sum_{i=1}^n X_i, \\beta + n - \\sum_{i=1}^n X_i \\right).\n\\]\nIt’s called Beta-Binomial model since the posterior only depends on \\(\\sum X_i\\) and the distribution of \\(\\sum X_i\\) is \\(\\bin(n,p)\\)."
  },
  {
    "objectID": "slides/02-intro-bayes.html#kernel-of-a-pdfpmf",
    "href": "slides/02-intro-bayes.html#kernel-of-a-pdfpmf",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Kernel of a pdf/pmf",
    "text": "Kernel of a pdf/pmf\n\nThe kernel is the form of the pdf or pmf in which any factors that are not functions of any of the variables in the domain are omitted.\nExamples:\n\n\nNormal: \\(\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\\)\nGamma: \\(x^{\\alpha-1}\\exp(-\\beta x)\\)\nBeta: \\(x^{\\alpha-1}(1-x)^{\\beta-1}\\)\nPoisson: \\(\\frac{\\lambda^x}{x!}\\)\n\n\nWe can use only the kernels to simplify the computation."
  },
  {
    "objectID": "slides/02-intro-bayes.html#posterior-distribution",
    "href": "slides/02-intro-bayes.html#posterior-distribution",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Posterior Distribution",
    "text": "Posterior Distribution\n\nset.seed(2023)\nn &lt;- 15\np &lt;- 0.3\nX &lt;- rbinom(n, 1, p)\ns &lt;- sum(X)\np_mle &lt;- s/n # MLE\nprint(X)\n\n [1] 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1\n\n\n\nlibrary(ggplot2)\nggplot(data = data.frame(p = c(0, 1)), aes(p)) + \n    lims(x = c(0, 1), y = c(0, 5)) + \n    labs(x = \"p\", y = \"Density\") +\n    geom_function(fun = dunif, aes(col = \"blue\")) + \n    geom_function(fun = dbeta, aes(col = \"red\"), \n                  args = list(shape1 = s + 1, shape2 = n - s + 1)) + \n    geom_vline(xintercept = p_mle, linetype = \"dashed\", col = \"darkgrey\") + \n    geom_vline(xintercept = p, linetype = \"dashed\", col = \"darkgreen\") + \n    scale_colour_manual(name = \"Distribution\", \n                        values = c(\"blue\", \"red\"),\n                        labels = c(\"Prior\", \"Posterior\"))"
  },
  {
    "objectID": "slides/02-intro-bayes.html#posterior-distribution-output",
    "href": "slides/02-intro-bayes.html#posterior-distribution-output",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Posterior Distribution",
    "text": "Posterior Distribution"
  },
  {
    "objectID": "slides/02-intro-bayes.html#posterior-inference",
    "href": "slides/02-intro-bayes.html#posterior-inference",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Posterior Inference",
    "text": "Posterior Inference\n\nThe posterior distribution of \\(p\\) contains much more information than the MLE.\nWe can use the posterior for:\n\nEstimation: \\(\\hat{p} = \\E(p \\mid X_1, \\ldots, X_n)\\) (posterior mean)\nPrediction: \\(\\P(X_{n+1} = 1 \\mid X_1, \\ldots, X_n)\\)\nInterval estimation: Find \\((L, U)\\) such that \\(\\P(L \\leq p \\leq U \\mid X_1, \\ldots, X_n) = 0.95\\)\n\nIn the Beta-Binomial model, an estimate for \\(p\\) is \\[\n\\hat{p} = \\E(p \\mid X_1, \\ldots, X_n) = \\frac{\\alpha + \\sum_{i=1}^n X_i}{\\alpha + \\beta + n}\n\\] whereas the MLE is \\(\\hat{p}_{\\text{MLE}} = \\frac{1}{n}\\sum_{i=1}^nX_i\\)."
  },
  {
    "objectID": "slides/02-intro-bayes.html#maximum-a-posteriori-estimate",
    "href": "slides/02-intro-bayes.html#maximum-a-posteriori-estimate",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Maximum-a-posteriori Estimate",
    "text": "Maximum-a-posteriori Estimate\n\nThe posterior mean is not the only estimate we can obtain from the posterior.\nAnother commonly used estimator is the maximum-a-posterior (MAP) estimate \\[\n\\hat{p}_{\\text{MAP}} = \\argmax_{0 &lt; p &lt; 1} \\pi(p \\mid X_1, \\ldots, X_n).\n\\]\nSince the mode of \\(\\text{Beta}(\\alpha, \\beta)\\) is \\(\\frac{\\alpha-1}{\\alpha+\\beta-2}\\) when \\(\\alpha, \\beta &gt; 1\\), \\[\n\\hat{p}_{\\text{MAP}} = \\frac{\\alpha -1 + \\sum_{i=1}^n X_i}{\\alpha + \\beta + n - 2}.\n\\]\nIf \\(\\alpha = \\beta = 1\\) and \\(\\sum X_i &gt; 1\\), then \\(\\hat{p}_{\\text{MAP}} = \\hat{p}_{\\text{MLE}} = \\bar{X}\\)."
  },
  {
    "objectID": "slides/02-intro-bayes.html#recall-laplaces-rule-of-succession",
    "href": "slides/02-intro-bayes.html#recall-laplaces-rule-of-succession",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Recall: Laplace’s Rule of Succession",
    "text": "Recall: Laplace’s Rule of Succession\nGiven binary iid random variables \\(X_1, \\ldots, X_n\\) with \\(\\sum_{i=1}^n X_i = s\\), then \\[\\P\\left(X_{n+1}=1 \\mid X_1+\\cdots+X_n=s\\right)=\\frac{s+1}{n+2}.\\]\nDerivation:\n\nLet \\(X_1, \\ldots, X_n \\iid \\ber(p)\\) and \\(p \\sim \\text{Beta}(1,1)\\) (the uniform prior).\nThe posterior is \\(p \\mid X_1 + \\cdots + X_n = s \\sim \\text{Beta}(s + 1, n-s + 1)\\).\nAssuming \\(X_{n+1} \\sim \\ber(p)\\) and \\(X_i\\)’s are iid conditioned on \\(p\\), \\[\\begin{align*}\n\\P\\left(X_{n+1}=1 \\mid \\sum X_i=s\\right)\n& = \\int \\P(X_{n+1}=1 \\mid p)\\pi\\left(p \\mid \\sum X_i = s\\right) dp\\\\\n& = \\int p \\pi\\left(p \\mid \\sum X_i = s\\right) dp\n= \\frac{s+1}{n+2}.\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/02-intro-bayes.html#interval-estimation-credible-intervalregion",
    "href": "slides/02-intro-bayes.html#interval-estimation-credible-intervalregion",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Interval Estimation: Credible Interval/Region",
    "text": "Interval Estimation: Credible Interval/Region\n\nFor frequentists, it’s called confidence interval/region.\nLet \\([L(X), U(X)]\\) be an interval for \\(\\theta\\) based on sample \\(X\\).\n\\(100\\times(1-\\alpha)\\%\\) Bayesian Coverage: \\(\\P(L(x) \\leq \\theta \\leq U(x) \\mid {\\color{magenta}X = x}) = 1-\\alpha\\).\n\ndescribes your information about the location of the true value of \\(\\theta\\) after you have observed \\(X = x\\)\n\n\\(100\\times(1-\\alpha)\\%\\) Frequentist Coverage: \\(\\P(L(X) \\leq \\theta \\leq U(X) \\mid {\\color{magenta}\\theta}) = 1-\\alpha\\)\n\ndescribes the probability that the interval will cover the true value before the data are observed\n\nThere are many ways to construct a credible interval:\n\nQuantile-based method\nHighest posterior density (HPD) region"
  },
  {
    "objectID": "slides/02-intro-bayes.html#quantile-based-method",
    "href": "slides/02-intro-bayes.html#quantile-based-method",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Quantile-based method",
    "text": "Quantile-based method\nTo find a \\(100\\times(1-\\alpha)\\%\\) credible interval for \\(\\theta\\):\n\nFind numbers \\(\\theta_{\\alpha / 2} &lt; \\theta_{1-\\alpha / 2}\\) such that\n\n\n\\(\\P\\left(\\theta&lt;\\theta_{\\alpha/2} \\mid X = x\\right)=\\alpha / 2\\);\n\\(\\P\\left(\\theta&gt;\\theta_{1-\\alpha / 2} \\mid X = x\\right)=\\alpha / 2\\).\n\n\nThe numbers \\(\\theta_{\\alpha / 2}, \\theta_{1-\\alpha / 2}\\) are the \\(\\alpha / 2\\) and \\(1-\\alpha / 2\\) posterior quantiles of \\(\\theta\\), and so \\[\\begin{align*}\n\\P\\left(\\theta \\in\\left[\\theta_{\\alpha / 2}, \\theta_{1-\\alpha / 2}\\right] \\mid X = x\\right) & =1-\\P\\left(\\theta \\notin\\left[\\theta_{\\alpha / 2}, \\theta_{1-\\alpha / 2}\\right] \\mid X = x\\right) \\\\\n& =1-\\left[\\P\\left(\\theta&lt;\\theta_{\\alpha / 2} \\mid X=x\\right)\\right.\\\\\n& \\qquad \\left.+\\P\\left(\\theta&gt;\\theta_{1-\\alpha / 2} \\mid X = x\\right)\\right] \\\\\n& =1-\\alpha.\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/02-intro-bayes.html#binomial-example",
    "href": "slides/02-intro-bayes.html#binomial-example",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Binomial Example",
    "text": "Binomial Example\nSuppose we observed \\(X=2\\) from a \\(\\bin(10, p)\\). Assume the uniform prior \\(p\\).\n\nalpha &lt;- 1; beta &lt;- 1 # uniform prior\nn &lt;- 10; X &lt;- 2 # data\n\nqbeta(c(0.025, 0.975), alpha + X, beta + n - X)\n\n[1] 0.06021773 0.51775585"
  },
  {
    "objectID": "slides/02-intro-bayes.html#highest-posterior-density-hpd-region",
    "href": "slides/02-intro-bayes.html#highest-posterior-density-hpd-region",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Highest posterior density (HPD) region",
    "text": "Highest posterior density (HPD) region\n\nDefinition 1 A \\(100 \\times(1-\\alpha) \\%\\) HPD region consists of a subset of the parameter space, \\(s(x) \\subset \\Theta\\) such that\n\n\\(\\P(\\theta \\in s(x) \\mid X = x)=1-\\alpha\\);\nIf \\(\\theta_a \\in s(x)\\), and \\(\\theta_b \\notin s(x)\\), then \\(\\pi\\left(\\theta_a \\mid X=x\\right)&gt;\\pi\\left(\\theta_b \\mid X=x\\right)\\).\n\n\n\nAn HPD region might not be an interval if the posterior density is multimodal (having multiple peaks)."
  },
  {
    "objectID": "slides/02-intro-bayes.html#binomial-example-1",
    "href": "slides/02-intro-bayes.html#binomial-example-1",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Binomial Example",
    "text": "Binomial Example\nLet’s compute the HPD for the previous example: \\(\\theta \\mid X = 2 \\sim \\text{Beta}(3, 9)\\).\n\nlibrary(HDInterval)\nhdi(qbeta, 0.95, shape1 = 3, shape2 = 9)\n\n     lower      upper \n0.04055517 0.48372366 \nattr(,\"credMass\")\n[1] 0.95\n\n\nThe HPD is narrower than the quantile-based interval.\n\n\n\n\nFig. 3.6 in Hoff’s book"
  },
  {
    "objectID": "slides/02-intro-bayes.html#wrap-up-for-the-beta-binomial-model",
    "href": "slides/02-intro-bayes.html#wrap-up-for-the-beta-binomial-model",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Wrap-up for the Beta-Binomial model",
    "text": "Wrap-up for the Beta-Binomial model\n\nThe Beta-Binomial model: \\[\\begin{align*}\nX \\mid p & \\sim \\bin(n,p)\\\\\np & \\sim \\text{Beta}(\\alpha,\\beta)\\\\\np \\mid X & \\sim \\text{Beta}(\\alpha + X, \\beta + n - X)\n\\end{align*}\\]\nNote that the posterior is in the same family of the prior: both of them are in the Beta family.\nIn this case, the Beta prior is called a conjugate prior for the Binomial model."
  },
  {
    "objectID": "slides/02-intro-bayes.html#catalog-for-conjugate-priors",
    "href": "slides/02-intro-bayes.html#catalog-for-conjugate-priors",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Catalog for conjugate priors",
    "text": "Catalog for conjugate priors\n\n\n\n\n\n\n\n\n\n\n\n\n\nSampling model\nParameter\nPrior\nPosterior\n\n\n\n\n\\(X \\sim \\bin(n, p)\\)\n\\(p\\)\n\\(\\text{Beta}(\\alpha_0, \\beta_0)\\)\n\\(\\text{Beta}(\\alpha_0 + X, \\beta_0+n-X)\\)\n\n\n\\(X \\sim N(\\mu, \\sigma^2)\\)\n\\(\\mu\\)\n\\(N(\\mu_0, \\sigma_0^2)\\)\n\\(N\\left(\\frac{1}{\\frac{1}{\\sigma_0^2}+\\frac{1}{\\sigma^2}}\\left(\\frac{\\mu_0}{\\sigma_0^2}+\\frac{ X}{\\sigma^2}\\right),\\left(\\frac{1}{\\sigma_0^2}+\\frac{1}{\\sigma^2}\\right)^{-1}\\right)\\)\n\n\n\\(X \\sim \\text{Poisson}(\\lambda)\\)\n\\(\\lambda\\)\n\\(\\text{Gamma}(\\alpha_0, \\beta_0)\\)\n\\(\\text{Gamma}(\\alpha_0 + X, \\beta_0+1)\\)\n\n\n\\(X \\sim \\text{Gamma}(\\alpha, \\beta)\\)\n\\(\\beta\\)\n\\(\\text{Gamma}(\\alpha_0, \\beta_0)\\)\n\\(\\text{Gamma}(\\alpha_0 + \\alpha, \\beta_0+X)\\)\n\n\n\\(X \\sim \\text{NB}(r, p)\\)\n\\(p\\)\n\\(\\text{Beta}(\\alpha_0, \\beta_0)\\)\n\\(\\text{Beta}(\\alpha_0 + r, \\beta_0+X)\\)\n\n\n\n\nMore can be found on Wiki\nExercise: Derive the posterior for the Normal-Normal model."
  },
  {
    "objectID": "slides/02-intro-bayes.html#real-data-example",
    "href": "slides/02-intro-bayes.html#real-data-example",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Real Data Example",
    "text": "Real Data Example\n\n\n2022年，基隆市男女嬰出生數分別為856及731，性別比例為1.1711（生下男嬰機率為0.5394）。\n同年，全台灣男女嬰出生數分別為71,208及66,205，性別比例為1.076（生下男嬰機率為0.5183）\n根據統計，全球人類自然出生性別比1.052（生下男嬰機率為0.5122）。\n基隆市自然出生性別比是否高於台灣平均？\n\n\n\n\n性別比 ＝ 男嬰數/女嬰數；生下男嬰機率 ＝ 男嬰數/(男嬰數+女嬰數)；生下男嬰機率 ＝ 性別比/(1+性別比)\n行政院資料庫Our World in Data"
  },
  {
    "objectID": "slides/02-intro-bayes.html#real-data-example-1",
    "href": "slides/02-intro-bayes.html#real-data-example-1",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Real Data Example",
    "text": "Real Data Example\n\nlibrary(tidyverse)\nlibrary(knitr)\n\nm &lt;- 856; f &lt;- 731\nprior_mean &lt;- c(0.5, 0.5122, 0.5122, 0.5122, 0.5122, 0.5122)\nalpha_plus_beta &lt;- c(2, 2, 10, 100, 1000, 10000)\nalpha &lt;- prior_mean * alpha_plus_beta\nbeta &lt;- alpha_plus_beta - alpha\n\n\ndata.frame(alpha, beta, prior_mean) |&gt; \n    mutate(alpha_plus_beta = alpha + beta,\n           post_mean = (alpha+m)/(alpha+beta+m+f),\n           ratio = post_mean/(1-post_mean),\n           post_int = paste0(\"[\", round(qbeta(0.025, alpha + m, beta + f), 3),\n                             \", \", round(qbeta(0.975, alpha + m, beta + f), 3), \"]\")) |&gt;\n    select(c(prior_mean, alpha_plus_beta, post_mean, ratio, post_int)) |&gt;\n    kable(format = \"markdown\", digits = 4,\n          col.names = c(\"Prior mean $\\\\frac{\\\\alpha}{\\\\alpha+\\\\beta}$\",\n                        \"$\\\\alpha + \\\\beta$\",\n                        \"Post. mean\", \"Gender ratio\",\n                        \"Post. 95% Interval\"))"
  },
  {
    "objectID": "slides/02-intro-bayes.html#real-data-example-1-output",
    "href": "slides/02-intro-bayes.html#real-data-example-1-output",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Real Data Example",
    "text": "Real Data Example\n\n\n\n\n\n\n\n\n\n\n\nPrior mean \\(\\frac{\\alpha}{\\alpha+\\beta}\\)\n\\(\\alpha + \\beta\\)\nPost. mean\nGender ratio\nPost. 95% Interval\n\n\n\n\n0.5000\n2\n0.5393\n1.1708\n[0.515, 0.564]\n\n\n0.5122\n2\n0.5393\n1.1708\n[0.515, 0.564]\n\n\n0.5122\n10\n0.5392\n1.1702\n[0.515, 0.564]\n\n\n0.5122\n100\n0.5378\n1.1634\n[0.514, 0.562]\n\n\n0.5122\n1000\n0.5289\n1.1226\n[0.51, 0.548]\n\n\n0.5122\n10000\n0.5159\n1.0658\n[0.507, 0.525]"
  },
  {
    "objectID": "slides/02-intro-bayes.html#some-follow-up-questions",
    "href": "slides/02-intro-bayes.html#some-follow-up-questions",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Some follow-up questions",
    "text": "Some follow-up questions\n\nIs the sample size (856+731=1587) large enough?\nWhich prior? Depend on how strong your prior belief is.\n\nNo prior knowledge \\(\\Rightarrow\\) Uniform prior\nReliable prior information \\(\\Rightarrow\\) prior with larger \\(\\alpha+\\beta\\)\n\nWith prior information, we can obtain better estimates when the sample size is small:\n\nFor example, think of a really small village in which only two boys and one girl were born last year.\nIf we don’t use any prior information, the gender ratio is 2.\n\nWhat if our prior information cannot be described by a Beta distribution?\n\nEx: our prior information indicates a multimodal distribution\nThe posterior will be complicated and we need some other methods for posterior inference."
  },
  {
    "objectID": "slides/02-intro-bayes.html#small-area-estimation-sae",
    "href": "slides/02-intro-bayes.html#small-area-estimation-sae",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Small Area Estimation (SAE)",
    "text": "Small Area Estimation (SAE)\n\nThe previous example is an example of an SAE problem.\n\n\n\nSmall area estimation is any of several statistical techniques involving the estimation of parameters for small sub-populations, generally used when the sub-population of interest is included in a larger survey1.\n\n\n\nThere is a hierarchical structure in SAE problems:\n\n\n\\[\n\\text{基隆} \\subset \\text{台灣} \\subset \\text{東亞} \\subset \\text{亞洲} \\subset{世界}\n\\]\n\n\nOne of the common frequentist models for SAE problems is the random effect model.\n\nWiki"
  },
  {
    "objectID": "slides/02-intro-bayes.html#small-area-estimation",
    "href": "slides/02-intro-bayes.html#small-area-estimation",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Small Area Estimation",
    "text": "Small Area Estimation\n\nBayesians typically use a hierarchical model to handle an SAE problem.\nIn this example, (conceptually) \\[\\begin{align*}\n  X \\mid p & \\sim \\bin\\left(n, p\\right)\\\\\n  p \\mid p_{\\text{TW}} & \\sim \\pi_1\\\\\n  p_{\\text{TW}} \\mid p_{\\text{World}} & \\sim \\pi_2\n\\end{align*}\\]\nNotations:\n\n\\(R = \\frac{p}{1-p}\\) is the Keelung’s gender ratio (parameter);\n\\(R_{\\text{TW}} = \\frac{p_{\\text{TW}}}{1-p_{\\text{TW}}}\\) is the Taiwan’s gender ratio (parameter);\n\\(p_{\\text{World}} = 0.5122\\)\n\nThis model is more complicated than the previous one, but descibes the data more accurately."
  },
  {
    "objectID": "slides/02-intro-bayes.html#what-is-good-and-bad-about-bayesian",
    "href": "slides/02-intro-bayes.html#what-is-good-and-bad-about-bayesian",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "What is good and bad about Bayesian?",
    "text": "What is good and bad about Bayesian?\n\nGood:\n\nconsistent and coherent: everything (sample or parameter) is a random variable\neverything is conditional: we only care about conditional independence rather than marginal independence\nstopping rule does not matter\n\nBad:\n\nthe choice of prior is subjective\ncomputationally challenging"
  },
  {
    "objectID": "slides/02-intro-bayes.html#exchangeability",
    "href": "slides/02-intro-bayes.html#exchangeability",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Exchangeability",
    "text": "Exchangeability\n\nDefinition 2 Let \\(p\\left(x_1, \\ldots, x_n\\right)\\) be the joint density of \\(X_1\\), \\(\\ldots, X_n\\). If \\(p\\left(x_1, \\ldots, x_n\\right)=p\\left(x_{\\pi(1)}, \\ldots, x_{\\pi(n)}\\right)\\) for all permutations \\(\\pi\\) of \\(\\{1, \\ldots, n\\}\\), then \\(X_1, \\ldots, X_n\\) are exchangeable.\n\n\nRoughly speaking, \\(X_1, \\ldots, X_n\\) are exchangeable if the subscript labels convey no information about the outcomes.\nApparently, independence implies exchangeability, but the converse is false.\nWhat is the relationship between conditional independence and exchangeability?"
  },
  {
    "objectID": "slides/02-intro-bayes.html#conditional-independence-and-exchangeability",
    "href": "slides/02-intro-bayes.html#conditional-independence-and-exchangeability",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Conditional independence and exchangeability",
    "text": "Conditional independence and exchangeability\n\nProposition 1 If \\(\\theta \\sim p(\\theta)\\) and \\(X_1, \\ldots, X_n\\) are conditionally i.i.d. given \\(\\theta\\), then marginally (unconditionally on \\(\\theta\\) ) \\(, X_1, \\ldots, X_n\\) are exchangeable.\n\n\nProof. Suppose \\(X_1, \\ldots, X_n\\) are conditionally iid given some unknown parameter \\(\\theta\\). Then for any permutation \\(\\pi\\) of \\(\\{1, \\ldots, n\\}\\) and any set of values \\(\\left(x_1, \\ldots, x_n\\right) \\in\\) \\(\\mc{X}^n\\) \\[\\begin{align*}\np\\left(x_1, \\ldots, x_n\\right) & =\\int p\\left(x_1, \\ldots, x_n \\mid \\theta\\right) p(\\theta) d \\theta & & \\text { (definition of marginal probability) } \\\\\n& =\\int\\left\\{\\prod_{i=1}^n p\\left(x_i \\mid \\theta\\right)\\right\\} p(\\theta) d \\theta & & \\text { ($X$'s are conditionally i.i.d.) } \\\\\n& =\\int\\left\\{\\prod_{i=1}^n p\\left(x_{\\pi(i)} \\mid \\theta\\right)\\right\\} p(\\theta) d \\theta & & \\text { (product does not depend on order) } \\\\\n& =p\\left(x_{\\pi(1)}, \\ldots x_{\\pi(n)}\\right) & & \\text { (definition of marginal probability) } .\n\\end{align*}\\]\n\n\n\nCh. 2.7 & 2.8 in Hoff’s book."
  },
  {
    "objectID": "slides/02-intro-bayes.html#de-finettis-theorem",
    "href": "slides/02-intro-bayes.html#de-finettis-theorem",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "de Finetti’s Theorem",
    "text": "de Finetti’s Theorem\n\nWe have seen that\n\n\n\\[\\begin{align*}\n\\left.\\begin{array}{l}\nX_1, \\ldots, X_n \\mid \\theta \\text { i.i.d } \\\\\n\\theta \\sim p(\\theta)\n\\end{array}\\right\\} \\Rightarrow X_1, \\ldots, X_n \\text { are exchangeable. }\n\\end{align*}\\]\n\n\nWhat about an arrow in the other direction?\n\n\nTheorem 1 (de Finetti) Let \\(X_i \\in \\mc{X}\\) for all \\(i \\in\\{1,2, \\ldots\\}\\). Suppose that, for any \\(n\\), \\(X_1, \\ldots, X_n\\) are exchangeable. Then our model can be written as \\[\\begin{align*}\np\\left(x_1, \\ldots, x_n\\right)=\\int\\left\\{\\prod_{i=1}^n p\\left(x_i \\mid \\theta\\right)\\right\\} p(\\theta) d \\theta\n\\end{align*}\\] for some parameter \\(\\theta\\), some prior distribution on \\(\\theta\\), and some sampling model \\(p(x \\mid \\theta)\\). The prior and sampling model depend on \\(p\\left(x_1, \\ldots, x_n\\right)\\)."
  },
  {
    "objectID": "slides/02-intro-bayes.html#de-finettis-theorem-1",
    "href": "slides/02-intro-bayes.html#de-finettis-theorem-1",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "de Finetti’s Theorem",
    "text": "de Finetti’s Theorem\n\n\nThe conclusion is\n\n\\[\\begin{align*}\n\\left.\\begin{array}{l}\nX_1, \\ldots, X_n \\mid \\theta \\text { are i.i.d. } \\\\\n\\theta \\sim p(\\theta)\n\\end{array}\\right\\} \\Leftrightarrow X_1, \\ldots, X_n \\text { are exchangeable for all } n \\text {. }\n\\end{align*}\\]\n\n\nThis justifies the use of prior distributions when samples are exchangeable.\nWhen is the condition “\\(X_1, \\ldots, X_n\\) are exchangeable for all \\(n\\)” reasonable?\n\n\\(X_1, \\ldots, X_n\\) are outcomes of a repeatable experiment;\n\\(X_1, \\ldots, X_n\\) are sampled from a finite population with replacement;\n\\(X_1, \\ldots, X_n\\) are sampled from an infinite population without replacement."
  },
  {
    "objectID": "slides/02-intro-bayes.html#stopping-rule",
    "href": "slides/02-intro-bayes.html#stopping-rule",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Stopping Rule",
    "text": "Stopping Rule\nLet \\(\\theta\\) be the probability of a particular coin landing on heads, and suppose we want to test the hypotheses \\[\\begin{align*}\nH_0: \\theta=1 / 2, \\quad H_1: \\theta&gt;1 / 2\n\\end{align*}\\] at a significance level of \\(\\alpha=0.05\\). Suppose we observe the following sequence of flips: \\[\n\\text{heads, heads, heads, heads, heads, tails (5 heads, 1 tails)}\n\\]\n\nTo perform a frequentist hypothesis test, we must define a random variable to describe the data.\nThe proper way to do this depends on exactly which of the following two experiments was actually performed:\n\n\n\nExample 1.1 in Essential Bayes"
  },
  {
    "objectID": "slides/02-intro-bayes.html#stopping-rule-1",
    "href": "slides/02-intro-bayes.html#stopping-rule-1",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Stopping Rule",
    "text": "Stopping Rule\nSuppose the experiment is “Flip six times and record the results.”\n\n\\(X\\) counts the number of heads, and \\(X \\sim \\bin(6, \\theta)\\).\nThe observed data was \\(x=5\\), and the \\(p\\)-value of our hypothesis test is \\[\\begin{align*}\np\\text{-value} & =\\P_{\\theta=1 / 2}(X \\geq 5) \\\\\n& =\\P_{\\theta=1 / 2}(X=5)+\\P_{\\theta=1 / 2}(X=6) \\\\\n& =\\frac{6}{64}+\\frac{1}{64}=\\frac{7}{64}=0.109375&gt;0.05 .\n\\end{align*}\\] So we fail to reject \\(H_0\\) at \\(\\alpha=0.05\\)."
  },
  {
    "objectID": "slides/02-intro-bayes.html#stopping-rule-2",
    "href": "slides/02-intro-bayes.html#stopping-rule-2",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Stopping Rule",
    "text": "Stopping Rule\nSuppose now the experiment is “Flip until we get tails.”\n\n\\(X\\) counts the number of the flip on which the first tails occurs, and \\(X \\sim \\text{Geometric}(1-\\theta)\\).\nThe observed data was \\(x=6\\), and the p-value of our hypothesis test is \\[\\begin{align*}\np \\text{-value} & = \\P_{\\theta=1 / 2}(X \\geq 6) \\\\\n& =1-\\P_{\\theta=1 / 2}(X&lt;6) \\\\\n& =1-\\sum_{x=1}^5 \\P_{\\theta=1 / 2}(X=x) \\\\\n& =1-\\left(\\frac{1}{2}+\\frac{1}{4}+\\frac{1}{8}+\\frac{1}{16}+\\frac{1}{32}\\right)=\\frac{1}{32}=0.03125&lt;0.05\n\\end{align*}\\] So we reject \\(H_0\\) at \\(\\alpha=0.05\\)."
  },
  {
    "objectID": "slides/02-intro-bayes.html#stopping-rule-3",
    "href": "slides/02-intro-bayes.html#stopping-rule-3",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Stopping Rule",
    "text": "Stopping Rule\n\nThe result our hypothesis test depends on whether we would have stopped flipping if we had gotten a tails sooner.\nThe tests are dependent on what we call the stopping rule.\nThe likelihood for the actual value of \\(x\\) that was observed is the same for both experiments (up to a constant): \\[\\begin{align*}\np(x \\mid \\theta) \\propto \\theta^5(1-\\theta) .\n\\end{align*}\\]\nA Bayesian approach would take the data into account only through this likelihood.\nHomework: Show that under a Beta prior, the posteriors under the two stopping rules are the same."
  },
  {
    "objectID": "slides/02-intro-bayes.html#more-importantly",
    "href": "slides/02-intro-bayes.html#more-importantly",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "More importantly …",
    "text": "More importantly …\n\n\n\n\n\n\n\n\nHome\n\n\nImage source: https://twitter.com/LSpakeAnthro/status/1257766583629275137"
  },
  {
    "objectID": "slides/04-estimation.html#introduction",
    "href": "slides/04-estimation.html#introduction",
    "title": "Lecture 04: Decision Theory and Bayesian Estimation",
    "section": "Introduction",
    "text": "Introduction\n\n\\[\n\\newcommand{\\mc}[1]{\\mathcal{#1}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\E}{\\mathbb{E}}\n\\renewcommand{\\P}{\\mathbb{P}}\n\\newcommand{\\var}{{\\rm Var}} % Variance\n\\newcommand{\\mse}{{\\rm MSE}} % MSE\n\\newcommand{\\bias}{{\\rm Bias}} % MSE\n\\newcommand{\\cov}{{\\rm Cov}} % Covariance\n\\newcommand{\\iid}{\\stackrel{\\rm iid}{\\sim}}\n\\newcommand{\\ind}{\\stackrel{\\rm ind}{\\sim}}\n\\renewcommand{\\choose}[2]{\\binom{#1}{#2}}  % Choose\n\\newcommand{\\chooses}[2]{{}_{#1}C_{#2}}  % Small choose\n\\newcommand{\\cd}{\\stackrel{d}{\\rightarrow}}\n\\newcommand{\\cas}{\\stackrel{a.s.}{\\rightarrow}}\n\\newcommand{\\cp}{\\stackrel{p}{\\rightarrow}}\n\\newcommand{\\bin}{{\\rm Bin}}\n\\newcommand{\\ber}{{\\rm Ber}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\]\n\n\nWe have seen that the determination of prior can be very complicated and different prior leads to different inference procedures.\nIn this lecture, we will see how we can evaluate and compare different inference procedures.\nMore importantly, besides offering coherent inference procedures, the Bayesian paradigm also enjoys some frequentist optimality properties.\nThe statistical inferential framework we focus in this lecture is called Decision Theory."
  },
  {
    "objectID": "slides/04-estimation.html#evaluating-estimators",
    "href": "slides/04-estimation.html#evaluating-estimators",
    "title": "Lecture 04: Decision Theory and Bayesian Estimation",
    "section": "Evaluating estimators",
    "text": "Evaluating estimators\n\nDecision theory can be applied to any kind of statistical problems, e.g., testing, estimation, etc.\nWe will temporarily focus on the statistical estimation setting.\nLet \\(\\Theta\\) be the parameter space and \\(\\mc{D}\\) be the decision space, which contain all possible decisions.\nIn standard estimation setting, \\(\\mc{D} = \\Theta\\).\n\n\nDefinition 1 A loss function is any function \\(L\\) from \\(\\Theta \\times \\mc{D}\\) to \\([0,+\\infty)\\)."
  },
  {
    "objectID": "slides/04-estimation.html#loss-function",
    "href": "slides/04-estimation.html#loss-function",
    "title": "Lecture 04: Decision Theory and Bayesian Estimation",
    "section": "Loss function",
    "text": "Loss function\n\nThis loss function is supposed to evaluate the penalty (or error) \\(L(\\theta, d)\\) associated with the decision \\(d\\) when the parameter takes the value \\(\\theta\\).\nA utility function is just the opposite of a loss function, e.g. \\(U(\\theta, d) = -L(\\theta, d)\\).\nIntuitively, a good decision \\(d\\) minimizes the loss (maximizes the utility) at every \\(\\theta\\).\nIs this possible?"
  },
  {
    "objectID": "slides/04-estimation.html#bayesian-decision-theory",
    "href": "slides/04-estimation.html#bayesian-decision-theory",
    "title": "Lecture 04: Decision Theory and Bayesian Estimation",
    "section": "Bayesian Decision Theory",
    "text": "Bayesian Decision Theory\n\nBayesian statistical inference should start with the rigorous determination of three factors:\n\nthe distribution family for the observations, \\(f(x\\mid\\theta)\\);\nthe prior distribution for the parameters, \\(\\pi(\\theta)\\);\nthe loss associated with the decisions, \\(L(\\theta, \\delta)\\);\n\nBad news: The determination of loss is as complicated as the determination of prior.\nActually, Lindley (1985) states that loss and prior are difficult to separate and should be analyzed simultaneously.\n\n\n\nLindley, D.V. (1985) Making Decisions (2nd edition). J. Wiley, New York."
  },
  {
    "objectID": "slides/04-estimation.html#commonly-used-loss-functions",
    "href": "slides/04-estimation.html#commonly-used-loss-functions",
    "title": "Lecture 04: Decision Theory and Bayesian Estimation",
    "section": "Commonly used loss functions",
    "text": "Commonly used loss functions\n\nSquared error loss: \\(L(\\theta, d) = (\\theta - d)^2\\)\nWeighted loss: \\(L(\\theta, d) = w(\\theta)(\\theta-d)^2\\)\n\nusing this loss with prior \\(\\pi(\\theta)\\) is the same as using squared error loss with prior \\(\\pi(\\theta)w(\\theta)\\)\n\nAbsolute error: \\(L(\\theta, d) = |\\theta - d|\\)\nThe 0-1 loss: \\(L(\\theta, d) = I(\\theta \\neq d)\\)\nIntrinsic loss:\n\nKullback-Leibler divergence \\(L_{\\text{KL}}(\\theta, \\delta) = \\E_\\theta\\left[\\log \\left(\\frac{f(x \\mid \\theta)}{f(x \\mid \\delta)}\\right)\\right]\\)\nHellinger loss \\(L_{\\text{H}}(\\theta, \\delta)=\\frac{1}{2} \\E_\\theta\\left[\\left(\\sqrt{\\frac{f(x \\mid \\delta)}{f(x \\mid \\theta)}}-1\\right)^2\\right] = \\frac{1}{2}\\int \\left(\\sqrt{f(x\\mid\\theta)}-\\sqrt{f(x\\mid\\delta)}\\right)^2 dx\\)"
  },
  {
    "objectID": "slides/04-estimation.html#example",
    "href": "slides/04-estimation.html#example",
    "title": "Lecture 04: Decision Theory and Bayesian Estimation",
    "section": "Example",
    "text": "Example\n\n\nFor \\(X \\sim N(\\theta, 1)\\), we have\n\n\\[\\begin{align*}\nL_{\\mathrm{KL}}(\\theta, \\delta) & =\\frac{1}{2} \\E_\\theta\\left[-(x-\\theta)^2+(x-\\delta)^2\\right]=\\frac{1}{2}(\\delta-\\theta)^2 \\\\\nL_{\\mathrm{H}}(\\theta, \\delta) & =1-\\exp \\left\\{-(\\delta-\\theta)^2 / 8\\right\\}.\n\\end{align*}\\]\n\n\n\nFor \\(X \\sim N(0, \\sigma^2)\\), we have\n\n\\[\\begin{align*}\nL_{\\mathrm{KL}}(\\sigma, \\delta) & =\\log \\frac{\\delta}{\\sigma}+\\frac{\\sigma^2}{2 \\delta^2}-\\frac{1}{2} \\\\\nL_{\\mathrm{H}}(\\sigma, \\delta) & =1-\\sqrt{\\frac{2 \\sigma \\delta}{\\sigma^2+\\delta^2}}.\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/04-estimation.html#statistical-model",
    "href": "slides/04-estimation.html#statistical-model",
    "title": "Lecture 04: Decision Theory and Bayesian Estimation",
    "section": "Statistical model",
    "text": "Statistical model\n\nFrom a decision-theoretic point of view, the statistical model now involves three spaces:\n\n\\(\\mc{X}\\), observation space (or sample space),\n\\(\\Theta\\), parameter space, and\n\\(\\mc{D}\\), decision space (or action space).\n\nWe then need to determine a loss function \\(L(\\theta, d)\\).\nThe goal is to find a decision rule \\(\\delta: \\mc{X} \\to \\mc{D}\\), such that the loss \\(L(\\theta, \\delta(x))\\) is minimized.\nExcept for trivial settings, it is generally impossible to uniformly minimize (in \\(d\\)) the loss function \\(L(\\theta, d)\\) when \\(\\theta\\) is unknown."
  },
  {
    "objectID": "slides/04-estimation.html#risk",
    "href": "slides/04-estimation.html#risk",
    "title": "Lecture 04: Decision Theory and Bayesian Estimation",
    "section": "Risk",
    "text": "Risk\nTo derive an effective comparison criterion from the loss function, we have\n\nFrequentist Risk: average over unknown \\(x\\) (samples) \\[\\begin{align*}\nR(\\theta, \\delta)  =\\E_\\theta[L(\\theta, \\delta(X))]\n=\\int_{\\mc{X}} L(\\theta, \\delta(x)) f(x \\mid \\theta) d x\n\\end{align*}\\]\nPosterior Risk: average over unknown \\(\\theta\\) (parameters) \\[\\begin{align*}\n\\rho(\\pi, d \\mid x)  =\\E^\\pi[L(\\theta, d) \\mid x]\n=\\int_{\\Theta} L(\\theta, d) \\pi(\\theta \\mid x) d \\theta\n\\end{align*}\\]\nIntegrated Risk: average over both \\(\\theta\\) and \\(x\\) \\[\\begin{align*}\nr(\\pi, \\delta) =\\mathbb{E}^\\pi[R(\\theta, \\delta)]\n=\\int_{\\Theta} \\int_{\\mathcal{X}} \\mathrm{L}(\\theta, \\delta(x)) f(x \\mid \\theta) d x \\pi(\\theta) d \\theta\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/04-estimation.html#frequentist-risk",
    "href": "slides/04-estimation.html#frequentist-risk",
    "title": "Lecture 04: Decision Theory and Bayesian Estimation",
    "section": "Frequentist Risk",
    "text": "Frequentist Risk\n\nGiven \\(\\delta\\), \\(R(\\theta, \\delta)\\) is a function of \\(\\theta\\).\nThe value \\(R(\\theta, \\delta)\\) is the long-run performance of \\(\\delta\\) when the true parameter is \\(\\theta\\).\nThe goal is to select a decision rule that has the best long-run performance uniformly in \\(\\theta\\), which is generally impossible.\nEven if we find a decision rule with good long-run performance, we are not able to evaluate the performance for the given observation \\(x\\).\nFinally, using this risk implicitly assumes that the same problem will be met again and again."
  },
  {
    "objectID": "slides/04-estimation.html#posterior-risk",
    "href": "slides/04-estimation.html#posterior-risk",
    "title": "Lecture 04: Decision Theory and Bayesian Estimation",
    "section": "Posterior Risk",
    "text": "Posterior Risk\n\nGiven \\(x\\), \\(\\rho(\\pi, d \\mid x)\\) is the average error resulting from decision \\(d\\).\nThe posterior risk is a function of \\(x\\), which is known.\nBy Fubini’s Theorem, \\[\n    r(\\pi, \\delta) = \\int \\rho(\\pi, \\delta(x) \\mid x) m(x)dx\n\\] where \\(m(x) = \\int f(x\\mid\\theta)\\pi(\\theta)d\\theta\\) is the marginal distribution.\nUnlike the frequentist risk which associates a function to \\(\\delta\\), the integrated risk associates a real number to \\(\\delta\\)."
  },
  {
    "objectID": "slides/04-estimation.html#bayes-estimator",
    "href": "slides/04-estimation.html#bayes-estimator",
    "title": "Lecture 04: Decision Theory and Bayesian Estimation",
    "section": "Bayes Estimator",
    "text": "Bayes Estimator\n\nGiven a prior distribution \\(\\pi\\) and a loss function \\(L\\)\nA Bayes estimator is any estimator \\(\\delta^\\pi\\) which minimizes \\(r(\\pi, \\delta)\\).\nFor every \\(x \\in \\mathcal{X}\\), it is given by \\(\\delta^\\pi(x)\\), argument of \\(\\min _d \\rho(\\pi, d \\mid x)\\).\nThe value \\(r(\\pi)=r\\left(\\pi, \\delta^\\pi\\right)\\) is then called the Bayes risk.\nFor example, posterior mean is the Bayes estimator under the squared error loss \\(L(\\theta, d) = (\\theta - d)^2\\) and posterior median is the Bayes estimator under \\(L(\\theta, d) = |\\theta-d|\\).\nWhen \\(\\pi\\) is improper, \\(r(\\pi, \\delta)\\) might not be finite. In this case, we \\(\\delta^{\\pi}(x)\\) as the minimum of the minimizer of the posterior risk and call it a generalized Bayes estimator."
  },
  {
    "objectID": "slides/04-estimation.html#the-0-1-loss-and-map",
    "href": "slides/04-estimation.html#the-0-1-loss-and-map",
    "title": "Lecture 04: Decision Theory and Bayesian Estimation",
    "section": "The 0-1 loss and MAP",
    "text": "The 0-1 loss and MAP\n\nThis loss is mainly used in the classical approach to hypothesis testing.\nFor discrete parameters, the Bayes estimator under the 0-1 loss is the MAP.\nFor example, consider \\(\\Theta = \\{0, 1\\}\\) and the posterior is \\(\\pi(\\theta = 1 \\mid x) = p(x) = 1 - \\pi(\\theta = 0 \\mid x)\\).\nThe posterior risk is \\[\n    \\rho(\\pi, d \\mid x) = \\E_{\\theta\\mid x}L(\\theta, d) = \\begin{cases}\n1 - p(x), & d = 1\\\\\np(x), & d = 0\n\\end{cases}.\n\\]\nThe Bayes estimator is \\[\n    \\delta^{\\pi}(x) = \\argmin_d \\rho(\\pi, d \\mid x) =  \\begin{cases}\n1, & \\pi(\\theta = 1 \\mid x) &gt; 1/2\\\\\n0, & \\pi(\\theta = 0 \\mid x) &gt; 1/2\n\\end{cases},\n\\] which is the MAP."
  },
  {
    "objectID": "slides/04-estimation.html#the-0-1-loss-and-map-1",
    "href": "slides/04-estimation.html#the-0-1-loss-and-map-1",
    "title": "Lecture 04: Decision Theory and Bayesian Estimation",
    "section": "The 0-1 loss and MAP",
    "text": "The 0-1 loss and MAP\n\nFor continuous parameters, it is slightly more complicated.\nFor any \\(d \\in \\Theta\\), the posterior risk is \\[\n\\rho(\\pi, d \\mid x) = \\E_{\\theta\\mid x}I(\\theta \\neq d) = \\P(\\theta \\neq d \\mid x) = 1,\n\\] which is independent of \\(d\\).\nHence every \\(d \\in \\Theta\\) is a minimizer of the posterior risk.\nHowever if we replace the 0-1 loss by a sequence of losses, \\(L_{\\varepsilon}(\\theta, d) = I(\\|\\theta-d\\| &gt; \\varepsilon)\\), the MAP estimate is then the limit of the Bayes estimates associated with \\(L_{\\varepsilon}\\), when \\(\\varepsilon \\to 0\\)."
  },
  {
    "objectID": "slides/04-estimation.html#a-quick-wrap-up",
    "href": "slides/04-estimation.html#a-quick-wrap-up",
    "title": "Lecture 04: Decision Theory and Bayesian Estimation",
    "section": "A quick wrap-up",
    "text": "A quick wrap-up\n\nTo perform a decision-theoretic evaluation for estimators (or any other statistical inference), we need to first choose a loss function \\(L\\).\nWith \\(L\\), we can obtain different risks by averaging over different spaces.\nAs a Bayesian, only the posterior risk \\(\\rho(\\pi, d\\mid x)\\) is important.\nThe integrated risk \\(r(\\pi, \\delta)\\) actually connects the posterior risk and the frequentist risk.\nIt also explains why Bayes estimators play an important role in frequentist optimality criteria."
  },
  {
    "objectID": "slides/04-estimation.html#two-optimalities",
    "href": "slides/04-estimation.html#two-optimalities",
    "title": "Lecture 04: Decision Theory and Bayesian Estimation",
    "section": "Two optimalities",
    "text": "Two optimalities\n\nThere are two fundamental notions of frequentist Decision Theory: minimaxity and admissibility.\nUnder the frequentist paradigm, there is no single optimal estimator.\nWe will see that Bayes estimators are often optimal for the frequentist concepts of optimality."
  },
  {
    "objectID": "slides/04-estimation.html#minimaxity",
    "href": "slides/04-estimation.html#minimaxity",
    "title": "Lecture 04: Decision Theory and Bayesian Estimation",
    "section": "Minimaxity",
    "text": "Minimaxity\n\nIt is an insurance against the worst case because it aims at minimizing the expected loss in the least favorable case.\nThe minimax risk associated with a loss function \\(L\\) is the value \\[\n\\bar{R}=\\inf _{\\delta \\in \\mc{D}} \\sup _\\theta R(\\theta, \\delta)=\\inf _{\\delta \\in \\mc{D}} \\sup_\\theta \\E_\\theta[L(\\theta, \\delta(X))].\n\\]\nA minimax estimator is any estimator \\(\\delta_0\\) such that \\[\n\\sup _\\theta R\\left(\\theta, \\delta_0\\right)=\\bar{R} .\n\\]\nIn other words, a minimax estimator has the best worst-case performance.\nThe notion of minimaxity provides a good illustration of the conservative aspects of the frequentist paradigm."
  },
  {
    "objectID": "slides/04-estimation.html#bayes-esitmator-and-minimaxity",
    "href": "slides/04-estimation.html#bayes-esitmator-and-minimaxity",
    "title": "Lecture 04: Decision Theory and Bayesian Estimation",
    "section": "Bayes esitmator and minimaxity",
    "text": "Bayes esitmator and minimaxity\n\nIn fact, from a Bayesian point of view, it is often equivalent to take a prior concentrated on these worst cases.\nIn some cases, a minimax estimator is also a Bayes estimator, that is, there exists a prior such that the Bayes estimator is minimax.\nSuch prior is called a least favorable prior, which has the largest Bayes risk among all other priors.\nIf you need a conservative Bayes estimate, you can choose the least favorable prior (if it exists).\nEven if a minimax estimator is not Bayes, it is often the limit of Bayes estimators, e.g., the MLE for \\(N(\\theta, 1)\\).\nA useful approach to the construction of minimax estimators is called a limiting Bayes approach.1\n\nSee Theorem 1.12 in Ch. 6 of Theory of Point Estimation for example."
  },
  {
    "objectID": "slides/04-estimation.html#admissibility",
    "href": "slides/04-estimation.html#admissibility",
    "title": "Lecture 04: Decision Theory and Bayesian Estimation",
    "section": "Admissibility",
    "text": "Admissibility\n\nAn estimator \\(\\delta_0\\) is inadmissible if there exists an estimator \\(\\delta_1\\) which dominates \\(\\delta_0\\), that is, such that, for every \\(\\theta\\), \\[\\begin{align*}\nR\\left(\\theta, \\delta_0\\right) \\geq R\\left(\\theta, \\delta_1\\right)\n\\end{align*}\\] and, for at least one value \\(\\theta_0\\) of the parameter, \\[\\begin{align*}\nR\\left(\\theta_0, \\delta_0\\right)&gt;R\\left(\\theta_0, \\delta_1\\right) .\n\\end{align*}\\]\nOtherwise, \\(\\delta_0\\) is said to be admissible."
  },
  {
    "objectID": "slides/04-estimation.html#example-1",
    "href": "slides/04-estimation.html#example-1",
    "title": "Lecture 04: Decision Theory and Bayesian Estimation",
    "section": "Example",
    "text": "Example\n\n\n\nStudent\n國文\n英文\n數學\n自然\n社會\n\n\n\n\nA\n80\n90\n95\n85\n85\n\n\nB\n70\n80\n85\n75\n75\n\n\nC\n85\n90\n95\n90\n60\n\n\nD\n90\n80\n50\n60\n80\n\n\nE\n80\n80\n80\n80\n80\n\n\n\nAmong these 5 students:\n\nA and E are minimax, since their have the highest worst score, 80.\nB and E are inadmissible, since A dominates B and E; A, C, and D are admissible.\nThe Bayes estimator under the prior (1,1,1,1,1) is A; the Bayes estimator under the prior (1,0,0,0,0) is D."
  },
  {
    "objectID": "slides/04-estimation.html#steins-paradox",
    "href": "slides/04-estimation.html#steins-paradox",
    "title": "Lecture 04: Decision Theory and Bayesian Estimation",
    "section": "Stein’s Paradox",
    "text": "Stein’s Paradox\n\nLet \\(X \\sim N_p(\\theta, I)\\) and \\(p&gt;2\\). Then \\(X\\) is the MLE for \\(\\theta\\) and is inadmissible.\nThe MLE is dominated by the James-Stein estimator \\[\n\\hat{\\theta}^{\\text{JS}} = \\left(1 - \\frac{p-2}{\\|X\\|^2}\\right)X.\n\\]\nThe James-Stein estimator is still inadmissible and is dominated by the positive-part James-Stein estimator1 \\[\n\\hat{\\theta}^{\\text{JS+}} = \\left(1 - \\frac{p-2}{\\|X\\|^2}\\right)_+X,\n\\] where \\((x)_+ = \\max(x, 0)\\).\nThe positive-part James-Stein estimator is still inadmissible since it is not differentiable.\n\nAll these three estimators are minimax."
  },
  {
    "objectID": "slides/04-estimation.html#optimality-of-bayes-estimators",
    "href": "slides/04-estimation.html#optimality-of-bayes-estimators",
    "title": "Lecture 04: Decision Theory and Bayesian Estimation",
    "section": "Optimality of Bayes estimators",
    "text": "Optimality of Bayes estimators\n\nWhat is more important is whether the Bayes risk is finite rather than the propriety of the prior.\nFinite Bayes risk: (regular) Bayes estimator;\nInfinite Bayes risk: generalized Bayes estimator\nUnique (regular) Bayes \\(\\Rightarrow\\) admissible\n\nWhen \\(L(\\theta, d)\\) is strictly convex in \\(d\\), the Bayes estimator is unique.\nFor example, with a proper prior, the posterior mean is admissible.\nWith an improper prior, the posterior mean is admissible if it has finite Bayes risk."
  },
  {
    "objectID": "slides/04-estimation.html#optimality-of-bayes-estimators-1",
    "href": "slides/04-estimation.html#optimality-of-bayes-estimators-1",
    "title": "Lecture 04: Decision Theory and Bayesian Estimation",
    "section": "Optimality of Bayes estimators",
    "text": "Optimality of Bayes estimators\n\nUnique (regular) Bayes with constant (frequentist) risk \\(\\Rightarrow\\) admissible and minimax\n\nIf \\(X \\mid p \\sim \\bin(n, p)\\) and \\(p \\sim \\text{Beta}(\\sqrt{n}/2, \\sqrt{n}/2)\\), the posterior mean of \\(p\\) is admissible and minimax (under the squared error loss).\nThis prior \\(\\text{Beta}(\\sqrt{n}/2, \\sqrt{n}/2)\\) is called a least favorable prior.\nA least favorable prior, if it exists, gives a conservative Bayes estimate.\n\n\n\n\n\n\nConclusion\n\n\nBayes estimators are admissible. It can also be minimax with a particular prior."
  },
  {
    "objectID": "slides/04-estimation.html#bayesian-estimation-1",
    "href": "slides/04-estimation.html#bayesian-estimation-1",
    "title": "Lecture 04: Decision Theory and Bayesian Estimation",
    "section": "Bayesian estimation",
    "text": "Bayesian estimation\n\nWhen the prior distribution \\(\\pi(\\theta)\\) is available, the posterior distribution \\(\\pi(\\theta \\mid x)\\) can be formally derived from the observation \\(x\\) with distribution \\(f(x \\mid \\theta)\\).\nThis updated distribution integrates simultaneously prior information about \\(\\theta\\) and information brought by the observation \\(x\\).\nEven though \\(\\theta\\) is not necessarily a random variable, the distribution \\(\\pi(\\theta\\mid x)\\) can be used as a regular probability distribution to describe the properties of \\(\\theta\\).\nSummarizing indices for \\(\\pi(\\theta\\mid x)\\) such as the posterior mean, the posterior mode, the posterior variance, and the posterior median, can be used."
  },
  {
    "objectID": "slides/04-estimation.html#map-estimator",
    "href": "slides/04-estimation.html#map-estimator",
    "title": "Lecture 04: Decision Theory and Bayesian Estimation",
    "section": "MAP estimator",
    "text": "MAP estimator\n\nThere is no way of selecting a best estimator, without using a loss criterion.\nNonetheless, a possible estimator of \\(\\theta\\) based on \\(\\pi(\\theta \\mid x)\\) is the maximum a posteriori (MAP) estimator, defined as the posterior mode: \\[\n\\hat{\\theta}_{\\text{MAP}} = \\argmax_\\theta \\pi(\\theta \\mid x) = \\argmax_{\\theta} \\ell(\\theta\\mid x) \\pi(\\theta)\n\\] where \\(\\ell(\\theta\\mid x)\\) is the likelihood function.\nNote that the MAP estimator also bypasses the computation of the marginal \\(m(x) = \\int \\ell(\\theta\\mid x)\\pi(\\theta)d\\theta\\).\nRecall that the MAP estimator is a Bayes estimator (in the decision-theoretic sense) under the 0-1 loss when \\(\\theta\\) is discrete and is the limit of Bayes estimators when \\(\\theta\\) is continuous."
  },
  {
    "objectID": "slides/04-estimation.html#penalized-maximum-likelihood-estimator",
    "href": "slides/04-estimation.html#penalized-maximum-likelihood-estimator",
    "title": "Lecture 04: Decision Theory and Bayesian Estimation",
    "section": "Penalized maximum likelihood estimator",
    "text": "Penalized maximum likelihood estimator\n\nFor frequentists, a common approach to preventing overfitting is through penalization or regularization.\nA typical penalized MLE is of the form \\[\n\\hat{\\theta} = \\argmax \\log \\ell(\\theta \\mid x) - \\text{Penalty}(\\theta).\n\\]\nThe MAP estimator can be expressed as a penalized MLE: \\[\\begin{align*}\n\\hat{\\theta}_{\\text{MAP}} & = \\argmax_{\\theta} \\ell(\\theta\\mid x) \\pi(\\theta)\n= \\argmax_{\\theta} \\log \\ell(\\theta\\mid x) + \\log \\pi(\\theta).\n\\end{align*}\\]\nThat is, the penalty of \\(\\theta\\) is given through the prior: \\(\\text{Penalty}(\\theta) = - \\log \\pi(\\theta)\\).\nSpecifying a prior is equivalent to specifying a penalty term for frequentists."
  },
  {
    "objectID": "slides/04-estimation.html#penalized-regularized-regression",
    "href": "slides/04-estimation.html#penalized-regularized-regression",
    "title": "Lecture 04: Decision Theory and Bayesian Estimation",
    "section": "Penalized (regularized) regression",
    "text": "Penalized (regularized) regression\n\nLinear regression: \\(Y = X \\beta + \\varepsilon\\), where \\(X\\) is a \\(k\\times p\\) covariate matrix, \\(\\beta \\in \\R^p\\) is the unknown parameter, and \\(\\varepsilon \\sim N_k(0, \\sigma^2I_k)\\).\nThe (unregularized) ordinary least-square estimator1 of \\(\\beta\\) is \\[\n    \\hat{\\beta}_{\\text{OLS}} = \\argmin_{\\beta} \\|Y - X\\beta\\|^2 = (X^TX)^{-1}X^TY.\n\\]\nSome commonly used regularization:\n\nRidge regression: \\(\\text{Penalty}(\\beta) = -\\lambda\\|\\beta\\|^2 \\Leftrightarrow \\beta_i \\iid N(0, \\lambda^{-1})\\).\nLasso2: \\(\\text{Penalty}(\\beta) = -\\lambda\\|\\beta\\|_1 = \\lambda\\sum_{i=1}^p|\\beta_i| \\Leftrightarrow \\beta_i \\iid \\text{Laplace}(0, \\lambda^{-1})\\)\n\nWhenever you’re applying regularization techniques, you are essentially doing Bayesian analysis.\n\nThe OLS estimate is the MLE under the normality assumption on \\(\\varepsilon\\).Least Absolute Shrinkage and Selection Operator"
  },
  {
    "objectID": "slides/04-estimation.html#example-2",
    "href": "slides/04-estimation.html#example-2",
    "title": "Lecture 04: Decision Theory and Bayesian Estimation",
    "section": "Example",
    "text": "Example\n\nHowever, the MAP might not always be useful.\nConsider \\(x \\sim \\mathcal{C}(\\theta, 1)\\), i.e., \\[\\begin{align*}\nf(x \\mid \\theta)=\\frac{1}{\\pi}\\left[1+(x-\\theta)^2\\right]^{-1},\n\\end{align*}\\] and \\(\\pi(\\theta)=\\frac{1}{2} e^{-|\\theta|}\\).\nThe MAP estimator of \\(\\theta\\) is then \\(\\delta^*(x)=0\\), as the maximum of \\(\\exp (-|\\theta|)\\left[1+(x-\\theta)^2\\right]^{-1}\\) is attained for \\(\\theta=0\\), whatever the value of \\(x\\).\nThis behavior may be explained by the flatness of the likelihood function, which is not informative enough, compared with the sharp prior distribution.\nHowever, from a practical point of view, this estimator is useless."
  },
  {
    "objectID": "slides/04-estimation.html#posterior-mean-vs-map",
    "href": "slides/04-estimation.html#posterior-mean-vs-map",
    "title": "Lecture 04: Decision Theory and Bayesian Estimation",
    "section": "Posterior mean vs MAP",
    "text": "Posterior mean vs MAP\n\nWhich is better?\nIf we adopt the decision theory framework and confine ourselves to the squared error loss \\(L(\\theta, d) = (\\theta-d)^2\\), then of course the posterior mean is better.\nComputation: Suppose we have the posterior \\(\\pi(\\theta \\mid x)\\).\n\nMAP: \\(\\hat{\\theta} = \\argmax_{\\theta} \\pi(\\theta \\mid x)\\)\nPosterior mean: \\(\\hat{\\theta} = \\int \\theta \\pi(\\theta \\mid x)d\\theta\\)\nWhich is computationally easier?"
  },
  {
    "objectID": "slides/04-estimation.html#capture-recapture-model",
    "href": "slides/04-estimation.html#capture-recapture-model",
    "title": "Lecture 04: Decision Theory and Bayesian Estimation",
    "section": "Capture-recapture model",
    "text": "Capture-recapture model\n\nIt’s a method commonly used in ecology to estimate an animal population’s size where it is impractical to count every individual.\nIt is based on taking at least two successive samples from the population of interest.\n\n\n\n\n\nSample 1/2\nCaptured\nMissed\n\n\nCaptured\n\\(n_{11}\\)\n\\(n_{12}\\)\n\n\nMissed\n\\(n_{21}\\)\n\\(n_{22}\\)\n\n\n\n\n\nAfter the two capture experiments, the population is divided as in the table above, with \\(n_{11} +n_{12} +n_{21} +n_{22} = N\\) (the fourth sample size \\(n_{22}\\) being unknown)."
  },
  {
    "objectID": "slides/04-estimation.html#uniform-model",
    "href": "slides/04-estimation.html#uniform-model",
    "title": "Lecture 04: Decision Theory and Bayesian Estimation",
    "section": "Uniform model",
    "text": "Uniform model\n\nFor the simplest model, called uniform, each individual has the same probability \\(p\\) of being captured in both experiments.\nTherefore, \\(p_{11}=p^2, p_{12}=p_{21}=p(1-p)\\), and \\(p_{22}=(1-p)^2\\).\nThe likelihood can be written \\[\nL\\left(N, p \\mid n_{11}, n_{12}, n_{21}\\right)=\\choose{N}{n_{11}\\;n_{12}\\;n_{21}} p^{\\tilde{n}}(1-p)^{2N-\\tilde{n}}\n\\] where \\(\\tilde{n}=2 n_{11}+n_{12}+n_{21}\\) is the total number of captured individuals.\nThere are two unknown parameters \\(N\\) and \\(p\\), which we assume a priori independent.\nThe prior is \\(\\pi(N, p) = \\pi(N)\\pi(p)\\) and we assume a Beta prior for \\(p\\).\nUnfortunately, the marginal posterior of \\(N\\) is complicated no matter we use \\(\\pi(N) = 1\\), \\(\\pi(N) = 1/N\\), or Poisson."
  },
  {
    "objectID": "slides/04-estimation.html#darroch-model",
    "href": "slides/04-estimation.html#darroch-model",
    "title": "Lecture 04: Decision Theory and Bayesian Estimation",
    "section": "Darroch model",
    "text": "Darroch model\n\nThe Darroch model1 is a hypergeometric model, in which the two sample sizes \\(n_1 = n_{11} + n_{12}\\) and \\(n_2 = n_{11} + n_{21}\\) are fixed.\nIn this case, the only remaining random variable is \\(n_{11}\\), with distribution \\(\\text{HyperGeo}(N, n_1, n_2)\\).\nRecall: The hypergeometric distribution \\(\\text{HyperGeo}(N, K, n)\\) has pmf \\[\n\\P(X = x) = \\frac{\\choose{K}{x}\\choose{N-K}{n-x}}{\\choose{N}{n}},\n\\] which models the number of successes in \\(n\\) draws (without replacement) from a population of size \\(N\\) with \\(K\\) successes.\n\nDarroch, J. N. (1958). The multiple-recapture census: I. Estimation of a closed population. Biometrika, 45(3/4), 343-359."
  },
  {
    "objectID": "slides/04-estimation.html#darroch-model-1",
    "href": "slides/04-estimation.html#darroch-model-1",
    "title": "Lecture 04: Decision Theory and Bayesian Estimation",
    "section": "Darroch model",
    "text": "Darroch model\n\nThe MLE of \\(N\\) is \\[\\begin{align*}\n\\hat{N}=\\left[\\frac{n_1}{\\left(n_{11} / n_2\\right)}\\right],\n\\end{align*}\\] where \\([a]\\) is the integer part of \\(a\\).\nIt identifies the proportion in the population \\(\\left(n_1 / N\\right)\\) and the proportion in the sample \\(\\left(n_{11} / n_2\\right)\\).\nA major drawback is that it cannot be used when \\(n_{11} = 0\\).\nA Bayesian analysis does not suffer from this defect because it reaches a conclusion even when \\(n_{11} = 0\\).\nGiven a prior distribution \\(\\pi\\) on \\(N\\), it is formally easy to derive the posterior \\(\\pi(N = n\\mid n_{11})\\) and draw an inference on \\(N\\)."
  },
  {
    "objectID": "slides/04-estimation.html#example-3",
    "href": "slides/04-estimation.html#example-3",
    "title": "Lecture 04: Decision Theory and Bayesian Estimation",
    "section": "Example",
    "text": "Example\n\nSuppose that a prior ecological study suggests that the population size varies between 36 and 50.\nHence here we use a uniform distribution on \\(\\{36,\\ldots,50\\}\\).\nConsidering \\(n_1 = n_2 = 5\\), we have \\[\n\\pi\\left(N=n \\mid n_{11}\\right)=\\frac{\\choose{n_1}{n_{11}}\\choose{n-n_1}{n_2-n_{11}} / \\choose{n}{n_2} \\pi(N=n)}{\\sum_{k=36}^{50}\\choose{n_1}{n_{11}}\\choose{k-n_1}{n_2-n_{11}} / \\choose{k}{n_2} \\pi(N=k)}.\n\\]\n\n\n\nlibrary(tidyverse)\nlibrary(kableExtra)\nN_vec &lt;- 36:50; n_1 &lt;- 5; n_2 &lt;- 5; n_11 &lt;- 0:5\n\nout &lt;-sapply(n_11, function(x) dhyper(x, n_1, N_vec-n_1, n_2)) %&gt;%\n    apply(., 2, function(x) x/sum(x)) %&gt;%\n    data.frame() \nout %&gt;% cbind(N = N_vec, .) %&gt;%\n    kbl(format=\"markdown\", digits = 4, col.names = c(\"N\", n_11))"
  },
  {
    "objectID": "slides/04-estimation.html#example-3-output",
    "href": "slides/04-estimation.html#example-3-output",
    "title": "Lecture 04: Decision Theory and Bayesian Estimation",
    "section": "Example",
    "text": "Example\n\n\n\n\nN\n0\n1\n2\n3\n4\n5\n\n\n\n\n36\n0.0580\n0.0725\n0.0886\n0.1061\n0.1246\n0.1438\n\n\n37\n0.0594\n0.0716\n0.0846\n0.0979\n0.1112\n0.1244\n\n\n38\n0.0608\n0.0708\n0.0808\n0.0905\n0.0996\n0.1080\n\n\n39\n0.0621\n0.0700\n0.0772\n0.0838\n0.0895\n0.0942\n\n\n40\n0.0634\n0.0691\n0.0739\n0.0778\n0.0806\n0.0824\n\n\n41\n0.0647\n0.0683\n0.0708\n0.0723\n0.0728\n0.0723\n\n\n42\n0.0659\n0.0674\n0.0679\n0.0673\n0.0659\n0.0637\n\n\n43\n0.0670\n0.0666\n0.0651\n0.0628\n0.0598\n0.0563\n\n\n44\n0.0682\n0.0658\n0.0625\n0.0587\n0.0544\n0.0499\n\n\n45\n0.0692\n0.0650\n0.0601\n0.0549\n0.0496\n0.0444\n\n\n46\n0.0703\n0.0642\n0.0578\n0.0515\n0.0453\n0.0396\n\n\n47\n0.0713\n0.0634\n0.0556\n0.0483\n0.0415\n0.0353\n\n\n48\n0.0723\n0.0626\n0.0536\n0.0454\n0.0380\n0.0317\n\n\n49\n0.0732\n0.0618\n0.0516\n0.0427\n0.0350\n0.0284\n\n\n50\n0.0741\n0.0611\n0.0498\n0.0402\n0.0322\n0.0256"
  },
  {
    "objectID": "slides/04-estimation.html#estimates-of-the-population-size",
    "href": "slides/04-estimation.html#estimates-of-the-population-size",
    "title": "Lecture 04: Decision Theory and Bayesian Estimation",
    "section": "Estimates of the population size",
    "text": "Estimates of the population size\n\n\nPosterior mean:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(n_{11}\\)\n0\n1\n2\n3\n4\n5\n\n\n\n\n\\(\\mathbb{E}(N\\mid n_{11})\\)\n43.32\n42.77\n42.23\n41.72\n41.23\n40.78\n\n\n\n\n\n\n\n\nMAP:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(n_{11}\\)\n0\n1\n2\n3\n4\n5\n\n\n\n\n\\(\\text{argmax }\\pi(N\\mid n_{11})\\)\n50\n36\n36\n36\n36\n36"
  },
  {
    "objectID": "slides/04-estimation.html#estimates-of-the-population-size-1",
    "href": "slides/04-estimation.html#estimates-of-the-population-size-1",
    "title": "Lecture 04: Decision Theory and Bayesian Estimation",
    "section": "Estimates of the population size",
    "text": "Estimates of the population size\nIf, instead of the squared error, we use the loss \\[\\begin{align*}\nL(N, \\delta)= \\begin{cases}10(\\delta-N) & \\text { if } \\delta&gt;N, \\\\ N-\\delta & \\text { otherwise }\\end{cases}\n\\end{align*}\\] in order to avoid an overestimation of the population size, the Bayes estimator is the \\((1 / 11)\\)-quantile of \\(\\pi\\left(N \\mid n_{11}\\right)\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(n_{11}\\)\n0\n1\n2\n3\n4\n5\n\n\n\n\n\\(\\delta(n_{11})\\)\n37\n37\n37\n36\n36\n36"
  },
  {
    "objectID": "slides/04-estimation.html#summary",
    "href": "slides/04-estimation.html#summary",
    "title": "Lecture 04: Decision Theory and Bayesian Estimation",
    "section": "Summary",
    "text": "Summary\n\nThe decision theory framework allows us to compare different estimators, under a given loss function.\nThe determination of loss is as difficult as the determination of prior.\nFrom a decision-theoretic point of view, Bayes estimators are always admissible, while MLE might not (e.g., Stein’s paradox).\nThe MAP estimator is essentially a penalized MLE. The penalization term is determined by the prior.\n\n\n\nHome"
  },
  {
    "objectID": "slides/06-testing.html#project-proposal",
    "href": "slides/06-testing.html#project-proposal",
    "title": "Lecture 06: Testing and Model Selection",
    "section": "Project Proposal",
    "text": "Project Proposal\n\n\\[\n\\newcommand{\\mc}[1]{\\mathcal{#1}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\E}{\\mathbb{E}}\n\\renewcommand{\\P}{\\mathbb{P}}\n\\newcommand{\\var}{{\\rm Var}} % Variance\n\\newcommand{\\mse}{{\\rm MSE}} % MSE\n\\newcommand{\\bias}{{\\rm Bias}} % MSE\n\\newcommand{\\cov}{{\\rm Cov}} % Covariance\n\\newcommand{\\iid}{\\stackrel{\\rm iid}{\\sim}}\n\\newcommand{\\ind}{\\stackrel{\\rm ind}{\\sim}}\n\\renewcommand{\\choose}[2]{\\binom{#1}{#2}}  % Choose\n\\newcommand{\\chooses}[2]{{}_{#1}C_{#2}}  % Small choose\n\\newcommand{\\cd}{\\stackrel{d}{\\rightarrow}}\n\\newcommand{\\cas}{\\stackrel{a.s.}{\\rightarrow}}\n\\newcommand{\\cp}{\\stackrel{p}{\\rightarrow}}\n\\newcommand{\\bin}{{\\rm Bin}}\n\\newcommand{\\ber}{{\\rm Ber}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\]\n\n\n\nA 10-15 min presentation describing your final project, including\n\nthe dataset\nthe scientific questions\nwhat statistical methods do you think can help you answer the questions\n\nUpload your slides to NTU cool by 10/23.\nThis presentation weights 20% of your final grade.\nYou can work in groups of 2-3 people.\nYou can choose a topic related to your own research."
  },
  {
    "objectID": "slides/06-testing.html#introduction",
    "href": "slides/06-testing.html#introduction",
    "title": "Lecture 06: Testing and Model Selection",
    "section": "Introduction",
    "text": "Introduction\n\nIn the last lecture, we saw that hierarchical structures allow us to construct models with high flexibility.\nIn practice, there are more than one way to build a hierarchical models, especially when you use latent variables.\nIs a model with more hierarchy always “better” than one with less hierarchy?\nWhat does it mean by saying “a model is better than the other?”\nYou can evaluate models by\n\nhow well it fits the data,\nhow good it is at prediction,\nhow we can interpret this model,\nhow robust it is to the presence of outliers, etc."
  },
  {
    "objectID": "slides/06-testing.html#hypothesis-testing-1",
    "href": "slides/06-testing.html#hypothesis-testing-1",
    "title": "Lecture 06: Testing and Model Selection",
    "section": "Hypothesis testing",
    "text": "Hypothesis testing\n\nThe goal of a hypothesis test is to decide whether you can reject the null hypothesis \\(H_0\\) and accept the alternative hypothesis \\(H_1\\) or not.\nUsually, the hypotheses are of the form \\[\nH_0: \\theta \\in \\Theta_0\\quad \\text{vs.} \\quad H_1: \\theta \\in \\Theta_1 = \\Theta_0^c.\n\\]\nThe philosophy is that we first assume \\(H_0\\) to be true, and see how strong the data is against \\(H_0\\). If it is strong enough, we reject \\(H_0\\) and accept \\(H_1\\)."
  },
  {
    "objectID": "slides/06-testing.html#likelihood-ratio-test",
    "href": "slides/06-testing.html#likelihood-ratio-test",
    "title": "Lecture 06: Testing and Model Selection",
    "section": "Likelihood Ratio Test",
    "text": "Likelihood Ratio Test\n\nOne of the frequentist approaches to hypothesis testing is the likelihood ratio test (LRT).\nSuppose \\(X_1, \\ldots, X_n \\iid f(x\\mid \\theta)\\) and \\(L(\\theta \\mid x) = \\prod_{i=1}^n f(x_i \\mid \\theta)\\) is the likelihood function.\nThe LRT statistic is \\[\n    \\lambda = \\frac{\\max_{\\theta \\in \\Theta_0} L(\\theta \\mid x)}{\\max_{\\theta \\in \\Theta_0 \\cup \\Theta_1} L(\\theta \\mid x)}.\n\\]\nApparently, \\(0 \\leq \\lambda \\leq 1\\) and small values of \\(\\lambda\\) provide evidence against \\(H_0\\).\nWhen should we reject \\(H_0\\)?"
  },
  {
    "objectID": "slides/06-testing.html#two-types-of-error",
    "href": "slides/06-testing.html#two-types-of-error",
    "title": "Lecture 06: Testing and Model Selection",
    "section": "Two types of error",
    "text": "Two types of error\n\nFalse rejection (Type I error) and False acceptance (Type II error)\nThe decision space is \\(\\mc{D} = \\{\\text{accept}, \\text{reject}\\} = \\{0, 1\\}\\).\nA typical loss is the 0-1 loss \\[\nL(\\theta, d)= \\begin{cases}1 & \\text { if } d = \\mathbb{I}_{\\Theta_0}(\\theta) \\\\ 0 & \\text { otherwise }\\end{cases}.\n\\]\nThe frequentist risk is \\[\\begin{align*}\nR(\\theta, \\delta) & = \\E L(\\theta, \\delta(X)) = \\P(\\delta(X) = 1 \\mid \\theta \\in \\Theta_0) + \\P(\\delta(X) = 0 \\mid \\theta \\notin \\Theta_0)\\\\\n& = \\text{Type I error} + \\text{Type II error}\n\\end{align*}\\]\nUnder this loss, the Bayesian solution \\[\n\\varphi^\\pi(x)= \\begin{cases}1 & \\text { if } \\P^\\pi\\left(\\theta \\in \\Theta_1 \\mid x\\right) &gt; \\P^\\pi\\left(\\theta \\in \\Theta_0 \\mid x\\right) \\\\ 0 & \\text { otherwise }\\end{cases}.\n\\]"
  },
  {
    "objectID": "slides/06-testing.html#bayesian-test",
    "href": "slides/06-testing.html#bayesian-test",
    "title": "Lecture 06: Testing and Model Selection",
    "section": "Bayesian test",
    "text": "Bayesian test\n\nA Bayesian test with prior \\(\\pi\\) rejects \\(H_0: \\theta \\in \\Theta_0\\) if \\[\n\\P^\\pi\\left(\\theta \\in \\Theta_1 \\mid x\\right) &gt; \\P^\\pi\\left(\\theta \\in \\Theta_0 \\mid x\\right).\n\\]\nWhat are the Type I and Type II errors?\nWhat is the power of the test?\nWhat if we have a point null hypothesis \\(H_0: \\theta = \\theta_0\\)? For continuous parameter, a Bayesian test will always reject \\(H_0\\).\nIs this rejection due to the prior or the observation?"
  },
  {
    "objectID": "slides/06-testing.html#bayes-factor",
    "href": "slides/06-testing.html#bayes-factor",
    "title": "Lecture 06: Testing and Model Selection",
    "section": "Bayes Factor",
    "text": "Bayes Factor\n\nThe Bayes factor is the ratio of posterior odds to the prior odds \\[\nB_{01}^\\pi(x)=\\left.\\frac{P\\left(\\theta \\in \\Theta_0 \\mid x\\right)}{P\\left(\\theta \\in \\Theta_1 \\mid x\\right)} \\middle/ \\frac{\\pi\\left(\\theta \\in \\Theta_0\\right)}{\\pi\\left(\\theta \\in \\Theta_1\\right)}\\right..\n\\]\nIt is an “objective” answer, since it partly eliminates the influence of the prior modeling and emphasizes the role of the observations.\nIf the Bayes factor is 1, the observation does not provide information in favor of either hypothesis.\nIf the Bayes factor is greater than 1, the observation favors \\(H_0\\) and hence we accept \\(H_0\\) when the Bayes factor is large.\nThe Bayes factor offers a continuum of evidence, rather than a dichotomous decision."
  },
  {
    "objectID": "slides/06-testing.html#differences-between-bayes-factor-and-p-value",
    "href": "slides/06-testing.html#differences-between-bayes-factor-and-p-value",
    "title": "Lecture 06: Testing and Model Selection",
    "section": "Differences between Bayes factor and \\(p\\)-value",
    "text": "Differences between Bayes factor and \\(p\\)-value\n\n\\(p\\)-value is the probability of observing a test statistic at least as extreme as the one observed, given that \\(H_0\\) is true.\nSo small \\(p\\)-values provide evidence against \\(H_0\\), but large \\(p\\)-values do not provide evidence in favor of \\(H_0\\) (or against \\(H_1\\)).\nIn contrast, large Bayes factors provide evidence in favor of \\(H_0\\), and small Bayes factors provide evidence in favor of \\(H_1\\) (or against \\(H_0\\)).\nThe \\(p\\)-value bases rejection of \\(H_0\\) on unlikely events that did not occur."
  },
  {
    "objectID": "slides/06-testing.html#type-i-and-type-ii-errors-for-bayesian-test",
    "href": "slides/06-testing.html#type-i-and-type-ii-errors-for-bayesian-test",
    "title": "Lecture 06: Testing and Model Selection",
    "section": "Type I and Type II errors for Bayesian test",
    "text": "Type I and Type II errors for Bayesian test\n\nAlthough it is possible to define Type I and Type II errors for Bayesian tests, the concept is not compatible with Bayesian paradigm.\nIn Bayesian inference, we tend not to give a simple, dichotomous answer to the question of whether a hypothesis is true or false.\nWe compute the posterior probability of the two hypotheses and choose the hypothesis when you think the evidence (e.g., Bayes factor) is strong enough."
  },
  {
    "objectID": "slides/06-testing.html#improper-prior-for-testing",
    "href": "slides/06-testing.html#improper-prior-for-testing",
    "title": "Lecture 06: Testing and Model Selection",
    "section": "Improper prior for testing",
    "text": "Improper prior for testing\n\nFor Bayesian estimation, improper priors (e.g., noninformative priors) sometimes give us good estimates, although we are required to check the propriety of the posterior.\nFor testing, it is not recommended to use improper prior for a couple of reasons.\n\nImproper priors make the Bayes factor undefined.\nThe testing setting is not coherent with an absolute lack of information since we need to decide our hypotheses.\n\nThere are still some solutions proposed to overcome the difficulties related to the use of improper priors.\nMost of them are based on the idea to use part of the data to transform the priors into proper distributions.\nFor example, partial Bayes factor uses a subset of samples (training sample) to construct a proper prior, and use the rest to compute the Bayes factor."
  },
  {
    "objectID": "slides/06-testing.html#bayesian-one-sample-t-test",
    "href": "slides/06-testing.html#bayesian-one-sample-t-test",
    "title": "Lecture 06: Testing and Model Selection",
    "section": "Bayesian one sample t-test",
    "text": "Bayesian one sample t-test\n\nSuppose \\(X_1, \\ldots, X_n \\iid N(\\mu, \\sigma^2)\\) and \\(\\sigma^2\\) is known.\nThe null hypothesis is \\(H_0: \\mu = \\mu_0\\) and the alternative hypothesis is \\(H_1: \\mu \\neq \\mu_0\\).\nWe can use the normal prior or Cauchy piror centered at \\(\\mu_0\\) (since it has heavy tails) for \\(\\mu\\).\nFor the variance \\(\\sigma^2\\), it is okay to use the Jeffreys prior.\nThen we can use sampling techniques (e.g., MCMC) to sample from the posterior and compute the Bayes factor."
  },
  {
    "objectID": "slides/06-testing.html#example",
    "href": "slides/06-testing.html#example",
    "title": "Lecture 06: Testing and Model Selection",
    "section": "Example",
    "text": "Example\nThis is a dataset which shows the effect of two soporific drugs (increase in hours of sleep compared to control) on 10 patients.\n\nlibrary(BayesFactor)\nlibrary(ggplot2)\nlibrary(tidyverse)\n\ndata(sleep)\nggplot(sleep, aes(x = group, y = extra)) + geom_boxplot()"
  },
  {
    "objectID": "slides/06-testing.html#example-1",
    "href": "slides/06-testing.html#example-1",
    "title": "Lecture 06: Testing and Model Selection",
    "section": "Example",
    "text": "Example\nWe want to see the difference in efficacy of the drugs.\n\nttestBF(x = sleep$extra[sleep$group==1], \n        y = sleep$extra[sleep$group==2], \n        paired=TRUE)\n\nBayes factor analysis\n--------------\n[1] Alt., r=0.707 : 17.25888 ±0%\n\nAgainst denominator:\n  Null, mu = 0 \n---\nBayes factor type: BFoneSample, JZS\n\n\nPriors:\n\n\nJeffreys prior for the variance\nCauchy prior for the mean (effect size)\n\n\nYou can change the priors and see how the results change. This might require you to implement the sampling process yourself."
  },
  {
    "objectID": "slides/06-testing.html#example-2",
    "href": "slides/06-testing.html#example-2",
    "title": "Lecture 06: Testing and Model Selection",
    "section": "Example",
    "text": "Example\nTo check the quality of the posterior samples, you can ask the function to return posterior rather than the Bayes factor.\n\nsamples &lt;- ttestBF(x = sleep$extra[sleep$group==1],\n                   y = sleep$extra[sleep$group==2], \n                   paired=TRUE, \n                   posterior = TRUE, iterations = 1000)\nplot(samples[, \"mu\"])"
  },
  {
    "objectID": "slides/06-testing.html#model-selection-1",
    "href": "slides/06-testing.html#model-selection-1",
    "title": "Lecture 06: Testing and Model Selection",
    "section": "Model selection",
    "text": "Model selection\n\nHypothesis testing is a special case of model selection.\nA general model selection problem is to choose between a collection of models \\(M_1, \\ldots, M_k\\).\nFor example, we can choose whether we want to use a linear regression model or a quadratic regression model.\nThe choice can be based on the \\(R^2\\) or the adjusted \\(R^2\\), i.e., choose the one with highest adjusted \\(R^2\\).\nFor general models (e.g., non-linear regression), we can use\n\ninformation criterion (e.g., AIC, BIC)\nBayes factor"
  },
  {
    "objectID": "slides/06-testing.html#information-criterion",
    "href": "slides/06-testing.html#information-criterion",
    "title": "Lecture 06: Testing and Model Selection",
    "section": "Information criterion",
    "text": "Information criterion\n\nThe general form of an information criterion is \\[\n-\\text{Model fit} + \\text{Penalty}.\n\\]\nWe choose the model with the smallest value of the information criterion.\nThe ‘Model fit’ is typically chosen as the log-likelihood of the model.\nThe ‘Penalty’ is typically chosen as a function of the number of parameters in the model.\nFor Bayesian information criterion (BIC), \\(\\text{Penalty} = p\\log n\\).\nFor Akaike information criterion (AIC), \\(\\text{Penalty} = 2p\\).\nHowever, a small value of the information criterion only indicates a good model fit of the dataset."
  },
  {
    "objectID": "slides/06-testing.html#cross-validation",
    "href": "slides/06-testing.html#cross-validation",
    "title": "Lecture 06: Testing and Model Selection",
    "section": "Cross-validation",
    "text": "Cross-validation\n\nCross-validation is a way to evaluate the predictive performance of a model.\nThe idea is to split the data into two parts: a training set and a validation set.\nThe model is fit to the training set, and then the predictive performance is evaluated on the validation set.\nCommonly used approaches are \\(k\\)-fold cross-validation and leave-one-out cross-validation."
  },
  {
    "objectID": "slides/06-testing.html#bayes-factor-1",
    "href": "slides/06-testing.html#bayes-factor-1",
    "title": "Lecture 06: Testing and Model Selection",
    "section": "Bayes Factor",
    "text": "Bayes Factor\n\nAlthough BIC has “Bayesian” in its name, it is not technically a Bayesian method. It just has a Bayesian interpretation.\nThe Bayes factor can be defined in the general model selection setting: \\[\n\\text{BF}_{ij} = \\left.\\frac{\\P(M_i \\mid x)}{\\P(M_j \\mid x)}\\middle/\\frac{\\P(M_i)}{\\P(M_j)}\\right.\n\\] where \\(\\P(M_i)\\) is the prior belief of the model \\(M_i\\).\nIf \\(\\text{BF}_{ij}\\) is large enough, than we prefer model \\(M_i\\) over model \\(M_j\\).\nIf we don’t have any preference between models, \\(\\P(M_i) = 1/k\\) and comparison of Bayes factor is equivalent to comparison of posterior probability of models.\nIf you prefer simpler models, you can assign higher prior preference to simpler models and see if the posterior outweighs your preference."
  },
  {
    "objectID": "slides/06-testing.html#bayes-factor-2",
    "href": "slides/06-testing.html#bayes-factor-2",
    "title": "Lecture 06: Testing and Model Selection",
    "section": "Bayes Factor",
    "text": "Bayes Factor\n\nUsually not only do we have priors for difference models, we also have priors for the parameters in each model.\nFor example, if model \\(M_i\\) is indexed by the parameter \\(\\theta_i\\) and \\(\\theta_i\\) has prior \\(\\pi_i(\\theta_i)\\), then \\[\n\\P(M_i \\mid x) = \\frac{\\P(M_i) \\P(x \\mid M_i)}{\\sum_{j=1}^k \\P(M_j) \\P(x \\mid M_j)}\n\\] where \\[\n\\P(x \\mid M_i) = \\int \\P(x \\mid \\theta_i, M_i) \\pi_i(\\theta_i) \\, d\\theta_i.\n\\]\nIf we assume \\(\\P(M_i) = 1/k\\), then the Bayes factor is \\[\n\\text{BF}_{ij} = \\frac{\\P(x \\mid M_i)}{\\P(x \\mid M_j)} = \\frac{\\int \\P(x \\mid \\theta_i, M_i) \\pi_i(\\theta_i) \\, d\\theta_i}{\\int \\P(x \\mid \\theta_j, M_j) \\pi_j(\\theta_j) \\, d\\theta_j}.\n\\]"
  },
  {
    "objectID": "slides/06-testing.html#model-averaging",
    "href": "slides/06-testing.html#model-averaging",
    "title": "Lecture 06: Testing and Model Selection",
    "section": "Model averaging",
    "text": "Model averaging\n\nModel averaging is a way to combine the results from multiple models.\nThe idea is to average the posterior distributions of the parameters over all models.\nFor prediction, we can use the posterior predictive distribution \\[\n\\P(x_{\\text{new}} \\mid x) = \\sum_{i=1}^k \\P(x_{\\text{new}} \\mid x, M_i) \\P(M_i \\mid x).\n\\]\nA commonly used trick in machine learning, called ensemble, is to use a weighted average of the predictions from different models: \\(x_{\\text{new}} = \\sum_{i=1}^k w_i x_{\\text{new},i}\\) where \\(x_{\\text{new},i}\\) is the prediction from model \\(M_i\\) and \\(w_i\\) is the weight for model \\(M_i\\).\nUsually the weights are chosen to be \\(1/k\\) or through cross-validation."
  },
  {
    "objectID": "slides/06-testing.html#model-diagnostics",
    "href": "slides/06-testing.html#model-diagnostics",
    "title": "Lecture 06: Testing and Model Selection",
    "section": "Model diagnostics",
    "text": "Model diagnostics\n\nModel diagnostics is a way to check whether the model is appropriate for the data.\nIf we have a collection of candidate models, we can choose the best model based on the techniques we discussed earlier.\nHowever, the selected model may not be a good model.\nWe have to check whether the model is good not just by comparing with other models."
  },
  {
    "objectID": "slides/06-testing.html#what-do-we-need-to-check",
    "href": "slides/06-testing.html#what-do-we-need-to-check",
    "title": "Lecture 06: Testing and Model Selection",
    "section": "What do we need to check?",
    "text": "What do we need to check?\n\nA ‘model’ encompasses the sampling distribution, the prior distribution, any hierarchical structure, and issues such as which explanatory variables have been included in a regression.\nSensitivity analysis: how much do posterior inferences change when other reasonable probability models are used in place of the present model?\nExternal validation: using the model to make predictions about future data, and then collecting those data and comparing to their predictions.\nRobustness: how do the model performs when every assumption we made is wrong?\nThere is no general recipe for model checking. It depends on the model and the purpose of the model.\nHowever, we can do simulation, which uses the worst data you can possibly imagine, to diagnose the model.\n\n\n\nHome"
  },
  {
    "objectID": "slides/08-hmc_vb.html#motivation",
    "href": "slides/08-hmc_vb.html#motivation",
    "title": "Lecture 08: Hamiltonian Monte Carlo and Variational Inference",
    "section": "Motivation",
    "text": "Motivation\n\n\\[\n\\newcommand{\\mc}[1]{\\mathcal{#1}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\E}{\\mathbb{E}}\n\\renewcommand{\\P}{\\mathbb{P}}\n\\newcommand{\\var}{{\\rm Var}} % Variance\n\\newcommand{\\mse}{{\\rm MSE}} % MSE\n\\newcommand{\\bias}{{\\rm Bias}} % MSE\n\\newcommand{\\cov}{{\\rm Cov}} % Covariance\n\\newcommand{\\iid}{\\stackrel{\\rm iid}{\\sim}}\n\\newcommand{\\ind}{\\stackrel{\\rm ind}{\\sim}}\n\\renewcommand{\\choose}[2]{\\binom{#1}{#2}}  % Choose\n\\newcommand{\\chooses}[2]{{}_{#1}C_{#2}}  % Small choose\n\\newcommand{\\cd}{\\stackrel{d}{\\rightarrow}}\n\\newcommand{\\cas}{\\stackrel{a.s.}{\\rightarrow}}\n\\newcommand{\\cp}{\\stackrel{p}{\\rightarrow}}\n\\newcommand{\\bin}{{\\rm Bin}}\n\\newcommand{\\ber}{{\\rm Ber}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\]\n\n\nIn theory, the Metropolis algorithm (with an arbitrary proposal) works for any distribution in any dimension.\nIn practice, the Metropolis algorithm is inefficient for high-dimensional distributions.\nCurrent research in MCMC focuses on finding good proposals to speed-up the algorithm, together with theoretical guarantees, e.g., making sure that the chain converges to the target distribution.\nIn this lecture, we will introduce two popular MCMC algorithms: Hamiltonian Monte Carlo (HMC) and No-U-Turn Sampler (NUTS).\nThese two algorithms are the ones used in the Stan software."
  },
  {
    "objectID": "slides/08-hmc_vb.html#a-few-words-about-acceptance-rate",
    "href": "slides/08-hmc_vb.html#a-few-words-about-acceptance-rate",
    "title": "Lecture 08: Hamiltonian Monte Carlo and Variational Inference",
    "section": "A few words about acceptance rate",
    "text": "A few words about acceptance rate\nIdeally, we want the acceptance rate to be far from 0 and 1.\n\n\nIf the acceptance rate is too low, the Markov chain is not moving.\nIf the acceptance rate is too high, the Markov chain is only exploring the high-density region."
  },
  {
    "objectID": "slides/08-hmc_vb.html#adaptive-mcmc",
    "href": "slides/08-hmc_vb.html#adaptive-mcmc",
    "title": "Lecture 08: Hamiltonian Monte Carlo and Variational Inference",
    "section": "Adaptive MCMC",
    "text": "Adaptive MCMC\n\nIn the original Metropolis algorithm, the proposal distribution is fixed.\nThis can be inefficient, since after some exploration, we gain some knowledge about the target distribution, and we can use this knowledge to improve the proposal distribution.\nThis can be easily implemented, e.g., reducing the variance of Gaussian proposal every 100 iterations.\nHowever, the theoretical analysis is complicated since the Markov chain is no longer time-homogeneous.\nCan we assure that the adaptive Markov chain still converges to the target distribution?\n\n\n\nThe material is adapted from http://probability.ca/jeff/ftpdir/Cambridge2010.pdf"
  },
  {
    "objectID": "slides/08-hmc_vb.html#optimal-gaussian-proposal",
    "href": "slides/08-hmc_vb.html#optimal-gaussian-proposal",
    "title": "Lecture 08: Hamiltonian Monte Carlo and Variational Inference",
    "section": "Optimal Gaussian proposal",
    "text": "Optimal Gaussian proposal\n\nLet \\(\\pi\\) be a distribution supported on \\(\\Theta \\subseteq \\mathbb{R}^d\\).\nWe want to sample from \\(\\pi\\) using Metropolis algorithm with a Gaussian proposal \\(N_d(\\theta^{(t-1)}, \\Xi)\\).\nWhat is the optimal choice of \\(\\Xi\\)?\nWhat do we mean by optimal in MCMC? We want the fastest convergence of the Markov Chain.\nRoberts & Rosenthal (2001)1 showed that, under some conditions on \\(\\pi\\), the optimal choice is \\[\n\\Xi = \\frac{2.38^2}{d} \\Sigma_{\\pi},\n\\] where \\(\\Sigma_{\\pi}\\) is the covariance matrix of \\(\\pi\\). But we don’t know \\(\\Sigma_{\\pi}\\)!\n\nRoberts, G. O., & Rosenthal, J. S. (2001). Optimal Scaling for Various Metropolis-Hastings Algorithms. Statistical Science, 16(4), 351–367."
  },
  {
    "objectID": "slides/08-hmc_vb.html#adaptive-random-walk-metropolis",
    "href": "slides/08-hmc_vb.html#adaptive-random-walk-metropolis",
    "title": "Lecture 08: Hamiltonian Monte Carlo and Variational Inference",
    "section": "Adaptive Random Walk Metropolis",
    "text": "Adaptive Random Walk Metropolis\n\nAlgorithm:\n\nAt \\(t\\)th iteration, compute \\(\\hat{\\Sigma}_n\\) using \\(\\theta^{(1)}, \\ldots, \\theta^{(t-1)}\\).\nSample \\(\\theta^*\\) from \\(N_d\\left(\\theta^{(t-1)}, \\frac{(2.38)^2}{d}\\hat{\\Sigma}_n\\right)\\).\nCompute the acceptance ratio \\(\\rho(\\theta^{(t-1)}, \\theta^{*})=\\min\\left\\{\\frac{\\pi(\\theta^* \\mid x)}{\\pi(\\theta^{(t-1)} \\mid x)}, 1\\right\\}\\).\nSet \\[\n\\theta^{(t)}= \\begin{cases}\\theta^* & \\text { with prob. } \\rho(\\theta^{(t-1)}, \\theta^{*}) \\\\ \\theta^{(t-1)} & \\text { with prob. } 1-\\rho(\\theta^{(t-1)}, \\theta^{*}).\\end{cases}\n\\]\n\nThe simplest choice of \\(\\hat{\\Sigma}_n\\) is the sample covariance matrix.\nHowever, it is not a good choice in high dimensions.\nGoogle: ``High-dimensional covariance estimation’’!"
  },
  {
    "objectID": "slides/08-hmc_vb.html#does-it-work",
    "href": "slides/08-hmc_vb.html#does-it-work",
    "title": "Lecture 08: Hamiltonian Monte Carlo and Variational Inference",
    "section": "Does it work?",
    "text": "Does it work?\n\nThe adaptive RWM works well in practice, but the theoretical analysis is difficult.\nAdaptation: the change of proposal distribution over time.\nFor an adaptive MCMC algorithm to work, the adaptation must satisfy two requirements:\n\nDiminishing adaptation: the adaptation must diminish over time.\nContainment: the time to stationary from \\(X_t\\) is bounded, if we fix the adaptation at \\(t\\), i.e., stop changing proposal distribution after time \\(t\\)."
  },
  {
    "objectID": "slides/08-hmc_vb.html#metropolis-adjusted-langevin-algorithm-mala",
    "href": "slides/08-hmc_vb.html#metropolis-adjusted-langevin-algorithm-mala",
    "title": "Lecture 08: Hamiltonian Monte Carlo and Variational Inference",
    "section": "Metropolis-adjusted Langevin algorithm (MALA)",
    "text": "Metropolis-adjusted Langevin algorithm (MALA)\n\nProposal: \\(\\theta^{(t)} \\sim N_d\\left(\\theta^{(t-1)} + \\frac{1}{2}\\Sigma\\nabla\\log\\pi(\\theta^{(t-1)}), \\Sigma\\right)\\)\nThis proposal is inspired by Langevin diffusion, and is non-symmatric.\nYou can also make MALA adaptive by replacing \\(\\Sigma\\) with \\(\\frac{(2.38)^2}{d}\\hat{\\Sigma}_n\\)."
  },
  {
    "objectID": "slides/08-hmc_vb.html#hamiltonian-monte-carlo-hmchmc",
    "href": "slides/08-hmc_vb.html#hamiltonian-monte-carlo-hmchmc",
    "title": "Lecture 08: Hamiltonian Monte Carlo and Variational Inference",
    "section": "Hamiltonian Monte Carlo (HMC)1",
    "text": "Hamiltonian Monte Carlo (HMC)1\n\nThe main reason that random walk Metropolis is inefficient is that it explores the parameter space by diffusing around.\nHamiltonian dynamics can be used to produce distant proposals for the Metropolis algorithm.\n\n\n\n\nThe idea behind HMC:\n\n\n\n\n\n\n\n\nNeal, R. M. (2011). MCMC using Hamiltonian dynamics. Handbook of markov chain monte carlo, 2(11), 2."
  },
  {
    "objectID": "slides/08-hmc_vb.html#hamiltonian-dynamics",
    "href": "slides/08-hmc_vb.html#hamiltonian-dynamics",
    "title": "Lecture 08: Hamiltonian Monte Carlo and Variational Inference",
    "section": "Hamiltonian dynamics",
    "text": "Hamiltonian dynamics\n\nLet \\(p\\) be a particle with position \\(x\\) and velocity \\(v\\).\nThe Hamiltonian of \\(p\\) is the sum of its potential energy and kinetic energy: \\[\nH(x, v) = U(x) + K(v).\n\\]\nThe potential energy \\(U(x)\\) is proportional to the height at \\(x\\).\nThe kinetic energy is \\(K(v) = \\frac{1}{2}mv^Tv\\), where \\(m\\) is the mass of the particle \\(p\\).\nThe momentum of \\(p\\) is defined as \\(y = mv\\) and hence \\(K(y) = \\frac{y^Ty}{2m}\\).\nThe total energy is constant:\n\nwhen \\(p\\) moves upward, potential energy increases and kinetic energy decreases;\nwhen \\(p\\) moves downward, potential energy decreases and kinetic energy increases."
  },
  {
    "objectID": "slides/08-hmc_vb.html#hamiltonian-dynamics-1",
    "href": "slides/08-hmc_vb.html#hamiltonian-dynamics-1",
    "title": "Lecture 08: Hamiltonian Monte Carlo and Variational Inference",
    "section": "Hamiltonian dynamics",
    "text": "Hamiltonian dynamics\n\nLet \\(x(t)\\) be the position of \\(p\\) at time \\(t\\) and \\(y(t)\\) be the momentum of \\(p\\) at time \\(t\\).\nDefine the potential energy to be \\(U(x) = -\\log \\pi(x)\\), where \\(\\pi(x)\\) is our target distribution.\nHence the Hamiltonian is \\[\nH(x, y) = -\\log \\pi(x) + \\frac{y^Ty}{2m}.\n\\]\nIn Hamiltonian dynamics, the pair \\((x(t), y(t))\\) satisfies a set of differential equations, given by \\[\n\\begin{aligned}\n\\frac{dx_i}{dt} &= \\frac{\\partial H}{\\partial y_i} = \\frac{\\partial K}{\\partial y_i} = \\frac{y_i}{m},\\\\\n\\frac{dy_i}{dt} &= -\\frac{\\partial H}{\\partial x_i} = -\\frac{\\partial U}{\\partial x_i} = \\frac{\\partial \\log \\pi(x)}{\\partial x_i}, \\quad i = 1,\\ldots,d.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/08-hmc_vb.html#hmc",
    "href": "slides/08-hmc_vb.html#hmc",
    "title": "Lecture 08: Hamiltonian Monte Carlo and Variational Inference",
    "section": "HMC",
    "text": "HMC\n\nThe HMC is actually sampling from \\[\np(x, y) \\propto \\exp(-H(x, y)) = \\pi(x)\\exp\\left(-\\frac{y^Ty}{2m}\\right).\n\\]\nThat is, \\(X \\sim \\pi\\) and \\(Y \\sim N(0, m^{-1}I)\\) are independent.\nAt \\(n\\)th iteration, we randomly assign a momentum \\(y \\sim N(0, m^{-1}I)\\) to \\(x^{(n-1)}\\).\nLet \\((x(0), y(0)) = (x^{(n-1)}, y)\\).\nEvolve \\((x(0, y(0)))\\) for time \\(t\\) according to Hamiltonian dynamics and get \\((x(t), y(t))\\).\nAccept \\((x(t), y(t))\\) with probability \\[\n\\rho = \\frac{\\exp(-H(x(t), y(t)))}{\\exp(-H(x(0), y(0)))} = \\exp(-H(x(t), y(t)) + H(x^{(n-1)}, y)).\n\\]\nIf accepted, set \\(x^{(n)} = x(t)\\); otherwise, set \\(x^{(n)} = x^{(n-1)}\\)."
  },
  {
    "objectID": "slides/08-hmc_vb.html#evolution-of-hamiltonian-dynamics",
    "href": "slides/08-hmc_vb.html#evolution-of-hamiltonian-dynamics",
    "title": "Lecture 08: Hamiltonian Monte Carlo and Variational Inference",
    "section": "Evolution of Hamiltonian dynamics",
    "text": "Evolution of Hamiltonian dynamics\n\nGiven \\((x(0), y(0))\\), we can evolve \\((x(t), y(t))\\) for time \\(t\\) by solving the Hamiltonian equations.\nThe most natural way is Euler’s method: \\[\n\\begin{aligned}\nx_i(t + \\epsilon) &= x_i(t) + \\epsilon\\frac{dx_i}{dt}(t) = x_i(t) + \\epsilon \\frac{y_i(t)}{m},\\\\\ny_i(t + \\epsilon) &= y_i(t) + \\epsilon\\frac{dy_i}{dt}(t) = y_i(t) + \\epsilon \\frac{\\partial}{\\partial x_i}\\log \\pi(x(t)).\n\\end{aligned}\n\\]\nIt turns out that the Euler method is inaccurate, and the most widely used method is the leapfrog method."
  },
  {
    "objectID": "slides/08-hmc_vb.html#leapfrog-method",
    "href": "slides/08-hmc_vb.html#leapfrog-method",
    "title": "Lecture 08: Hamiltonian Monte Carlo and Variational Inference",
    "section": "Leapfrog method",
    "text": "Leapfrog method\n\nThe leapfrog method maps \\((x(t), y(t))\\) to \\((x(t+\\epsilon), y(t+\\epsilon))\\) through the following steps:\n\n\\(y(t + \\epsilon/2) = y(t) + \\frac{\\epsilon}{2}\\nabla_x \\log \\pi(x(t))\\);\n\\(x(t + \\epsilon) = x(t) + \\epsilon\\frac{y(t + \\epsilon/2)}{m}\\);\n\\(y(t + \\epsilon) = y(t + \\epsilon/2) + \\frac{\\epsilon}{2}\\nabla_x\\log \\pi(x(t + \\epsilon))\\).\n\nIn HMC, the leapfrog method evolves \\((x(0), y(0))\\) for \\(L\\) iterations."
  },
  {
    "objectID": "slides/08-hmc_vb.html#general-hmc-algorithm",
    "href": "slides/08-hmc_vb.html#general-hmc-algorithm",
    "title": "Lecture 08: Hamiltonian Monte Carlo and Variational Inference",
    "section": "General HMC algorithm",
    "text": "General HMC algorithm\n\n\nInput: un-normalized density \\(\\pi(x)\\).\nTuning parameters: step size \\(\\epsilon\\), number of steps \\(L\\), mass matrix \\(M\\).\nInitialize \\(x^{(0)}\\).\nfor \\(i = 1, \\ldots, n\\):\n\ngenerate \\(y^{(0)} \\sim N(0, M)\\);\nset \\(\\tilde{x} \\leftarrow x^{(i-1)}\\), \\(\\tilde{y} \\leftarrow y^{(0)}\\);\nfor \\(j = 1, \\ldots, L\\):\n\n\\(\\tilde{y} \\leftarrow \\tilde{y} + \\frac{\\epsilon}{2}\\nabla_x \\log \\pi(\\tilde{x})\\);\n\\(\\tilde{x} \\leftarrow \\tilde{x} + \\epsilon M^{-1}\\tilde{y}\\);\n\\(\\tilde{y} \\leftarrow \\tilde{y} + \\frac{\\epsilon}{2}\\nabla_x \\log \\pi(\\tilde{x})\\);\n\n\\(u \\sim \\text{Unif}(0, 1)\\);\nif \\(u &lt; \\exp(-H(\\tilde{x}, \\tilde{y}) + H(x^{(i-1)}, y^{(0)}))\\):\n\n\\(x^{(i)} \\leftarrow \\tilde{x}\\);\n\nelse:\n\n\\(x^{(i)} \\leftarrow x^{(i-1)}\\);"
  },
  {
    "objectID": "slides/08-hmc_vb.html#remarks",
    "href": "slides/08-hmc_vb.html#remarks",
    "title": "Lecture 08: Hamiltonian Monte Carlo and Variational Inference",
    "section": "Remarks",
    "text": "Remarks\n\nRandom walk Metropolis can fail badly when the target distribution is multi-modal because it uses proposals that are unrelated to the target distribution.\nHMC makes better use of what is known about the target distribution by using the gradient of the log density.\nHMC can only be used to sample from continuous distributions whose density can be evaluated (up to a normalizing constant) and whose gradient can be computed.\nIn particular, HMC cannot be used to sample from discrete distributions.\nAlthough we did not show in the class, the HMC indeed has a stationary distribution \\(\\pi\\).\nSee Neal (2011) for more details."
  },
  {
    "objectID": "slides/08-hmc_vb.html#tuning-parameters",
    "href": "slides/08-hmc_vb.html#tuning-parameters",
    "title": "Lecture 08: Hamiltonian Monte Carlo and Variational Inference",
    "section": "Tuning parameters",
    "text": "Tuning parameters\n\nThere are three tuning parameters in HMC: step size \\(\\epsilon\\), number of steps \\(L\\), and mass matrix \\(M\\).\nSmall \\(\\epsilon\\):\n\nthe leapfrog approximation is good\nhigher acceptance rate\n\nLarge \\(L\\):\n\nbigger moves\nsmaller correlations between successive samples\nmore computation\n\nNeal (2011) recommends setting \\(\\epsilon\\) and \\(L\\) such that the acceptance rate is about 0.65.\nThe mass matrix \\(M\\) is usually set to be the identity matrix."
  },
  {
    "objectID": "slides/08-hmc_vb.html#no-u-turn-sampler-nutsnuts",
    "href": "slides/08-hmc_vb.html#no-u-turn-sampler-nutsnuts",
    "title": "Lecture 08: Hamiltonian Monte Carlo and Variational Inference",
    "section": "No-U-Turn Sampler (NUTS)1",
    "text": "No-U-Turn Sampler (NUTS)1\n\nThe hand-tuning of \\(\\epsilon\\) and \\(L\\) is tedious and time-consuming.\nThe No-U-Turn Sampler (NUTS) is an extension of HMC that eliminates the need to set the number of steps \\(\\epsilon\\) and \\(L\\).\nThe main ides behind NUTS is to simulate a trajectory of the Hamiltonian dynamics until it turns back on itself.\nSee Hoffman and Gelman (2014) for more details.\nBetancourt (2017) also provides a nice introduction to HMC and NUTS.\n\nHoffman, M. D., & Gelman, A. (2014). The No-U-Turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo. J. Mach. Learn. Res., 15(1), 1593-1623."
  },
  {
    "objectID": "slides/08-hmc_vb.html#motivation-1",
    "href": "slides/08-hmc_vb.html#motivation-1",
    "title": "Lecture 08: Hamiltonian Monte Carlo and Variational Inference",
    "section": "Motivation",
    "text": "Motivation\n\nWe have been focusing on simulating samples from the posterior distribution to make inference.\nHowever, the sampling-based approach will be very inefficient for large problems, i.e., a problem with hundreds or thousands of parameters.\nIn such cases, we might want to approximate the posterior distribution with a simpler distribution.\nThat is, we trade accuracy for efficiency.\nThe simplest approach is modal-approximation.\nHowever, this approach is not very flexible and does not scale well to high-dimensional problems.\nThere are many other approaches, and we focus on one of them: variational inference."
  },
  {
    "objectID": "slides/08-hmc_vb.html#variational-inference",
    "href": "slides/08-hmc_vb.html#variational-inference",
    "title": "Lecture 08: Hamiltonian Monte Carlo and Variational Inference",
    "section": "Variational inference",
    "text": "Variational inference\n\nVariational Bayes (VB) is an algorithmic framework for approximating joint distributions.\nSuppose we want to approximate the posterior distribution \\(\\pi(\\theta \\mid y)\\) with a simpler distribution \\(q(\\theta)\\) by minimizing the Kullback-Leibler divergence \\[\n\\text{KL}(q\\|\\pi) = \\int \\log\\frac{q(\\theta)}{\\pi(\\theta \\mid y)} q(\\theta)d\\theta = \\E_q\\left[\\log\\frac{q(\\theta)}{\\pi(\\theta \\mid y)}\\right].\n\\]\nThe difficulties are:\n\n\\(\\pi(\\theta \\mid y)\\) is available only up to a normalizing constant;\nwe cannot easily take posterior draws from \\(\\pi(\\theta \\mid y)\\);\nwe cannot easily compute expectations of interest, \\(\\E(h(\\theta) \\mid y)\\).\n\nHence we are not able to minimize the KL divergence directly."
  },
  {
    "objectID": "slides/08-hmc_vb.html#evidence-lower-bound-elbo",
    "href": "slides/08-hmc_vb.html#evidence-lower-bound-elbo",
    "title": "Lecture 08: Hamiltonian Monte Carlo and Variational Inference",
    "section": "Evidence Lower Bound (ELBO)",
    "text": "Evidence Lower Bound (ELBO)\n\nThe marginal distribution of \\(y\\) is given by \\[\np(y) = \\int \\pi(y, \\theta) d\\theta = \\int \\pi(y \\mid \\theta) \\pi(\\theta) d\\theta.\n\\]\nThen the evidence (log of marginal) is \\[\\begin{align*}\n\\log p(y) & = \\log \\int p(y, \\theta) d\\theta\n= \\log \\int \\frac{p(y, \\theta)}{q(\\theta)} q(\\theta) d\\theta\\\\\n& = \\log\\left(\\E_{q}\\left[\\frac{\\pi(y, \\theta)}{q(\\theta)}\\right]\\right)\n\\geq \\E_q\\left[\\log p(y, \\theta)\\right] - \\E_q\\left[\\log q(\\theta)\\right].\n\\end{align*}\\]\nThe last inequality is due to Jensen’s inequality.\nThe quantity \\(\\E_q\\left[\\log p(y, \\theta)\\right] - \\E_q\\left[\\log q(\\theta)\\right]\\) is called the evidence lower bound (ELBO)."
  },
  {
    "objectID": "slides/08-hmc_vb.html#evidence-lower-bound-elbo-1",
    "href": "slides/08-hmc_vb.html#evidence-lower-bound-elbo-1",
    "title": "Lecture 08: Hamiltonian Monte Carlo and Variational Inference",
    "section": "Evidence Lower Bound (ELBO)",
    "text": "Evidence Lower Bound (ELBO)\n\nThe KL divergence can be written as \\[\\begin{align*}\n\\text{KL}(q \\| \\pi) & = \\E_q\\left[\\log\\frac{q(\\theta)}{\\pi(\\theta \\mid y)}\\right] \\\\\n& = \\E_q\\left[\\log q(\\theta)\\right] - \\E_q\\left[\\log \\pi(\\theta \\mid y)\\right] \\\\\n& = \\E_q\\left[\\log q(\\theta)\\right] - \\E_q\\left[\\log p(y \\mid \\theta)\\pi(\\theta)\\right] + \\log p(y)\\\\\n& = - \\text{ELBO}(q) + \\log p(y).\n\\end{align*}\\]\nHence minimizing the KL divergence is equivalent to maximizing the ELBO."
  },
  {
    "objectID": "slides/08-hmc_vb.html#class-of-approximate-distributions",
    "href": "slides/08-hmc_vb.html#class-of-approximate-distributions",
    "title": "Lecture 08: Hamiltonian Monte Carlo and Variational Inference",
    "section": "Class of approximate distributions",
    "text": "Class of approximate distributions\n\nNow we need to choose a class \\(\\mc{Q}\\) of distributions \\(q(\\theta)\\) to approximate the posterior distribution \\(\\pi(\\theta \\mid y)\\).\nWe want the \\(\\mc{Q}\\) to be:\n\nflexible enough to approximate \\(\\pi(\\theta \\mid y)\\) well;\nsimple enough to be tractable;\nthe ELBO can be easily computed.\n\nThe class \\(\\mc{Q}\\) is often a parametric family, i.e., \\[\n\\mc{Q} = \\{q(\\theta \\mid \\phi): \\phi \\in \\Phi\\}.\n\\] and \\(\\phi\\) is called the variational parameter."
  },
  {
    "objectID": "slides/08-hmc_vb.html#mean-field-variational-family",
    "href": "slides/08-hmc_vb.html#mean-field-variational-family",
    "title": "Lecture 08: Hamiltonian Monte Carlo and Variational Inference",
    "section": "Mean-field variational family",
    "text": "Mean-field variational family\n\nThe simplest choice of \\(\\mc{Q}\\) is the mean-field variational family: \\[\n\\mc{Q}_{\\text{MF}} = \\left\\{q(\\theta \\mid \\phi) = \\prod_{j=1}^d q_j(\\theta_j \\mid \\phi_j): \\phi = (\\phi_1, \\ldots, \\phi_d) \\in \\Phi\\right\\}.\n\\]\nThe mean-field variational family assumes that the parameters are independent.\nObviously, this family does not contain the true posterior distribution.\nHowever, it is often used in practice because it is simple and easy to compute.\nWe can also partition \\(\\theta\\) into \\(K\\) blocks and assume that the parameters between different blocks are independent."
  },
  {
    "objectID": "slides/08-hmc_vb.html#maximizing-elbo",
    "href": "slides/08-hmc_vb.html#maximizing-elbo",
    "title": "Lecture 08: Hamiltonian Monte Carlo and Variational Inference",
    "section": "Maximizing ELBO",
    "text": "Maximizing ELBO\n\nThe ELBO is given by \\(\\text{ELBO}(q) = \\E_q\\left[\\log p(y, \\theta)\\right] - \\E_q\\left[\\log q(\\theta)\\right]\\).\nFor \\(q \\in \\mc{Q}_{\\text{MF}}\\), the entropy term is \\[\n\\E_q\\left[\\log q(\\theta)\\right] = \\E_q\\left[\\sum_{j=1}^d \\log q_j(\\theta_j \\mid \\phi_j)\\right] = \\sum_{j=1}^d \\E_{j}\\left[\\log q_j(\\theta_j \\mid \\phi_j)\\right]\n\\] where \\(\\E_j\\) is the expectation with respect to \\(q_j\\).\nThe first term is \\[\\begin{align*}\n\\E_q\\left[\\log p(y, \\theta)\\right] &= \\int \\log \\pi(y, \\theta) \\prod_{j=1}^d q_j(\\theta_j) d\\theta_1\\cdots d\\theta_d \\\\\n& = \\int \\E_{-j}\\left[\\log \\pi(y, \\theta)\\right] q_j(\\theta_j) d\\theta_j \\\\\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/08-hmc_vb.html#maximizing-elbo-1",
    "href": "slides/08-hmc_vb.html#maximizing-elbo-1",
    "title": "Lecture 08: Hamiltonian Monte Carlo and Variational Inference",
    "section": "Maximizing ELBO",
    "text": "Maximizing ELBO\n\nThe ELBO is \\[\n\\mc{L}(q_1, \\ldots, q_d) = \\int \\E_{-j}\\left[\\log \\pi(y, \\theta)\\right] q_j(\\theta_j) d\\theta_j - \\sum_{j=1}^d \\E_{j}\\left[\\log q_j(\\theta_j \\mid \\phi_j)\\right].\n\\]\nWe need to find \\(q_1, \\ldots, q_d\\) that maximize \\(\\mc{L}(q_1, \\ldots, q_d)\\) subject to the constraints \\(\\int q_j(\\theta_j) d\\theta_j = 1\\).\nUse Lagrange multipliers, take ``functional derivative’’ with respect to \\(q_j\\), set it to zero, and we have \\[\n\\log q_j(\\theta_j) = \\E_{-j}\\left[\\log \\pi(y, \\theta)\\right] + \\text{const}.\n\\]\nHence the optimal \\(q_j\\) is \\[\nq^{*}_j(\\theta_j) \\propto \\exp\\left(\\E_{-j}\\left[\\log \\pi(y, \\theta)\\right]\\right).\n\\]"
  },
  {
    "objectID": "slides/08-hmc_vb.html#example-8-school-problem",
    "href": "slides/08-hmc_vb.html#example-8-school-problem",
    "title": "Lecture 08: Hamiltonian Monte Carlo and Variational Inference",
    "section": "Example: 8-school problem",
    "text": "Example: 8-school problem\n\nThe model in the eight-school problem is \\[\\begin{align*}\nY_i & \\ind N(\\alpha_i, \\sigma_i^2), \\quad i = 1, \\ldots, 8 \\\\\n\\alpha_i & \\iid N(\\mu, \\tau)\\\\\n\\pi(\\mu, \\tau) & \\propto 1.\n\\end{align*}\\] (It can be shown that the posterior is proper.)\nLet \\(\\theta = (\\alpha_1, \\ldots, \\alpha_8, \\mu, \\tau)\\).\nThe log posterior is \\[\n\\log \\pi(\\theta \\mid y) = -\\frac{1}{2} \\sum_{j=1}^8 \\frac{\\left(y_j-\\alpha_j\\right)^2}{\\sigma_j^2}\n- 8 \\log \\tau-\\frac{1}{2} \\frac{1}{\\tau^2} \\sum_{j=1}^8\\left(\\alpha_j-\\mu\\right)^2 + \\text{const}.\n\\]\nWe consider the mean-field approximation \\(q(\\theta) = q(\\alpha_1) \\cdots q(\\alpha_8) q(\\mu) q(\\tau)\\)."
  },
  {
    "objectID": "slides/08-hmc_vb.html#mean-field-approximation-alpha_j",
    "href": "slides/08-hmc_vb.html#mean-field-approximation-alpha_j",
    "title": "Lecture 08: Hamiltonian Monte Carlo and Variational Inference",
    "section": "Mean-field approximation: \\(\\alpha_j\\)",
    "text": "Mean-field approximation: \\(\\alpha_j\\)\n\nAveraging over all the parameters other than \\(\\alpha_j\\), we have \\[\n\\E_{-\\alpha_j} \\log p(\\theta \\mid y)=-\\frac{1}{2} \\frac{\\left(y_j-\\alpha_j\\right)^2}{\\sigma_j^2}-\\frac{1}{2} \\E_{-\\alpha_j}\\left(\\frac{1}{\\tau^2}\\right) \\E_{-\\alpha_j}\\left(\\left(\\alpha_j-\\mu\\right)^2\\right)+\\text{const.}\n\\] which is a quadratic function of \\(\\alpha_j\\).\nThe optimal \\(q(\\alpha_j)\\) is a Gaussian distribution \\[\nq\\left(\\alpha_j\\right)=N\\left(\\alpha_j \\middle| \\frac{\\frac{1}{\\sigma_j^2} y_j+\\E_{-\\alpha_j}\\left(\\frac{1}{\\tau^2}\\right) \\E_{-\\alpha_j}(\\mu)}{\\frac{1}{\\sigma_j^2}+\\E_{-\\alpha_j}\\left(\\frac{1}{\\tau^2}\\right)}, \\frac{1}{\\frac{1}{\\sigma_j^2}+\\E_{-\\alpha_j}\\left(\\frac{1}{\\tau^2}\\right)}\\right).\n\\]"
  },
  {
    "objectID": "slides/08-hmc_vb.html#mean-field-approximation-mu",
    "href": "slides/08-hmc_vb.html#mean-field-approximation-mu",
    "title": "Lecture 08: Hamiltonian Monte Carlo and Variational Inference",
    "section": "Mean-field approximation: \\(\\mu\\)",
    "text": "Mean-field approximation: \\(\\mu\\)\n\nAveraging over all the parameters other than \\(\\mu\\), we have \\[\n\\E_{-\\mu} \\log p(\\theta \\mid y) = -\\frac{1}{2} \\E_{-\\mu}\\left(\\frac{1}{\\tau^2}\\right) \\sum_{j=1}^8\\left(\\mu - \\E_{-\\mu} \\left(\\alpha_j\\right)\\right)^2+\\text{const.}\n\\]\nThe optimal \\(q(\\mu)\\) is also a Gaussian distribution \\[\nq(\\mu)=N\\left(\\mu \\middle| \\frac{1}{8} \\sum_{j=1}^8 \\E_{-\\mu}\\left(\\alpha_j\\right), \\frac{1}{8} \\frac{1}{\\E_{-\\mu}\\left(\\frac{1}{\\tau^2}\\right)}\\right)\n\\]"
  },
  {
    "objectID": "slides/08-hmc_vb.html#mean-field-approximation-tau",
    "href": "slides/08-hmc_vb.html#mean-field-approximation-tau",
    "title": "Lecture 08: Hamiltonian Monte Carlo and Variational Inference",
    "section": "Mean-field approximation: \\(\\tau\\)",
    "text": "Mean-field approximation: \\(\\tau\\)\n\nAveraging over all the parameters other than \\(\\tau\\), we have \\[\n\\E_{-\\tau} \\log p(\\theta \\mid y) = -8 \\log \\tau -\\frac{1}{2} \\frac{1}{\\tau^2} \\sum_{j=1}^8 \\E_{-\\tau}\\left(\\left(\\alpha_j-\\mu\\right)^2\\right)+\\text{const.}\n\\]\nThe optimal \\(q(\\tau^2)\\) is an Inverse-Gamma distribution \\[\nq(\\tau^2) = \\text{Inv-Gamma}\\left(\\tau^2 \\middle| \\frac{7}{2}, \\frac{1}{2} \\sum_{j=1}^8 \\E_{-\\tau}\\left(\\left(\\alpha_j-\\mu\\right)^2\\right)\\right).\n\\]"
  },
  {
    "objectID": "slides/08-hmc_vb.html#summarizing-the-mean-field-approximation",
    "href": "slides/08-hmc_vb.html#summarizing-the-mean-field-approximation",
    "title": "Lecture 08: Hamiltonian Monte Carlo and Variational Inference",
    "section": "Summarizing the mean-field approximation",
    "text": "Summarizing the mean-field approximation\n\nThe mean-field approximation is \\[\\begin{align*}\n\\alpha_j & \\sim N(M_{\\alpha_j}, S^2_{\\alpha_j})\\\\\n\\mu & \\sim N(M_{\\mu}, S^2_{\\mu})\\\\\n\\tau^2 & \\sim \\text{Inv-Gamma}\\left(\\frac{7}{2}, \\frac{7}{2}B^2_{\\tau}\\right).\n\\end{align*}\\]\nTherefore \\[\\begin{align*}\n\\E_{-\\alpha_j}(\\tau^{-2}) & = \\E_{-\\mu}(\\tau^{-2})= \\frac{1}{B^2_{\\tau}}\\\\\n\\E_{-\\mu}(\\alpha_j) & = M_{\\alpha_j}\\\\\n\\E_{-\\tau}(\\alpha_j-\\mu)^2 & = S^2_{\\alpha_j} + M^2_{\\alpha_j} - 2M_{\\alpha_j}M_{\\mu}+ S^2_{\\mu} + M^2_{\\mu}.\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/08-hmc_vb.html#summarizing-the-mean-field-approximation-1",
    "href": "slides/08-hmc_vb.html#summarizing-the-mean-field-approximation-1",
    "title": "Lecture 08: Hamiltonian Monte Carlo and Variational Inference",
    "section": "Summarizing the mean-field approximation",
    "text": "Summarizing the mean-field approximation\nCombining all the results we have\n\\[\\begin{align*}\nM_{\\alpha_j} & = \\frac{\\frac{1}{\\sigma_j^2} y_j+\\E_{-\\alpha_j}\\left(\\frac{1}{\\tau^2}\\right) \\E_{-\\alpha_j}(\\mu)}{\\frac{1}{\\sigma_j^2}+\\E_{-\\alpha_j}\\left(\\frac{1}{\\tau^2}\\right)} = \\frac{\\frac{1}{\\sigma_j^2} y_j+\\frac{1}{B^2_{\\tau}} M_{\\mu}}{\\frac{1}{\\sigma_j^2}+\\frac{1}{B^2_{\\tau}}} = \\frac{B^2_{\\tau} y_j + \\sigma_j^2 M_{\\mu}}{\\sigma_j^2 + B^2_{\\tau}}\\\\\nS^2_{\\alpha_j} & = \\frac{1}{\\frac{1}{\\sigma_j^2}+\\E_{-\\alpha_j}\\left(\\frac{1}{\\tau^2}\\right)} = \\frac{1}{\\frac{1}{\\sigma_j^2}+\\frac{1}{B^2_{\\tau}}} = \\frac{B^2_{\\tau}\\sigma_j^2}{\\sigma_j^2 + B^2_{\\tau}}\\\\\nM_{\\mu} & = \\frac{1}{8} \\sum_{j=1}^8 \\E_{-\\mu}(\\alpha_j) = \\frac{1}{8} \\sum_{j=1}^8 M_{\\alpha_j}\\\\\nS^2_{\\mu} & = \\frac{1}{8} \\frac{1}{\\E_{-\\mu}\\left(\\frac{1}{\\tau^2}\\right)} = \\frac{1}{8} \\frac{1}{\\frac{1}{B^2_{\\tau}}} = \\frac{B^2_{\\tau}}{8}\\\\\nB^2_{\\tau} & = \\frac{1}{7} \\sum_{j=1}^8 \\E_{-\\tau}\\left(\\left(\\alpha_j-\\mu\\right)^2\\right) = \\frac{1}{7} \\sum_{j=1}^8 \\left(S^2_{\\alpha_j} + M^2_{\\alpha_j} - 2M_{\\alpha_j}M_{\\mu}+ S^2_{\\mu} + M^2_{\\mu}\\right)\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/08-hmc_vb.html#r-code",
    "href": "slides/08-hmc_vb.html#r-code",
    "title": "Lecture 08: Hamiltonian Monte Carlo and Variational Inference",
    "section": "R code",
    "text": "R code\n\nmax_iter &lt;- 100\n\n# Data\ny &lt;- c(28, 8, -3, 7, -1, 1, 18, 12)\nsig2 &lt;- c(15, 10, 16, 11, 9, 11, 10, 18)^2\n\n# Variational parameter\nvar_par &lt;- list(M_alpha = matrix(0, nrow = max_iter, ncol = 8), \n                S2_alpha = matrix(1, nrow = max_iter, ncol = 8),\n                M_mu = rep(0, max_iter),\n                S2_mu = rep(1, max_iter),\n                B2_tau = rep(1, max_iter))\n\nfor (i in 2:max_iter){\n    for(j in 1:8){\n        var_par$M_alpha[i, j] &lt;- (var_par$B2_tau[i-1] * y[j] + sig2[j] * var_par$M_mu[i-1]) / \n            (sig2[j] + var_par$B2_tau[i-1])\n        var_par$S2_alpha[i, j] &lt;- var_par$B2_tau[i-1] * sig2[j] / \n            (sig2[j] + var_par$B2_tau[i-1])\n    }\n    var_par$M_mu[i] &lt;- mean(var_par$M_alpha[i, ])\n    var_par$S2_mu[i] &lt;- var_par$B2_tau[i-1] / 8\n    var_par$B2_tau[i] &lt;- sum(var_par$S2_alpha[i, ])/7 + \n        sum(var_par$M_alpha[i, ]^2)/7 - \n        2*sum(var_par$M_alpha[i, ]) * var_par$M_mu[i]/7 +\n        var_par$S2_mu[i]*8/7 + \n        var_par$M_mu[i]^2*8/7\n}"
  },
  {
    "objectID": "slides/08-hmc_vb.html#results",
    "href": "slides/08-hmc_vb.html#results",
    "title": "Lecture 08: Hamiltonian Monte Carlo and Variational Inference",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "slides/08-hmc_vb.html#vb-inference",
    "href": "slides/08-hmc_vb.html#vb-inference",
    "title": "Lecture 08: Hamiltonian Monte Carlo and Variational Inference",
    "section": "VB Inference",
    "text": "VB Inference\n\n\n\nPosterior mean and standard deviation\n\n\nparameter\npost.mean\npost.sd\n\n\n\n\nalpha_1\n13.715\n7.970\n\n\nalpha_2\n8.051\n6.852\n\n\nalpha_3\n5.246\n8.109\n\n\nalpha_4\n7.633\n7.149\n\n\nalpha_5\n3.347\n6.503\n\n\nalpha_6\n5.099\n7.149\n\n\nalpha_7\n12.746\n6.852\n\n\nalpha_8\n8.934\n8.337\n\n\nmu\n8.096\n3.326\n\n\ntau\n10.591\n3.423"
  },
  {
    "objectID": "slides/08-hmc_vb.html#vb-using-stan",
    "href": "slides/08-hmc_vb.html#vb-using-stan",
    "title": "Lecture 08: Hamiltonian Monte Carlo and Variational Inference",
    "section": "VB using Stan",
    "text": "VB using Stan\nYou can also perform VB using Stan. The following is the Stan code for the school example.\n\nschool_model &lt;- rstan::stan_model(model_code = \"\n    data {\n      int&lt;lower=0&gt; J;         \n      real y[J];              \n      real&lt;lower=0&gt; sigma[J];\n    }\n    parameters {\n      real mu;                \n      real&lt;lower=0&gt; tau;      \n      real alpha[J];\n    }\n    model {\n      alpha ~ normal(mu, tau); // prior\n      y ~ normal(alpha, sigma); // likelihood\n    }\")"
  },
  {
    "objectID": "slides/08-hmc_vb.html#vb-using-stan-1",
    "href": "slides/08-hmc_vb.html#vb-using-stan-1",
    "title": "Lecture 08: Hamiltonian Monte Carlo and Variational Inference",
    "section": "VB using Stan",
    "text": "VB using Stan\n\nlibrary(rstan)\nschools_dat &lt;- list(J = 8,\n    y = c(28, 8, -3, 7, -1, 1, 18, 12),\n    sigma = c(15, 10, 16, 11, 9, 11, 10, 18)) \nfit_vb &lt;- rstan::vb(school_model, data = schools_dat, \n                    algorithm = \"meanfield\")\nfit_sampling &lt;- rstan::sampling(school_model, data = schools_dat, \n                                iter = 1000, chains = 4)"
  },
  {
    "objectID": "slides/08-hmc_vb.html#stan-output-vb",
    "href": "slides/08-hmc_vb.html#stan-output-vb",
    "title": "Lecture 08: Hamiltonian Monte Carlo and Variational Inference",
    "section": "Stan output (VB)",
    "text": "Stan output (VB)\n\n\n\n\n\n\nchain:1\n\n\n\n\nmu\n1.960\n\n\ntau\n10.271\n\n\nalpha.1\n4.828\n\n\nalpha.2\n2.878\n\n\nalpha.3\n-0.873\n\n\nalpha.4\n1.647\n\n\nalpha.5\n0.027\n\n\nalpha.6\n1.156\n\n\nalpha.7\n5.013\n\n\nalpha.8\n1.404\n\n\nlp__\n0.000"
  },
  {
    "objectID": "slides/08-hmc_vb.html#stan-output-sampling",
    "href": "slides/08-hmc_vb.html#stan-output-sampling",
    "title": "Lecture 08: Hamiltonian Monte Carlo and Variational Inference",
    "section": "Stan output (sampling)",
    "text": "Stan output (sampling)\n\n\n\n\n\n\nmean-chain:1\nmean-chain:2\nmean-chain:3\nmean-chain:4\nmean-all chains\n\n\n\n\nmu\n8.184\n7.792\n7.693\n8.177\n7.961\n\n\ntau\n7.277\n7.948\n8.428\n6.167\n7.455\n\n\nalpha[1]\n12.076\n11.829\n11.925\n10.648\n11.619\n\n\nalpha[2]\n8.036\n8.353\n8.149\n8.519\n8.264\n\n\nalpha[3]\n6.550\n5.451\n5.088\n6.471\n5.890\n\n\nalpha[4]\n7.620\n7.781\n7.503\n8.004\n7.727\n\n\nalpha[5]\n4.761\n4.094\n4.231\n5.642\n4.682\n\n\nalpha[6]\n6.059\n5.497\n6.013\n6.566\n6.034\n\n\nalpha[7]\n11.143\n11.490\n11.501\n10.567\n11.175\n\n\nalpha[8]\n8.447\n8.167\n7.922\n8.836\n8.343\n\n\nlp__\n-18.207\n-19.714\n-19.857\n-17.177\n-18.739"
  },
  {
    "objectID": "slides/08-hmc_vb.html#remark",
    "href": "slides/08-hmc_vb.html#remark",
    "title": "Lecture 08: Hamiltonian Monte Carlo and Variational Inference",
    "section": "Remark",
    "text": "Remark\nThe rstan::vb function is buggy and unstable. DO NOT use it for your research.\n\n\nHome"
  },
  {
    "objectID": "slides/10-glm.html#exponential-family",
    "href": "slides/10-glm.html#exponential-family",
    "title": "Lecture 10: Generalized Linear Model and Latent Variable Model",
    "section": "Exponential family",
    "text": "Exponential family\n\nRecall that an exponential family is a family of distributions \\[\nf(x \\mid \\theta) = h(x)\\exp(\\theta^T T(x) - \\psi(\\theta))\n\\] where \\(\\theta \\in \\R^k\\) and \\(T(x) = [T_1(x), \\ldots, T_k(x)]^T\\).\nTwo useful properties (from Bartlett’s identities):\n\n\\(\\E(T(X)) = \\nabla\\psi(\\theta)\\)\n\\(\\var(T(X)) = \\text{Hess}(\\psi(\\theta)) = \\nabla(\\nabla \\psi(\\theta))\\).\n\nThat is, the relationship between the parameter \\(\\theta\\) and the expectation \\(\\E(T(X))\\) determined by \\(\\nabla \\psi\\)."
  },
  {
    "objectID": "slides/10-glm.html#examples",
    "href": "slides/10-glm.html#examples",
    "title": "Lecture 10: Generalized Linear Model and Latent Variable Model",
    "section": "Examples",
    "text": "Examples\n\nNormal: \\(f(x \\mid \\mu, \\sigma^2) = \\exp\\left(-\\frac{1}{2\\sigma^2} x^2 + \\frac{\\mu}{\\sigma^2}x - \\frac{\\mu^2}{2\\sigma^2}\\right)\\), \\(x \\in \\R\\)\n\n\\(\\theta = \\left(-\\frac{1}{2\\sigma^2}, \\frac{\\mu}{\\sigma^2}\\right)\\), \\(T(x) = (-x^2, x)\\), \\(\\psi(\\theta) = -\\frac{\\theta_2^2}{4\\theta_1} = \\frac{\\mu^2}{2\\sigma^2}\\)\n\nBernoulli: \\(f(x \\mid p) = p^x(1-p)^{1-x} = \\exp\\left(x\\log\\frac{p}{1-p} + \\log(1-p)\\right)\\), \\(x \\in \\{0, 1\\}\\)\n\n\\(\\theta = \\log\\frac{p}{1-p}\\), \\(T(x) = x\\), \\(\\psi(\\theta) = -\\log(1-p) = \\log(1 + e^{\\theta})\\)\n\nPoisson: \\(f(x \\mid \\lambda) = \\frac{1}{x!}\\exp(x\\log\\lambda - \\lambda)\\), \\(x = 0, 1, 2, \\ldots\\)\n\n\\(\\theta = \\log\\lambda\\), \\(T(x) = x\\), \\(\\psi(\\theta) = \\exp(\\theta) = \\lambda\\)"
  },
  {
    "objectID": "slides/10-glm.html#generalized-linear-model-glm",
    "href": "slides/10-glm.html#generalized-linear-model-glm",
    "title": "Lecture 10: Generalized Linear Model and Latent Variable Model",
    "section": "Generalized Linear Model (GLM)",
    "text": "Generalized Linear Model (GLM)\n\nLet \\(Y\\) be univariate, \\(X \\in \\R^p\\), and \\(\\beta \\in \\R^p\\).\nA GLM is assuming \\(Y \\mid X, \\beta \\sim F_{\\theta}\\), where \\(\\theta = X^T\\beta\\) and \\(F_\\theta\\) has the density function \\[\nf(y \\mid \\theta) = h(y)\\exp(\\theta\\cdot y - \\psi(\\theta)).\n\\]\nTherefore, \\[\\begin{align*}\n\\E(Y \\mid X, \\beta) & = \\frac{d}{d\\theta}\\psi(\\theta) = \\psi^{\\prime}(X^T\\beta)\n\\end{align*}\\]\nEquivalently, \\[\ng(\\E(Y \\mid X, \\beta)) = X^T\\beta\n\\] where \\(g\\) is the inverse of \\(\\psi^{\\prime}\\).\nThe function \\(g\\) is called the link function."
  },
  {
    "objectID": "slides/10-glm.html#bernoulli-linear-model",
    "href": "slides/10-glm.html#bernoulli-linear-model",
    "title": "Lecture 10: Generalized Linear Model and Latent Variable Model",
    "section": "Bernoulli linear model",
    "text": "Bernoulli linear model\n\nLet \\(Y \\mid X, \\beta \\sim \\text{Ber}(p)\\).\nWe have \\(\\theta = \\log\\frac{p}{1-p}\\), \\(T(x) = x\\), \\(\\psi(\\theta) = -\\log(1-p) = \\log(1 + e^{\\theta})\\)\nThus, \\(\\psi^{\\prime}(\\theta) = \\frac{e^{\\theta}}{1 + e^{\\theta}}\\) and \\(g(u) = (\\psi^{\\prime})^{-1}(u) = \\log\\frac{u}{1-u}\\).\n\\(\\phi^{\\prime}\\) is called the logistic function; \\(g\\) is called the logit function.\nPutting altogether, we have \\[\ng(\\E(Y\\mid X, \\beta)) = \\log\\frac{\\E(Y \\mid X, \\beta)}{1 - \\E(Y \\mid X, \\beta)} = X^T\\beta\n\\] or equivalently \\[\n\\E(Y \\mid X, \\beta) = \\psi^{\\prime}(X^T\\beta) = \\frac{\\exp(X^T\\beta)}{1 + \\exp(X^T\\beta)},\n\\] aka logistic regression."
  },
  {
    "objectID": "slides/10-glm.html#poisson-linear-model",
    "href": "slides/10-glm.html#poisson-linear-model",
    "title": "Lecture 10: Generalized Linear Model and Latent Variable Model",
    "section": "Poisson linear model",
    "text": "Poisson linear model\n\nLet \\(Y \\mid X, \\beta \\sim \\text{Pois}(\\lambda)\\).\nWe have \\(\\theta = \\log\\lambda\\), \\(T(x) = x\\), \\(\\psi(\\theta) = \\exp(\\theta) = \\lambda\\).\nThus \\(\\psi^{\\prime}(\\theta) = \\exp(\\theta)\\) and \\(g(u) = (\\psi^{\\prime})^{-1}(u) = \\log u\\).\nPutting altogether, we have \\[\ng(\\E(Y\\mid X, \\beta)) = \\log\\E(Y \\mid X, \\beta) = X^T\\beta\n\\] or equivalently \\[\n\\E(Y \\mid X, \\beta) = \\exp(X^T\\beta),\n\\] aka Poisson log-linear regression."
  },
  {
    "objectID": "slides/10-glm.html#remarks",
    "href": "slides/10-glm.html#remarks",
    "title": "Lecture 10: Generalized Linear Model and Latent Variable Model",
    "section": "Remarks",
    "text": "Remarks\n\nThe link function \\(g = (\\psi^{\\prime})^{-1}\\) is sometimes called the canonical link function, since it is derived from the canonical representation of an exponential family.\nAll we need for a link function is that it matches the domain of \\(\\E(Y \\mid X, \\beta)\\) and \\(X^T\\beta\\).\nFor example, in the Bernoulli linear model, we could have used the probit link function \\[\ng(u) = \\Phi^{-1}(u): [0, 1] \\to \\R\n\\] where \\(\\Phi\\) is the CDF of the standard normal distribution.\nThis is called the probit regression."
  },
  {
    "objectID": "slides/10-glm.html#over--and-underdispersion",
    "href": "slides/10-glm.html#over--and-underdispersion",
    "title": "Lecture 10: Generalized Linear Model and Latent Variable Model",
    "section": "Over- and underdispersion",
    "text": "Over- and underdispersion\n\nIn the normal linear model, the conditional mean and variance of \\(Y \\mid X\\) are modeled by two different parameters \\(\\beta\\) and \\(\\sigma^2\\).\nHowever, in Poisson linear model, \\[\n\\E(Y \\mid X, \\beta) = \\exp(X^T\\beta) = \\var(Y \\mid X, \\beta).\n\\]\nAlso, in Bernoulli linear model, \\[\n\\var(Y \\mid X, \\beta) = \\E(Y\\mid X, \\beta)(1-\\E(Y \\mid X, \\beta)) = \\frac{\\exp(X^T\\beta)}{(1+\\exp(X^T\\beta))^2}.\n\\]\nThat is, the variance is determined by the mean, which might not be a reasonable assumption in practice.\nWhen the observed variance is smaller or larger than the assumed variance, it is called an underdispersion or overdispersion, respectively."
  },
  {
    "objectID": "slides/10-glm.html#exponential-dispersion-family",
    "href": "slides/10-glm.html#exponential-dispersion-family",
    "title": "Lecture 10: Generalized Linear Model and Latent Variable Model",
    "section": "Exponential Dispersion Family",
    "text": "Exponential Dispersion Family\n\nLet \\(\\theta \\in \\R\\) and \\(\\phi \\in \\R\\).\nThe exponential dispersion family is of the form \\[\nf(x \\mid \\theta, \\phi) = \\exp\\left(\\frac{x\\theta - b(\\theta)}{\\phi} + c(x, \\phi)\\right).\n\\]\nSimilarly from Bartlett’s identities1, \\[\n\\E(X) = b^{\\prime}(\\theta) \\quad \\text{and} \\quad\n\\var(X) = \\phi b^{\\prime\\prime}(\\theta).\n\\]\nThe parameter \\(\\phi\\) is called the dispersion parameter.\nLetting \\(\\mu = \\E(X) = b^{\\prime}(\\theta)\\), we have \\(\\var(X) = \\phi b^{\\prime\\prime}\\left((b^{\\prime})^{-1}(\\mu)\\right) = \\phi \\mc{V}(\\mu)\\), where \\(\\mc{V}(\\mu) = b^{\\prime\\prime}\\left((b^{\\prime})^{-1}(\\mu)\\right)\\) is called the variance function.\n\nSee https://bookdown.org/ssjackson300/ASM_Lecture_Notes/exponential_dispersion_family.html for the derivation."
  },
  {
    "objectID": "slides/10-glm.html#example",
    "href": "slides/10-glm.html#example",
    "title": "Lecture 10: Generalized Linear Model and Latent Variable Model",
    "section": "Example",
    "text": "Example\n\nNormal: \\(\\theta = \\mu\\), \\(\\phi = \\sigma^2\\), \\(b(\\theta) = \\frac{\\theta^2}{2}\\), \\(c(x, \\phi) = -\\frac{x^2}{2\\phi} - \\frac{1}{2}\\log(2\\pi\\phi)\\).\nBernoulli: \\(\\theta = \\log\\frac{p}{1-p}\\), \\(\\phi = 1\\), \\(b(\\theta) = \\log(1 + e^{\\theta})\\), \\(c(x, \\phi) = 0\\).\nPoisson: \\(\\theta = \\log\\lambda\\), \\(\\phi = 1\\), \\(b(\\theta) = e^{\\theta}\\), \\(c(x, \\phi) = -\\log(x!)\\).\nCan we generalize the Poisson/Bernoulli family to include a dispersion parameter?1\nWhat can we do if we observe over-/underdispersion in a Poisson/Bernoulli linear model?\n\nIn R, use family=quasi* which allows you to specify the variance function, e.g., quasipoisson or quasibinomial.\nOr you can use a negative binomial model for count data (you can easily show that the scaled negative binomial distribution is an exponential dispersion family).\n\n\nSee the Wiki and Huang & Kim (2021) for the Conway-Maxwell-Poisson distribution"
  },
  {
    "objectID": "slides/10-glm.html#example-recreation-demand",
    "href": "slides/10-glm.html#example-recreation-demand",
    "title": "Lecture 10: Generalized Linear Model and Latent Variable Model",
    "section": "Example: Recreation Demand",
    "text": "Example: Recreation Demand"
  },
  {
    "objectID": "slides/10-glm.html#example-1",
    "href": "slides/10-glm.html#example-1",
    "title": "Lecture 10: Generalized Linear Model and Latent Variable Model",
    "section": "Example",
    "text": "Example\n\nlibrary(AER)\nlibrary(MASS)\nlibrary(gtsummary)\ndata(\"RecreationDemand\")\nfit_pois &lt;- glm(trips ~ ., data = RecreationDemand, family = poisson)\n\ndt &lt;- dispersiontest(fit_pois)\n\nprint(paste(\"Dispersion Test: p-value =\", round(dt$p.value,4)))\n\n[1] \"Dispersion Test: p-value = 0.0079\"\n\nfit_nb &lt;- glm.nb(trips ~ ., data = RecreationDemand)\n\nt1 &lt;- tbl_regression(fit_pois, exponentiate = TRUE)\nt2 &lt;- tbl_regression(fit_nb, exponentiate = TRUE)\n\nt_all &lt;- tbl_merge(tbls = list(t1, t2), \n                   tab_spanner = c(\"**Poisson**\", \"**Neg-Bin**\"))"
  },
  {
    "objectID": "slides/10-glm.html#example-2",
    "href": "slides/10-glm.html#example-2",
    "title": "Lecture 10: Generalized Linear Model and Latent Variable Model",
    "section": "Example",
    "text": "Example\n\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      \n        Poisson\n      \n      \n        Neg-Bin\n      \n    \n    \n      IRR1\n      95% CI1\n      p-value\n      IRR1\n      95% CI1\n      p-value\n    \n  \n  \n    quality\n1.60\n1.55, 1.66\n&lt;0.001\n2.06\n1.89, 2.25\n&lt;0.001\n    ski\n\n\n\n\n\n\n        no\n—\n—\n\n—\n—\n\n        yes\n1.52\n1.36, 1.70\n&lt;0.001\n1.84\n1.38, 2.48\n&lt;0.001\n    income\n0.89\n0.86, 0.93\n&lt;0.001\n0.97\n0.89, 1.06\n0.5\n    userfee\n\n\n\n\n\n\n        no\n—\n—\n\n—\n—\n\n        yes\n2.46\n2.10, 2.86\n&lt;0.001\n1.95\n1.01, 4.22\n0.058\n    costC\n1.00\n0.99, 1.00\n0.3\n1.05\n1.02, 1.08\n&lt;0.001\n    costS\n0.96\n0.96, 0.96\n&lt;0.001\n0.91\n0.90, 0.93\n&lt;0.001\n    costH\n1.04\n1.03, 1.04\n&lt;0.001\n1.04\n1.02, 1.06\n&lt;0.001\n  \n  \n  \n    \n      1 IRR = Incidence Rate Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\n\n\nHome"
  },
  {
    "objectID": "slides/11-bayes_np.html#simulating-gaussian-processes",
    "href": "slides/11-bayes_np.html#simulating-gaussian-processes",
    "title": "Lecture 11: Gaussian Process",
    "section": "Simulating Gaussian processes",
    "text": "Simulating Gaussian processes\n\nlibrary(mvtnorm)\nGP_sim &lt;- function(from = 0, to = 1, mean_func = function(x){0},\n                   cov_func = function(x1, x2){exp(-16*(x1-x2)^2)}, \n                   m = 500){\n    x &lt;- seq(from, to, length.out = m)\n    mu &lt;- sapply(x, mean_func)\n    Sigma &lt;- outer(x, x, Vectorize(cov_func))\n    y &lt;- rmvnorm(1, mu, Sigma)\n    return(list(x = x, y = y))\n}"
  },
  {
    "objectID": "slides/11-bayes_np.html#simulating-gaussian-processes-1",
    "href": "slides/11-bayes_np.html#simulating-gaussian-processes-1",
    "title": "Lecture 11: Gaussian Process",
    "section": "Simulating Gaussian processes",
    "text": "Simulating Gaussian processes"
  },
  {
    "objectID": "slides/11-bayes_np.html#simulating-dirichlet-processes",
    "href": "slides/11-bayes_np.html#simulating-dirichlet-processes",
    "title": "Lecture 11: Gaussian Process",
    "section": "Simulating Dirichlet processes",
    "text": "Simulating Dirichlet processes\n\nlibrary(MCMCpack)\nDP_sim &lt;- function(alpha = 10, F0 = rnorm, n = 1e3){\n    s &lt;- F0(n)\n    V &lt;- rbeta(n, 1, alpha)\n    w &lt;- c(V[1], rep(0, n-1))\n    w[2:n] &lt;- sapply(2:n, function(i) V[i] * prod(1 - V[1:(i-1)]))\n    s_ord &lt;- order(s)\n    s &lt;- s[s_ord]\n    cum_prob &lt;- cumsum(w[s_ord])\n\n    return(list(x = s, cdf = cum_prob))\n}"
  },
  {
    "objectID": "slides/11-bayes_np.html#simulating-dirichlet-process",
    "href": "slides/11-bayes_np.html#simulating-dirichlet-process",
    "title": "Lecture 11: Bayesian Nonparametrics",
    "section": "Simulating Dirichlet process",
    "text": "Simulating Dirichlet process\n\nDraw \\(s_1, s_2, \\ldots\\) independently from \\(F_0\\).\nDraw \\(V_1, V_2, \\ldots \\iid \\text{Beta}(1, \\alpha)\\).\nLet \\(w_1=V_1\\) and \\(w_j=V_j \\prod_{i=1}^{j-1}\\left(1-V_i\\right)\\) for \\(j=2,3, \\ldots\\)\nLet \\(F\\) be the discrete distribution that puts mass \\(w_j\\) at \\(s_j\\), that is, \\(F=\\sum_{j=1}^{\\infty} w_j \\delta_{s_j}\\) where \\(\\delta_{s_j}\\) is a point mass at \\(s_j\\)."
  },
  {
    "objectID": "slides/11-bayes_np.html#simulating-dirichlet-processesref",
    "href": "slides/11-bayes_np.html#simulating-dirichlet-processesref",
    "title": "Lecture 11: Gaussian Process",
    "section": "Simulating Dirichlet processes1",
    "text": "Simulating Dirichlet processes1\n\nDraw \\(s_1, s_2, \\ldots\\) independently from \\(F_0\\).\nDraw \\(V_1, V_2, \\ldots \\iid \\text{Beta}(1, \\alpha)\\).\nLet \\(w_1=V_1\\) and \\(w_j=V_j \\prod_{i=1}^{j-1}\\left(1-V_i\\right)\\) for \\(j=2,3, \\ldots\\)\nLet \\(F\\) be the discrete distribution that puts mass \\(w_j\\) at \\(s_j\\), that is, \\(F=\\sum_{j=1}^{\\infty} w_j \\delta_{s_j}\\) where \\(\\delta_{s_j}\\) is a point mass at \\(s_j\\).\n\nhttps://www.stat.cmu.edu/~larry/=sml/nonparbayes.pdf"
  },
  {
    "objectID": "slides/11-bayes_np.html#simulating-dirichlet-processes-1",
    "href": "slides/11-bayes_np.html#simulating-dirichlet-processes-1",
    "title": "Lecture 11: Gaussian Process",
    "section": "Simulating Dirichlet processes",
    "text": "Simulating Dirichlet processes"
  },
  {
    "objectID": "slides/11-bayes_np.html#estimating-a-cdf",
    "href": "slides/11-bayes_np.html#estimating-a-cdf",
    "title": "Lecture 11: Gaussian Process",
    "section": "Estimating a cdf",
    "text": "Estimating a cdf\n\nSuppose we have \\(X_1, \\ldots, X_n \\iid F\\) and we want to estimate \\(F\\).\nThe most common estimate is the empirical cdf: \\[\n\\hat{F}_n(x) = \\frac{1}{n}\\sum_{i=1}^n I(X_i \\leq x).\n\\]\nThe Glivenko-Cantelli theorem states that \\(\\hat{F}_n \\to F\\) almost surely (in the sup norm).\nA Bayesian approach is to put a prior on \\(F\\) and find the posterior distribution of \\(F\\), i.e., \\[\\begin{align*}\nX_1, \\ldots, X_n &\\iid F\\\\\nF & \\sim \\text{Dir}(\\alpha, F_0).\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/11-bayes_np.html#posterior-1",
    "href": "slides/11-bayes_np.html#posterior-1",
    "title": "Lecture 11: Gaussian Process",
    "section": "Posterior",
    "text": "Posterior\n\nThe posterior of \\(F\\) given \\(X_1,\\ldots, X_n\\) is \\[\nF \\mid X_1, \\ldots, X_n \\sim \\text{Dir}\\left(\\alpha + n, \\frac{\\alpha}{\\alpha + n}F_0 + \\frac{n}{\\alpha + n}\\hat{F}_n\\right).\n\\]\nHence the posterior mean is \\[\n\\E[F \\mid X_1, \\ldots, X_n] = \\frac{\\alpha}{\\alpha + n}F_0 + \\frac{n}{\\alpha + n}\\hat{F}_n.\n\\]\nFor any measurable set \\(A\\), we have \\[\\begin{align*}\n\\E[F(A) \\mid X_1, \\ldots, X_n] & = \\frac{\\alpha}{\\alpha + n}F_0(A) + \\frac{1}{\\alpha + n}\\sum_{i=1}^n\\delta_{x_i}(A)\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/11-bayes_np.html#dirichlet-process-mixture-model",
    "href": "slides/11-bayes_np.html#dirichlet-process-mixture-model",
    "title": "Lecture 11: Gaussian Process",
    "section": "Dirichlet process mixture model",
    "text": "Dirichlet process mixture model\n\nFor density estimation, the Dirichlet process is not a useful prior, since it produces discrete distributions.\nInstead, we can use a Dirichlet process mixture model (DPMM): \\[\\begin{align*}\nF \\mid \\alpha, F_0 & \\sim \\text{DP}(\\alpha, F_0)\\\\\n\\theta_1, \\ldots, \\theta_n & \\iid F\\\\\nX_i \\mid \\theta_i & \\sim f(x \\mid \\theta_i).\n\\end{align*}\\]\n\n\n\nHome"
  },
  {
    "objectID": "slides/11-bayes_np.html#density-estimation",
    "href": "slides/11-bayes_np.html#density-estimation",
    "title": "Lecture 11: Gaussian Process",
    "section": "Density Estimation",
    "text": "Density Estimation\n\nSuppose we have \\(X_1, \\ldots, X_n \\iid F\\) with density \\(f\\) and we want to estimate \\(f\\).\nThe most common estimate is the kernel density estimate (KDE): \\[\n\\hat{f}_n(x) = \\frac{1}{n}\\sum_{i=1}^n \\frac{1}{h}K\\left(\\frac{x-X_i}{h}\\right)\n\\] where \\(K\\) is a kernel function and \\(h\\) is a bandwidth.\nFor example, the Gaussian kernel is \\(K(u) = \\exp(-u^2/2)\\).\nIn R, the KDE is implemented in the function density."
  },
  {
    "objectID": "slides/11-bayes_np.html#mixture-model",
    "href": "slides/11-bayes_np.html#mixture-model",
    "title": "Lecture 11: Gaussian Process",
    "section": "Mixture model",
    "text": "Mixture model\n\nA related approach is to use a mixture model: \\[\n\\hat{f}(x) = \\sum_{i=1}^k w_if(x \\mid \\theta_i)\n\\] where \\(f(x \\mid \\theta_i)\\) are density functions and \\(w_i\\) are weights.\nFor example, the Gaussian mixture model (GMM) is \\[\n\\hat{f}(x) = \\sum_{i=1}^k w_i\\phi(x \\mid \\mu_i, \\sigma_i^2)\n\\] where \\(\\phi(x \\mid \\mu_i, \\sigma_i^2)\\) is the density of a normal distribution with mean \\(\\mu_i\\) and variance \\(\\sigma_i^2\\).\nIn fact, a GMM can approximate any density (on \\(\\R\\)) arbitrarily well."
  },
  {
    "objectID": "slides/11-bayes_np.html#infinite-mixture-model",
    "href": "slides/11-bayes_np.html#infinite-mixture-model",
    "title": "Lecture 11: Gaussian Process",
    "section": "Infinite mixture model",
    "text": "Infinite mixture model\n\nWe can also consider an infinite mixture model: \\[\n\\hat{f}(x) = \\sum_{i=1}^\\infty w_if(x \\mid \\theta_i).\n\\]\nDirichlet process is an example of infinite mixture models, since \\[\nF = \\sum_{i=1}^{\\infty}w_i \\delta_{x_i}\n\\] where"
  },
  {
    "objectID": "slides/12-ebayes.html#dirichlet-distribution",
    "href": "slides/12-ebayes.html#dirichlet-distribution",
    "title": "Lecture 12: Dirichlet Process",
    "section": "Dirichlet distribution",
    "text": "Dirichlet distribution\n\nBeta distribution \\(p \\sim \\text{Beta}(\\alpha, \\beta)\\):\n\\[\nf(p \\mid \\alpha, \\beta) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}p^{\\alpha-1}(1-p)^{\\beta-1}, \\quad p \\in [0,1].\n\\]\nAlternative parameterization: \\(\\alpha^{\\prime} = \\alpha + \\beta\\), \\(\\nu = \\frac{\\alpha}{\\alpha + \\beta} = \\E(p)\\).\nDirichlet distribution \\((p_1, p_2, \\ldots, p_k) \\sim \\text{Dir}(\\alpha_1, \\ldots, \\alpha_k)\\): \\[\nf(p_1, \\ldots, p_k \\mid \\alpha_1, \\ldots, \\alpha_k) =\n\\frac{\\Gamma(\\alpha_1 + \\cdots + \\alpha_k)}{\\Gamma(\\alpha_1)\\cdots\\Gamma(\\alpha_k)} p_1^{\\alpha_1-1}\\cdots p_k^{\\alpha_k-1}\n\\] where \\(p_i \\in [0,1]\\) and \\(\\sum_{i=1}^k p_i = 1\\).\nAlternative: \\(\\alpha = \\sum_{i=1}^k\\alpha_i\\) and \\(\\nu = \\left(\\alpha_1/\\alpha, \\ldots \\alpha_k/\\alpha\\right) = \\E[(p_1, \\ldots, p_k)]\\)."
  },
  {
    "objectID": "slides/12-ebayes.html#definition",
    "href": "slides/12-ebayes.html#definition",
    "title": "Lecture 12: Dirichlet Process",
    "section": "Definition",
    "text": "Definition\nA ``random distribution’’ \\(F\\) is said to follow a Dirichlet process, denoted by \\[\nF \\sim \\mc{DP}(\\alpha, F_0),\n\\] if for any measurable partition \\(B_1, \\ldots, B_n\\) of the sample space of \\(F_0\\), the random vector \\((F(B_1), \\ldots, F(B_n))\\) has a Dirichlet distribution, i.e., \\[\n(F(B_1), \\ldots, F(B_n)) \\sim \\text{Dir}(\\alpha F_0(B_1), \\ldots, \\alpha F_0(B_n)).\n\\]\n\nThe parameter \\(\\alpha &gt; 0\\) is called the concentration parameter.\nThe parameter \\(F_0\\) is called the mean distribution."
  },
  {
    "objectID": "slides/12-ebayes.html#simulating-dirichlet-processesref",
    "href": "slides/12-ebayes.html#simulating-dirichlet-processesref",
    "title": "Lecture 12: Dirichlet Process",
    "section": "Simulating Dirichlet processes1",
    "text": "Simulating Dirichlet processes1\n\nDraw \\(s_1, s_2, \\ldots\\) independently from \\(F_0\\).\nDraw \\(V_1, V_2, \\ldots \\iid \\text{Beta}(1, \\alpha)\\).\nLet \\(w_1=V_1\\) and \\(w_j=V_j \\prod_{i=1}^{j-1}\\left(1-V_i\\right)\\) for \\(j=2,3, \\ldots\\)\nLet \\(F\\) be the discrete distribution that puts mass \\(w_j\\) at \\(s_j\\), that is, \\(F=\\sum_{j=1}^{\\infty} w_j \\delta_{s_j}\\) where \\(\\delta_{s_j}\\) is a point mass at \\(s_j\\).\n\nhttps://www.stat.cmu.edu/~larry/=sml/nonparbayes.pdf"
  },
  {
    "objectID": "slides/12-ebayes.html#simulating-dirichlet-processes",
    "href": "slides/12-ebayes.html#simulating-dirichlet-processes",
    "title": "Lecture 12: Dirichlet Process",
    "section": "Simulating Dirichlet processes",
    "text": "Simulating Dirichlet processes\n\nlibrary(MCMCpack)\nDP_sim &lt;- function(alpha = 10, F0 = rnorm, n = 1e3){\n    s &lt;- F0(n)\n    V &lt;- rbeta(n, 1, alpha)\n    w &lt;- c(V[1], rep(0, n-1))\n    w[2:n] &lt;- sapply(2:n, function(i) V[i] * prod(1 - V[1:(i-1)]))\n    s_ord &lt;- order(s)\n    s &lt;- s[s_ord]\n    cum_prob &lt;- cumsum(w[s_ord])\n\n    return(list(x = s, cdf = cum_prob))\n}"
  },
  {
    "objectID": "slides/12-ebayes.html#simulating-dirichlet-processes-1",
    "href": "slides/12-ebayes.html#simulating-dirichlet-processes-1",
    "title": "Lecture 12: Dirichlet Process",
    "section": "Simulating Dirichlet processes",
    "text": "Simulating Dirichlet processes"
  },
  {
    "objectID": "slides/12-ebayes.html#estimating-a-cdf",
    "href": "slides/12-ebayes.html#estimating-a-cdf",
    "title": "Lecture 12: Dirichlet Process",
    "section": "Estimating a cdf",
    "text": "Estimating a cdf\n\nSuppose we have \\(X_1, \\ldots, X_n \\iid F\\) and we want to estimate \\(F\\).\nThe most common estimate is the empirical cdf: \\[\n\\hat{F}_n(x) = \\frac{1}{n}\\sum_{i=1}^n I(X_i \\leq x).\n\\]\nThe Glivenko-Cantelli theorem states that \\(\\hat{F}_n \\to F\\) almost surely (in the sup norm).\nA Bayesian approach is to put a prior on \\(F\\) and find the posterior distribution of \\(F\\), i.e., \\[\\begin{align*}\nX_1, \\ldots, X_n &\\iid F\\\\\nF & \\sim \\text{Dir}(\\alpha, F_0).\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/12-ebayes.html#posterior",
    "href": "slides/12-ebayes.html#posterior",
    "title": "Lecture 12: Dirichlet Process",
    "section": "Posterior",
    "text": "Posterior\n\nThe posterior of \\(F\\) given \\(X_1,\\ldots, X_n\\) is \\[\nF \\mid X_1, \\ldots, X_n \\sim \\text{Dir}\\left(\\alpha + n, \\frac{\\alpha}{\\alpha + n}F_0 + \\frac{n}{\\alpha + n}\\hat{F}_n\\right).\n\\]\nHence the posterior mean is \\[\n\\E[F \\mid X_1, \\ldots, X_n] = \\frac{\\alpha}{\\alpha + n}F_0 + \\frac{n}{\\alpha + n}\\hat{F}_n.\n\\]\nFor any measurable set \\(A\\), we have \\[\\begin{align*}\n\\E[F(A) \\mid X_1, \\ldots, X_n] & = \\frac{\\alpha}{\\alpha + n}F_0(A) + \\frac{1}{\\alpha + n}\\sum_{i=1}^n\\delta_{x_i}(A)\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/12-ebayes.html#example",
    "href": "slides/12-ebayes.html#example",
    "title": "Lecture 12: Dirichlet Process",
    "section": "Example",
    "text": "Example\n\nn &lt;- 10\nx &lt;- rcauchy(n)\nalpha &lt;- 10\ncurve(pnorm(x), -5, 5, lty = 2, lwd = 2, \n      ylab = TeX(\"$P(X \\\\leq x)$\"), xlab = \"x\")\nlines(ecdf(x), col = \"red\", lwd = 2)\n\nF_bar &lt;- function(m, alpha, n, F0, x){\n    u &lt;- runif(m)\n    out &lt;- rep(NA, m)\n    out[u &lt; alpha/(alpha + n)] &lt;- F0(sum(u &lt; alpha/(alpha + n)))\n    out[u &gt;= alpha/(alpha + n)] &lt;- sample(x, sum(u &gt;= alpha/(alpha + n)), replace = TRUE)\n    return(out)\n}"
  },
  {
    "objectID": "slides/12-ebayes.html#density-estimation",
    "href": "slides/12-ebayes.html#density-estimation",
    "title": "Lecture 12: Dirichlet Process",
    "section": "Density Estimation",
    "text": "Density Estimation\n\nSuppose we have \\(X_1, \\ldots, X_n \\iid F\\) with density \\(f\\) and we want to estimate \\(f\\).\nThe most common estimate is the kernel density estimate (KDE): \\[\n\\hat{f}_n(x) = \\frac{1}{n}\\sum_{i=1}^n \\frac{1}{h}K\\left(\\frac{x-X_i}{h}\\right)\n\\] where \\(K\\) is a kernel function and \\(h\\) is a bandwidth.\nFor example, the Gaussian kernel is \\(K(u) = \\exp(-u^2/2)\\).\nIn R, the KDE is implemented in the function density."
  },
  {
    "objectID": "slides/12-ebayes.html#mixture-model",
    "href": "slides/12-ebayes.html#mixture-model",
    "title": "Lecture 12: Dirichlet Process",
    "section": "Mixture model",
    "text": "Mixture model\n\nA related approach is to use a mixture model: \\[\n\\hat{f}(x) = \\sum_{i=1}^k w_if(x \\mid \\theta_i)\n\\] where \\(f(x \\mid \\theta_i)\\) are density functions and \\(w_i\\) are weights.\nFor example, the Gaussian mixture model (GMM) is \\[\n\\hat{f}(x) = \\sum_{i=1}^k w_i\\phi(x \\mid \\mu_i, \\sigma_i^2)\n\\] where \\(\\phi(x \\mid \\mu_i, \\sigma_i^2)\\) is the density of a normal distribution with mean \\(\\mu_i\\) and variance \\(\\sigma_i^2\\).\nIn fact, a GMM can approximate any density (on \\(\\R\\)) arbitrarily well."
  },
  {
    "objectID": "slides/12-ebayes.html#infinite-mixture-model",
    "href": "slides/12-ebayes.html#infinite-mixture-model",
    "title": "Lecture 12: Dirichlet Process",
    "section": "Infinite mixture model",
    "text": "Infinite mixture model\n\nWe can also consider an infinite mixture model: \\[\n\\hat{f}(x) = \\sum_{i=1}^\\infty w_if(x \\mid \\theta_i).\n\\]\nDirichlet process is an example of infinite mixture models, since \\[\nF = \\sum_{i=1}^{\\infty}w_i \\delta_{x_i}\n\\] where"
  },
  {
    "objectID": "slides/12-ebayes.html#dirichlet-process-mixture-model",
    "href": "slides/12-ebayes.html#dirichlet-process-mixture-model",
    "title": "Lecture 12: Dirichlet Process",
    "section": "Dirichlet process mixture model",
    "text": "Dirichlet process mixture model\n\nFor density estimation, the Dirichlet process is not a useful prior, since it produces discrete distributions.\nInstead, we can use a Dirichlet process mixture model (DPMM): \\[\\begin{align*}\nF \\mid \\alpha, F_0 & \\sim \\text{DP}(\\alpha, F_0)\\\\\n\\theta_1, \\ldots, \\theta_n \\mid F & \\iid F\\\\\nX_i \\mid \\theta_i & \\ind f(x \\mid \\theta_i).\n\\end{align*}\\]"
  }
]