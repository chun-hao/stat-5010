[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Statistical Methods (Fall 2023)",
    "section": "",
    "text": "Week\nDate\nTopics\nReading\n\n\n\n\n1\n9/5\nIntroduction & History of Bayes Theorem\n\n\n\n2\n9/12\nBackbones of Bayesian Analysis; One-parameter Models; Conjugate Priors\nHoff Ch. 1-3, BC Ch. 1\n\n\n3\n9/19\nPrior Information and Prior Distribution\nBC Ch. 3\n\n\n4\n9/26\nDecision Theory and Bayesian Estimation\nBC Ch. 2, 4\n\n\n5\n10/3\nConnections to non-Bayesian Analysis; Hierarchical Models\nBDA Ch. 4, 5\n\n\n6\n10/10\nNo class (National Holiday)\n\n\n\n7\n10/17\nTesting and Model Comparison\nBC Ch. 5, 7, BDA Ch. 6, 7\n\n\n8\n10/24\nProject Proposal\n\n\n\n9\n10/31\nMetropolis-Hastings algorithms; Gibbs sampler\nBDA Ch. 10-11\n\n\n10\n11/7\nHamiltonian Monte Carlo; Variational Inference\nBDA Ch. 12-13\n\n\n11\n11/14\nBayesian regression\nBDA Ch. 14\n\n\n12\n11/21\nGeneralized Linear Models; Latent Variable Model\nBDA Ch. 16, 18\n\n\n13\n11/28\nEmpirical Bayes\nBC Ch. 10\n\n\n14\n12/5\nBayesian Nonparametrics\nBDA Ch. 21, 23\n\n\n15\n12/12\nFinal Project Presentation\n\n\n\n16\n12/19\nFinal Project Presentation"
  },
  {
    "objectID": "index.html#course-schedule",
    "href": "index.html#course-schedule",
    "title": "Bayesian Statistical Methods (Fall 2023)",
    "section": "",
    "text": "Week\nDate\nTopics\nReading\n\n\n\n\n1\n9/5\nIntroduction & History of Bayes Theorem\n\n\n\n2\n9/12\nBackbones of Bayesian Analysis; One-parameter Models; Conjugate Priors\nHoff Ch. 1-3, BC Ch. 1\n\n\n3\n9/19\nPrior Information and Prior Distribution\nBC Ch. 3\n\n\n4\n9/26\nDecision Theory and Bayesian Estimation\nBC Ch. 2, 4\n\n\n5\n10/3\nConnections to non-Bayesian Analysis; Hierarchical Models\nBDA Ch. 4, 5\n\n\n6\n10/10\nNo class (National Holiday)\n\n\n\n7\n10/17\nTesting and Model Comparison\nBC Ch. 5, 7, BDA Ch. 6, 7\n\n\n8\n10/24\nProject Proposal\n\n\n\n9\n10/31\nMetropolis-Hastings algorithms; Gibbs sampler\nBDA Ch. 10-11\n\n\n10\n11/7\nHamiltonian Monte Carlo; Variational Inference\nBDA Ch. 12-13\n\n\n11\n11/14\nBayesian regression\nBDA Ch. 14\n\n\n12\n11/21\nGeneralized Linear Models; Latent Variable Model\nBDA Ch. 16, 18\n\n\n13\n11/28\nEmpirical Bayes\nBC Ch. 10\n\n\n14\n12/5\nBayesian Nonparametrics\nBDA Ch. 21, 23\n\n\n15\n12/12\nFinal Project Presentation\n\n\n\n16\n12/19\nFinal Project Presentation"
  },
  {
    "objectID": "index.html#important-dates",
    "href": "index.html#important-dates",
    "title": "Bayesian Statistical Methods (Fall 2023)",
    "section": "Important Dates:",
    "text": "Important Dates:\n\n10/10: No class (National Holiday)\n10/24: Project proposal: you need to prepare a 10-min presentation for your project proposal\n12/12,19: Final project presentation: a 20-min presentation for your final project"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "BDA Book Website\nStan"
  },
  {
    "objectID": "resources.html#links",
    "href": "resources.html#links",
    "title": "Resources",
    "section": "",
    "text": "BDA Book Website\nStan"
  },
  {
    "objectID": "slides/01-intro.html#course-description",
    "href": "slides/01-intro.html#course-description",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Course Description",
    "text": "Course Description\n\nStatistical analysis is an inversion process: retrieving the causes (models/parameters) from the effects (observations/sample/data)\nBayes’ Theorem: If \\(A\\) and \\(E\\) are events such that \\(\\P(E) \\neq 0\\), \\[\\P(A \\mid E)=\\frac{\\P(E \\mid A) \\P(A)}{\\P(E \\mid A) \\P(A)+\\P\\left(E \\mid\nA^c\\right) \\P\\left(A^c\\right)}.\\]\nLet \\(A\\) be causes and \\(E\\) be effects. Bayes’ Theorem gives the relationship between \\(\\P(A\\mid E)\\) (statistical inference) and \\(\\P(E\\mid A)\\) (phenomenon).\nThis course will focus on statistical analysis under the Bayesian paradigm."
  },
  {
    "objectID": "slides/01-intro.html#course-description-1",
    "href": "slides/01-intro.html#course-description-1",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Course Description",
    "text": "Course Description\n\n\nAdapting Bayes’ Theorem to a parametric problem,\n\n\\[\\pi(\\theta|x) = \\frac{p(x|\\theta)\\pi(\\theta)}{\\int\n    p(x|\\theta^{\\prime})\\pi(\\theta^{\\prime})d\\theta^{\\prime}}.\\]\n\\[\\begin{align*}\n\\theta        & \\longleftrightarrow \\text{parameter}\\\\\nx             & \\longleftrightarrow \\text{data}\\\\\n\\pi(\\theta)   & \\longleftrightarrow \\textcolor{magenta}{\\text{prior distribution}}\\\\\np(x|\\theta)   & \\longleftrightarrow \\text{likelihood/model}\\\\\n\\pi(\\theta|x) & \\longleftrightarrow \\textcolor{magenta}{\\text{posterior distribution}}\\\\\n\\end{align*}\\] \n\nKey components: priors and posteriors"
  },
  {
    "objectID": "slides/01-intro.html#course-description-2",
    "href": "slides/01-intro.html#course-description-2",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Course Description",
    "text": "Course Description\nA general procedure for Bayesian statistical analysis:\n\nChoose a class \\(\\mc{M}\\) of models for the sample \\(x\\);\nChoose a prior distribution over \\(\\mc{M}\\);\nUse Bayes Theorem to compute the posterior distribution of models given the sample;\nPick a “good” model based on the posterior distribution."
  },
  {
    "objectID": "slides/01-intro.html#topics",
    "href": "slides/01-intro.html#topics",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Topics",
    "text": "Topics\n\n\nPrinciples for the choice of priors\nInference based on the posterior\nBayesian analysis for different problems:\n\nestimation\nhypothesis testing\nregression\nprediction\nnonparametric model\nmodel comparison"
  },
  {
    "objectID": "slides/01-intro.html#course-materials",
    "href": "slides/01-intro.html#course-materials",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Course Materials",
    "text": "Course Materials\n\n\nHoff, P. D. (2009). A First Course in Bayesian Statistical Methods.\nRobert, C. P. (2007). The Bayesian Choice, 2nd Edition.\nGelman, A. et al. (2020). Bayesian Data Analysis, 3rd Edition.\nRobert, C. P., & Casella, G. (2004). Monte Carlo Statistical Methods, 2nd Edition.\nProf. Rebecca Steorts at Duke. Lecture notes\nSlides will be posted on NTU Cool."
  },
  {
    "objectID": "slides/01-intro.html#course-schedule",
    "href": "slides/01-intro.html#course-schedule",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Course Schedule",
    "text": "Course Schedule\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopics\nReading\n\n\n\n\n1\n9/5\nIntroduction & History of Bayes Theorem\n\n\n\n2\n9/12\nBackbones of Bayesian Analysis; One-parameter Models; Conjugate Priors\nHoff Ch. 1-3, BC Ch. 1\n\n\n3\n9/19\nPrior Information and Prior Distribution\nBC Ch. 3\n\n\n4\n9/26\nDecision Theory and Bayesian Estimation\nBC Ch. 2, 4\n\n\n5\n10/3\nConnections to non-Bayesian Analysis; Hierarchical Models\nBDA Ch. 4, 5\n\n\n6\n10/10\nNo class (National Holiday)\n\n\n\n7\n10/17\nTesting and Model Comparison\nBC Ch. 5, 7, BDA Ch. 6, 7\n\n\n8\n10/24\nProject Proposal\n\n\n\n9\n10/31\nMetropolis-Hastings algorithms; Gibbs sampler\nBDA Ch. 10-11\n\n\n10\n11/7\nHamiltonian Monte Carlo; Variational Inference\nBDA Ch. 12-13\n\n\n11\n11/14\nBayesian regression\nBDA Ch. 14\n\n\n12\n11/21\nGeneralized Linear Models; Latent Variable Model\nBDA Ch. 16, 18\n\n\n13\n11/28\nEmpirical Bayes\nBC Ch. 10\n\n\n14\n12/5\nBayesian Nonparametrics\nBDA Ch. 21, 23\n\n\n15\n12/12\nFinal Project Presentation\n\n\n\n16\n12/19\nFinal Project Presentation"
  },
  {
    "objectID": "slides/01-intro.html#grading",
    "href": "slides/01-intro.html#grading",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Grading",
    "text": "Grading\n\n\nHomework (3 - 4 homeworks): 30%\nOral Presentation (project proposal and final presentation): 40%\nWritten Report: 30%\n\n\nContact information\n\n\nOffice: Room 602, Cosmology Hall\nE-mail: chunhaoy@ntu.edu.tw\nOffice Hours: Tue. 3-5pm or by appointment"
  },
  {
    "objectID": "slides/01-intro.html#final-project",
    "href": "slides/01-intro.html#final-project",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Final Project",
    "text": "Final Project\n\n\nA group of 2-3 students\nChoose a dataset and find a problem which you can answer with your dataset.\nFigure out how to perform legitimate statistical analysis based on (but not limited to) what we learn in this course.\nAn oral presentation of your results (20 mins per group)\nA written report (8 pages) including:\n\ndescription of the dataset\nstatistical problem\nmodel and inference\nresults and conclusion\nreference\n\nMore details will be announced after midterm exam."
  },
  {
    "objectID": "slides/01-intro.html#prerequisites",
    "href": "slides/01-intro.html#prerequisites",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Prerequisites",
    "text": "Prerequisites\n\n\nBasic calculus and linear algebra\nMathematical statistics (undergrad level is fine)\nFamiliarity with some data analytic programming language (R/Python/MATLAB)\nExperiences with data analysis is not required but will be helpful"
  },
  {
    "objectID": "slides/01-intro.html#example-8-school-dataset",
    "href": "slides/01-intro.html#example-8-school-dataset",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Example: 8-school dataset",
    "text": "Example: 8-school dataset\nA study was performed for the ETS to analyze the effects of special coaching programs on SAT scores. Eight schools with their own coaching programs were involved in this study.\n\n\n\n\n\nSchool\nEffect\nStd\n\n\n\n\nA\n28.39\n14.9\n\n\nB\n7.94\n10.2\n\n\nC\n-2.75\n16.3\n\n\nD\n6.82\n11.0\n\n\nE\n-0.64\n9.4\n\n\nF\n0.63\n11.4\n\n\nG\n18.01\n10.4\n\n\nH\n12.16\n17.6\n\n\n\n\n\n\n\n\n\nSee Ch 5.5 of BDA3."
  },
  {
    "objectID": "slides/01-intro.html#example-8-school-dataset-1",
    "href": "slides/01-intro.html#example-8-school-dataset-1",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Example: 8-school dataset",
    "text": "Example: 8-school dataset\nWhat can we ask about this dataset?\n\nIs there any difference between the coaching programs?\nWhich program is considered effective?\nWhich program is the most effective?\nHow much can we expect to improve on the SAT scores after taking the coaching programs?"
  },
  {
    "objectID": "slides/01-intro.html#step-1-2-determine-the-likelihood-and-prior",
    "href": "slides/01-intro.html#step-1-2-determine-the-likelihood-and-prior",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Step 1 & 2: Determine the likelihood and prior",
    "text": "Step 1 & 2: Determine the likelihood and prior\n\nConsider \\[\\begin{align*}\nY_{i} \\mid \\theta_i, \\sigma_i^2 & \\ind N(\\theta_i, \\sigma_i^2), i = 1,\\ldots, 8\\\\\n\\theta_i \\mid \\mu, \\tau & \\ind N(\\mu, \\tau^2)\n\\end{align*}\\] where \\(Y_i\\)’s are the observed average treatment effect and \\(\\sigma_i^2\\)’s are the variance of the treatment effects.\nWe are interested in estimating the \\(\\theta_i\\)’s, which are the effects of the coaching programs."
  },
  {
    "objectID": "slides/01-intro.html#step-4-inference-based-on-the-posterior",
    "href": "slides/01-intro.html#step-4-inference-based-on-the-posterior",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Step 4: Inference based on the posterior",
    "text": "Step 4: Inference based on the posterior\n\ndata {\n  int&lt;lower=0&gt; J;         \n  real y[J];              \n  real&lt;lower=0&gt; sigma[J];\n}\nparameters {\n  real mu;                \n  real&lt;lower=0&gt; tau;      \n  real theta[J];\n}\nmodel {\n  theta ~ normal(mu, tau); // prior\n  y ~ normal(theta, sigma); // likelihood\n}"
  },
  {
    "objectID": "slides/01-intro.html#step-4-inference-based-on-the-posterior-1",
    "href": "slides/01-intro.html#step-4-inference-based-on-the-posterior-1",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Step 4: Inference based on the posterior",
    "text": "Step 4: Inference based on the posterior\n\nlibrary(rstan)\nschools_dat &lt;- list(J = 8,\n    y = c(28, 8, -3, 7, -1, 1, 18, 12),\n    sigma = c(15, 10, 16, 11, 9, 11, 10, 18)) \nfit &lt;- rstan::sampling(school_model, data = schools_dat,\n    chains = 4, iter = 11000, warmup = 1000, thin = 10)"
  },
  {
    "objectID": "slides/01-intro.html#step-4-inference-based-on-the-posterior-2",
    "href": "slides/01-intro.html#step-4-inference-based-on-the-posterior-2",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Step 4: Inference based on the posterior",
    "text": "Step 4: Inference based on the posterior"
  },
  {
    "objectID": "slides/01-intro.html#stan-statistical-computing-platform",
    "href": "slides/01-intro.html#stan-statistical-computing-platform",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Stan: Statistical Computing Platform",
    "text": "Stan: Statistical Computing Platform\n\n\n\n\n\n\n\nProvide automated Bayesian computation\nInterface with R/Python/MATLAB/…"
  },
  {
    "objectID": "slides/01-intro.html#who-is-the-father-of-statistics",
    "href": "slides/01-intro.html#who-is-the-father-of-statistics",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Who is the father of statistics?",
    "text": "Who is the father of statistics?\nSome important statisticians:\n\nCarl Friedrich Gauss (1777 - 1855): Gaussian distribution, …\nRonald A. Fisher (1890 - 1962): likelihood based inference, …\nKarl Pearson (1857 - 1936): Pearson’s correlation coefficient, …\nWilliam Sealy Gosset (1876 - 1937): \\(t\\)-test and \\(t\\) distribution\nJerzy Neyman (1894 - 1981): Neyman-Pearson Lemma, …\nPierre-Simon Laplace (1749 - 1827): Central Limit Theorem, …\nThomas Bayes (1702 - 1761): Bayes Theorem, … :::"
  },
  {
    "objectID": "slides/01-intro.html#thomas-bayes",
    "href": "slides/01-intro.html#thomas-bayes",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Thomas Bayes",
    "text": "Thomas Bayes\n\n\n\nBritish Reverend Thomas Bayes (1702 - 1761)\nWork on inverse probability problem\nDiscover a relationship between causes and observations (1746 - 1749), which is later called Bayes Theorem\nWrite “An Essay towards solving a Problem in the Doctrine of Chances”\nHis philosopher friend, Richard Price, edited and published his result."
  },
  {
    "objectID": "slides/01-intro.html#bayes-thought-experiment",
    "href": "slides/01-intro.html#bayes-thought-experiment",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Bayes’ Thought Experiment",
    "text": "Bayes’ Thought Experiment"
  },
  {
    "objectID": "slides/01-intro.html#pierre-simon-laplace",
    "href": "slides/01-intro.html#pierre-simon-laplace",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Pierre-Simon Laplace",
    "text": "Pierre-Simon Laplace\n\n\n\nA French mathematician (1749 - 1827)\nDiscover the same rule in 1774, independent of Bayes\nUsing conditional probability, write down the rule as \\[\\begin{align*}\n\\P(A \\mid E)=\\frac{\\P(E \\mid A) \\P(A)}{\\P(E \\mid A) \\P(A)+\\P\\left(E \\mid\nA^c\\right) \\P\\left(A^c\\right)}.\n\\end{align*}\\]\nPropose to use equi-probability as the prior, i.e., the uniform prior"
  },
  {
    "objectID": "slides/01-intro.html#laplaces-rule-of-succession",
    "href": "slides/01-intro.html#laplaces-rule-of-succession",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Laplace’s Rule of Succession",
    "text": "Laplace’s Rule of Succession\n\nIf we repeat an experiment that we know can result in a success or failure, \\(n\\) times independently, and get \\(s\\) successes, and \\(n-s\\) failures, then what is the probability that the next repetition will succeed?\nLaplace’s Answer: \\[\\P\\left(X_{n+1}=1 \\mid X_1+\\cdots+X_n=s\\right)=\\frac{s+1}{n+2}\\]\nThis answer is obtained from the Beta-Binomial model."
  },
  {
    "objectID": "slides/01-intro.html#pierre-simon-laplace-1",
    "href": "slides/01-intro.html#pierre-simon-laplace-1",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Pierre-Simon Laplace",
    "text": "Pierre-Simon Laplace\n\nLaplace was not satisfied with the uniform prior, neither did other mathematicians.\nHowever, he still applied Bayes Theorem to solve many practical problems:\n\nEstimating the French population\nBirth and census study\nCredibility of witnesses in the court\n\nIn 1810, he announced the Central Limit Theorem.\nAt the age of 62, he turned to frequentist-based approach. Why?"
  },
  {
    "objectID": "slides/01-intro.html#decline-of-bayesian-statistics",
    "href": "slides/01-intro.html#decline-of-bayesian-statistics",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Decline of Bayesian Statistics",
    "text": "Decline of Bayesian Statistics\n\nThe subjective component, prior distribution, is heavily criticized by mathematicians and theorists.\nFor large dataset, Bayesian and frequentist produced almost the same result.\nDataset became more reliable, and frequentist approaches are easier to implement.\nBayesian approaches are computationally challenging, even for simple models.\nResearchers tend to design their own experiments to answer industrial/scientific questions."
  },
  {
    "objectID": "slides/01-intro.html#sir-ronald-a.-fisher",
    "href": "slides/01-intro.html#sir-ronald-a.-fisher",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Sir Ronald A. Fisher",
    "text": "Sir Ronald A. Fisher\n\n\n\nBritish Statistician/Mathematician (1890 - 1962)\nTea and milk experiment\nFather of modern statistics and experimental design\nProblem –&gt; Experiment –&gt; Data –&gt; Analysis\nDevelop a collection of statistical methods, e.g., MLE, ANOVA, Fisher information, sufficient statistics, etc."
  },
  {
    "objectID": "slides/01-intro.html#revival-of-bayesian-statistics",
    "href": "slides/01-intro.html#revival-of-bayesian-statistics",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Revival of Bayesian Statistics",
    "text": "Revival of Bayesian Statistics\n\nClean, reliable data -&gt; Frequentist\nThrough the use of prior information, Bayesian approaches are actually more powerful and flexible when handling complex datasets.\nAdvances in computing technologies\nReadings:\n\nLindley, D. V. (1975). The future of statistics: A Bayesian 21st century. Advances in Applied Probability, 7, 106-115.\nEfron, B. (1986). Why isn’t everyone a Bayesian? The American Statistician, 40(1), 1-5.\nEfron, B. (1998). R. A. Fisher in the 21st century. Statistical Science, 95-114."
  },
  {
    "objectID": "slides/01-intro.html#what-is-probability",
    "href": "slides/01-intro.html#what-is-probability",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "What is probability?",
    "text": "What is probability?\n\nFrequentist: Probability is the limit of relative frequency as the experiment repeats infinitely.\nBayesian: Probability reflects one’s belief.\nHowever, it doesn’t matter how you interpret probability because\n\n\n\n\n\n\n\n\n\n\nImage source: https://www.lacan.upc.edu/admoreWeb/2018/05/all-models-are-wrong-but-some-are-useful-george-e-p-box/"
  },
  {
    "objectID": "slides/01-intro.html#the-theory-that-would-not-die",
    "href": "slides/01-intro.html#the-theory-that-would-not-die",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "The Theory That Would Not Die",
    "text": "The Theory That Would Not Die\n\n\nFor more interesting stories about Bayesian statistics, check\n\n\n\n\n\n\n\nThere is a talk at Google given by the author Sharon McGrayne.\nhttps://www.lesswrong.com/posts/RTt59BtFLqQbsSiqd/a-history-of-bayes-theorem"
  },
  {
    "objectID": "slides/01-intro.html#probability",
    "href": "slides/01-intro.html#probability",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Probability",
    "text": "Probability\n\nThe value \\(\\P(E)\\) is the probability of the occurrence of event \\(E\\).\nLet \\(\\Omega\\) be the sample space, i.e., the space containing all possible outcomes.\nA probability \\(\\P\\) on \\(\\Omega\\) satisfies:\n\n\\(\\P(\\Omega) = 1\\)\n\\(\\P(E) \\geq 0\\) for any \\(E \\subset \\Omega\\)\n(countable additivity) \\(\\P(\\cup_{i=1}^{\\infty} E_i) = \\sum_{i=1}^\\infty \\P(E_i)\\) for pairwise disjoint \\(E_i\\)’s"
  },
  {
    "objectID": "slides/01-intro.html#conditional-probability",
    "href": "slides/01-intro.html#conditional-probability",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Conditional Probability",
    "text": "Conditional Probability\n\n\nThe conditional probability of \\(A\\) given \\(B\\) is\n\n\\[\\P(A \\mid B) \\coloneqq \\frac{\\P(A \\cap B)}{\\P(B)}, \\quad \\text{if}\\;\\;\\P(B) &gt; 0.\\]\n\n\n\nOne-line proof of Bayes Theorem: If \\(\\P(A)\\) and \\(\\P(B)\\) are both non-zero,\n\n\\[\\P(A \\mid B) = \\frac{\\P(A \\cap B)}{\\P(B)} = \\frac{\\P(B \\mid A)\\P(A)}{\\P(B)}.\\]"
  },
  {
    "objectID": "slides/01-intro.html#random-variables",
    "href": "slides/01-intro.html#random-variables",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Random Variables",
    "text": "Random Variables\n\nA random variable is\n\na measurable function\nan outcome from a random experiment\n\nThe information about a random variable is given by it probability density function (pdf) or probability mass function (pmf).\npdf: \\(f(x) \\geq 0\\) and \\(\\int f(x) dx = 1\\)\npmf: \\(f(x) = \\P(X = x)\\) and \\(\\sum f(x) = 1\\)\nExpectation: \\(\\E(X) = \\int xf(x)dx\\) or \\(\\E(X) = \\sum x\\P(X = x)\\)\nVariance: \\(\\var(X) = \\E[(X - \\mu)^2] = \\E(X^2) - \\mu^2\\) where \\(\\mu = \\E(X)\\).\nCovariance: \\(\\cov(X, Y) = \\E[(X - \\mu_X)(Y - \\mu_Y)] = \\E(XY) - \\mu_X\\mu_Y\\)"
  },
  {
    "objectID": "slides/01-intro.html#independence",
    "href": "slides/01-intro.html#independence",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Independence",
    "text": "Independence\n\nIndependence of events: \\(\\P(A \\cap B) = \\P(A)\\P(B)\\)\nIndependent random variables:\n\n\\(\\Leftrightarrow\\) \\(f_{X,Y}(x, y) = f_X(x)f_Y(y)\\)\n\\(\\Rightarrow\\) \\(\\E(XY) = \\E(X) \\E(Y)\\) (the reverse is not true)\n\\(\\Leftrightarrow\\) \\(f_{X|Y}(x|y) = f_X(x)\\)\n\nConditional independence\n\n\\(X \\perp Y \\mid Z\\): given \\(Z\\), \\(X\\) and \\(Y\\) are independent\nEquivalently, \\(X \\mid Z\\) and \\(Y \\mid Z\\) are independent.\n\nRemark: (mutual) independence DOES NOT imply conditional independence\nExample: \\(X, Z \\iid \\text{Ber}(1/2)\\) and \\(Y = I(X \\neq Z)\\). Check that \\(X\\) and \\(Y\\) are independent, but they are not conditionally independent given \\(Z\\)."
  },
  {
    "objectID": "slides/01-intro.html#law-of-total-probabilityexpectationvariance",
    "href": "slides/01-intro.html#law-of-total-probabilityexpectationvariance",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Law of Total Probability/Expectation/Variance",
    "text": "Law of Total Probability/Expectation/Variance\n\n\nLaw of Total Probability: For \\(B_i \\cap B_j = \\emptyset\\) and \\(\\cup_{i=1}^k B_i = \\Omega\\),\n\n\\[\\P(A) = \\sum_{i=1}^k \\P(A \\mid B_i)\\P(B_i).\\]\n\n\n\nLaw of Total Expectation (tower property/double expectation):\n\n\\[\\E(X) = \\E_Y(\\E_{X|Y}(X|Y)).\\]\n\n\n\nLaw of Total Variance:\n\n\\[\\var(X) = \\var(\\E(X|Y)) + \\E(\\var(X|Y)).\\]"
  },
  {
    "objectID": "slides/01-intro.html#important-univariate-distributions",
    "href": "slides/01-intro.html#important-univariate-distributions",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Important univariate distributions",
    "text": "Important univariate distributions\n\n\nContinuous univariate distribution:\n\nNormal distribution (on \\(\\R\\))\nGamma distribution (on \\(\\R_+\\))\nBeta distribution (on \\([0, 1]\\))\n\nDiscrete univariate distribution:\n\nBinomial distribution (on \\(\\{0, 1, \\ldots, n\\}\\))\nPoisson distribution (on \\(\\{0, 1, 2, \\ldots\\}\\))\nNegative Binomial distribution (on \\(\\{0, 1, 2, \\ldots\\}\\))"
  },
  {
    "objectID": "slides/01-intro.html#normal-distribution",
    "href": "slides/01-intro.html#normal-distribution",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Normal Distribution",
    "text": "Normal Distribution\n\n\n\\(N(\\mu, \\sigma^2)\\), \\(\\mu \\in \\R\\), \\(\\sigma &gt; 0\\)\nDensity function:\n\n\\[f(x;\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right), \\quad x \\in \\R\\]\n\n\\(\\E(X) = \\mu\\) and \\(\\var(X) = \\sigma^2\\)"
  },
  {
    "objectID": "slides/01-intro.html#gamma-distribution",
    "href": "slides/01-intro.html#gamma-distribution",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Gamma Distribution",
    "text": "Gamma Distribution\n\n\n\\(\\text{Gamma}(\\alpha, \\beta)\\), \\(\\alpha, \\beta &gt; 0\\)\nDensity function:\n\n\\[f(x;\\alpha, \\beta) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}x^{\\alpha-1}\\exp(-\\beta x), \\quad x &gt; 0\\]\n\n\\(\\E(X) = \\frac{\\alpha}{\\beta}\\) and \\(\\var(X) = \\frac{\\alpha}{\\beta^2}\\).\nSpecial Case: Exponential distribution (\\(\\alpha = 1\\)) and \\(\\chi^2\\) distribution (\\(\\alpha = k/2\\) and \\(\\beta = \\frac{1}{2}\\), where \\(k\\) is the degree of freedom)"
  },
  {
    "objectID": "slides/01-intro.html#beta-distribution",
    "href": "slides/01-intro.html#beta-distribution",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Beta Distribution",
    "text": "Beta Distribution\n\n\n\\(\\text{Beta}(\\alpha, \\beta)\\), \\(\\alpha, \\beta &gt; 0\\)\nDensity function:\n\n\\[f(x;\\alpha, \\beta) = \\frac{1}{B(\\alpha, \\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}, \\quad 0 \\leq x \\leq 1\\]\n\n\\(B(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}\\) is the Beta function\n\\(\\E(X) = \\frac{\\alpha}{\\alpha+\\beta}\\)\nSpecial case: Uniform distribution (\\(\\alpha = \\beta = 1\\))"
  },
  {
    "objectID": "slides/01-intro.html#binomial-distribution",
    "href": "slides/01-intro.html#binomial-distribution",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\n\n\\(\\text{Bin}(n, p)\\), \\(n \\in \\mathbb{N}\\), \\(0 &lt; p &lt; 1\\)\nNumber of positive outcomes out of \\(n\\) binary trials\nMass function:\n\n\\[f(x; n, p) = \\choose{n}{x}p^x(1-p)^{n-x}, \\quad x = 0, 1,\\ldots, n\\]\n\n\\(\\E(X) = np\\) and \\(\\var(X) = np(1-p)\\)\nSpecial case: Bernoulli distribution (\\(n = 1\\))"
  },
  {
    "objectID": "slides/01-intro.html#poisson-distribution",
    "href": "slides/01-intro.html#poisson-distribution",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\n\n\n\\(\\text{Poi}(\\lambda)\\), \\(\\lambda &gt; 0\\)\nMass function:\n\n\\[f(x;\\lambda) = \\frac{e^{-\\lambda}\\lambda^x}{x!}, \\quad x = 0, 1, 2, \\ldots\\]\n\n\\(\\E(X) = \\var(X) = \\lambda\\)."
  },
  {
    "objectID": "slides/01-intro.html#negative-binomial-distribution",
    "href": "slides/01-intro.html#negative-binomial-distribution",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Negative Binomial Distribution",
    "text": "Negative Binomial Distribution\n\n\n\\(\\text{NB}(r, p)\\), \\(r = 1,2,\\ldots\\), \\(0 &lt; p &lt; 1\\)\nNumber of failures before the \\(r\\)th success\nMass function:\n\n\\[f(x;r,p) = \\choose{x+r-1}{x}p^r(1-p)^x, \\quad x = 0, 1, \\ldots\\]\n\n\\(\\E(X) = \\frac{r(1-p)}{p}\\) and \\(\\var(X) = \\frac{r(1-p)}{p^2}\\).\nSpecial case: Geometric distribution (\\(r=1\\), number of failures before the first success)"
  },
  {
    "objectID": "slides/01-intro.html#maximum-likelihood-estimation",
    "href": "slides/01-intro.html#maximum-likelihood-estimation",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Maximum Likelihood Estimation",
    "text": "Maximum Likelihood Estimation\n\nSuppose \\(X_1, \\ldots, X_n \\iid f(x|\\theta)\\).\nThe likelihood function is \\[L(\\theta) = \\prod_{i=1}^n f(x_i|\\theta).\\]\nThe MLE of \\(\\theta\\) is \\(\\hat{\\theta} = \\argmax_{\\theta}L(\\theta)\\).\nUnder some regularity conditions on \\(f(x|\\theta)\\), the MLE is efficient (has the smallest variance) and its distribution is approximately normal (when \\(n\\) is large enough)."
  },
  {
    "objectID": "slides/01-intro.html#law-of-large-numbers-central-limit-theorem",
    "href": "slides/01-intro.html#law-of-large-numbers-central-limit-theorem",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Law of Large Numbers & Central Limit Theorem",
    "text": "Law of Large Numbers & Central Limit Theorem\nSuppose \\(X_1, \\ldots, X_n\\) are iid (independent and identically distributed) from some distribution \\(F\\) with \\(\\E(X) = \\mu\\) and \\(\\var(X) = \\sigma^2 &lt; \\infty\\). Let \\(\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i\\). Then\n\nLaw of Large Numbers (LLN): \\(\\bar{X}_n\\) will be very closed to \\(\\mu\\) for large \\(n\\) \\[ \\bar{X}_n \\cas \\mu\\]\nCentral Limit Theorem (CLT): the distribution of \\(\\bar{X}_n\\) will be approximately normal for large \\(n\\) \\[\\sqrt{n}(\\bar{X}_n - \\mu) \\cd N(0, \\sigma^2)\\]"
  },
  {
    "objectID": "slides/01-intro.html#monte-carlo-approximationestimation",
    "href": "slides/01-intro.html#monte-carlo-approximationestimation",
    "title": "Lecture 01: Course Introduction and Review",
    "section": "Monte Carlo Approximation/Estimation",
    "text": "Monte Carlo Approximation/Estimation\n\nSuppose we have function \\(f: [a, b] \\to \\R\\) and we want to compute \\(I = \\int_a^b f(x)dx\\).\nWrite \\[I = (b-a)\\int_a^b f(x)\\frac{1}{b-a}dx = (b-a)\\E(f(X)), \\quad X \\sim \\text{Unif}(a,b).\\]\nMonte Carlo approximation:\n\nGenerate \\(X_1, \\ldots, X_n \\iid \\text{Unif}(a,b)\\).\nCompute \\(\\hat{I}_n = \\frac{b-a}{n}\\sum_{i=1}^n f(X_i)\\).\nBy LLN and CLT, \\[\\hat{I}_n \\cas I \\quad \\text{and} \\quad \\sqrt{n}(\\hat{I}_n - I) \\cd N(0, (b-a)^2\\sigma^2)\\] if \\(\\sigma^2 = \\var(f(X)) &lt; \\infty\\).\n\n\n\n\nHome"
  },
  {
    "objectID": "slides/02-intro-bayes.html#beta-binomial-model-1",
    "href": "slides/02-intro-bayes.html#beta-binomial-model-1",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Beta-Binomial model",
    "text": "Beta-Binomial model\n\nLet \\(X_1, \\ldots, X_n \\mid p \\iid \\ber(p)\\).\nConsider the prior \\(p \\sim \\text{Beta}(\\alpha, \\beta)\\) where \\(\\alpha\\) and \\(\\beta\\) are known.\nThe posterior distribution of \\(p\\) given \\(X_1, \\ldots, X_n\\) is \\[\np \\mid X_1, \\ldots, X_n \\sim \\text{Beta}\\left(\\alpha + \\sum_{i=1}^n X_i, \\beta + n - \\sum_{i=1}^n X_i \\right).\n\\]"
  },
  {
    "objectID": "slides/02-intro-bayes.html#derivation",
    "href": "slides/02-intro-bayes.html#derivation",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Derivation",
    "text": "Derivation\n\nBayes Theorem: \\[\\begin{align*}\n\\pi(p \\mid x) & = \\frac{f(x \\mid p) \\pi(p)}{\\int f(x \\mid p) \\pi(p) dp} = \\frac{\\text{likelihood} \\times \\text{prior}}{\\text{marginal}}\\\\\n& \\propto f(x \\mid p) \\pi(p) = \\text{likelihood} \\times \\text{prior}\n\\end{align*}\\]\nThe marginal (and other normalizing constants) can be ignored.\nThe likelihood is \\[\nf(x_1, \\ldots, x_n \\mid p) = \\prod_{i=1}^n p^{x_i}(1-p)^{1-x_i} = p^{\\sum_{i=1}^n x_i}(1-p)^{n - \\sum_{i=1}^n x_i}.\n\\]\nThe prior is \\[\n\\pi(p) = \\frac{1}{B(\\alpha, \\beta)} p^{\\alpha-1} (1-p)^{\\beta-1} \\propto p^{\\alpha-1}(1-p)^{\\beta - 1}.\n\\]"
  },
  {
    "objectID": "slides/02-intro-bayes.html#derivation-1",
    "href": "slides/02-intro-bayes.html#derivation-1",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Derivation",
    "text": "Derivation\n\nHence the posterior is \\[\\begin{align*}\n\\pi(p \\mid X_1, \\ldots, X_n) & \\propto p^{\\sum_{i=1}^n x_i}(1-p)^{n - \\sum_{i=1}^n x_i} \\times p^{\\alpha-1}(1-p)^{\\beta - 1}\\\\\n& = p^{\\alpha + \\sum_{i=1}^n x_i - 1}(1-p)^{\\beta + n - \\sum_{i=1}^n x_i - 1}.\n\\end{align*}\\]\nRecognizing that this is the kernel of a Beta distribution, the posterior is \\[\np \\mid X_1, \\ldots, X_n \\sim \\text{Beta}\\left(\\alpha + \\sum_{i=1}^n X_i, \\beta + n - \\sum_{i=1}^n X_i \\right).\n\\]\nIt’s called Beta-Binomial model since the posterior only depends on \\(\\sum X_i\\) and the distribution of \\(\\sum X_i\\) is \\(\\bin(n,p)\\)."
  },
  {
    "objectID": "slides/02-intro-bayes.html#kernel-of-a-pdfpmf",
    "href": "slides/02-intro-bayes.html#kernel-of-a-pdfpmf",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Kernel of a pdf/pmf",
    "text": "Kernel of a pdf/pmf\n\nThe kernel is the form of the pdf or pmf in which any factors that are not functions of any of the variables in the domain are omitted.\nExamples:\n\n\nNormal: \\(\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\\)\nGamma: \\(x^{\\alpha-1}\\exp(-\\beta x)\\)\nBeta: \\(x^{\\alpha-1}(1-x)^{\\beta-1}\\)\nPoisson: \\(\\frac{\\lambda^x}{x!}\\)\n\n\nWe can use only the kernels to simplify the computation."
  },
  {
    "objectID": "slides/02-intro-bayes.html#posterior-distribution",
    "href": "slides/02-intro-bayes.html#posterior-distribution",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Posterior Distribution",
    "text": "Posterior Distribution\n\nset.seed(2023)\nn &lt;- 15\np &lt;- 0.3\nX &lt;- rbinom(n, 1, p)\ns &lt;- sum(X)\np_mle &lt;- s/n # MLE\nprint(X)\n\n [1] 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1\n\n\n\nlibrary(ggplot2)\nggplot(data = data.frame(p = c(0, 1)), aes(p)) + \n    lims(x = c(0, 1), y = c(0, 5)) + \n    labs(x = \"p\", y = \"Density\") +\n    geom_function(fun = dunif, aes(col = \"blue\")) + \n    geom_function(fun = dbeta, aes(col = \"red\"), \n                  args = list(shape1 = s + 1, shape2 = n - s + 1)) + \n    geom_vline(xintercept = p_mle, linetype = \"dashed\", col = \"darkgrey\") + \n    geom_vline(xintercept = p, linetype = \"dashed\", col = \"darkgreen\") + \n    scale_colour_manual(name = \"Distribution\", \n                        values = c(\"blue\", \"red\"),\n                        labels = c(\"Prior\", \"Posterior\"))"
  },
  {
    "objectID": "slides/02-intro-bayes.html#posterior-distribution-output",
    "href": "slides/02-intro-bayes.html#posterior-distribution-output",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Posterior Distribution",
    "text": "Posterior Distribution"
  },
  {
    "objectID": "slides/02-intro-bayes.html#posterior-inference",
    "href": "slides/02-intro-bayes.html#posterior-inference",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Posterior Inference",
    "text": "Posterior Inference\n\nThe posterior distribution of \\(p\\) contains much more information than the MLE.\nWe can use the posterior for:\n\nEstimation: \\(\\hat{p} = \\E(p \\mid X_1, \\ldots, X_n)\\) (posterior mean)\nPrediction: \\(\\P(X_{n+1} = 1 \\mid X_1, \\ldots, X_n)\\)\nInterval estimation: Find \\((L, U)\\) such that \\(\\P(L \\leq p \\leq U \\mid X_1, \\ldots, X_n) = 0.95\\)\n\nIn the Beta-Binomial model, an estimate for \\(p\\) is \\[\n\\hat{p} = \\E(p \\mid X_1, \\ldots, X_n) = \\frac{\\alpha + \\sum_{i=1}^n X_i}{\\alpha + \\beta + n}\n\\] whereas the MLE is \\(\\hat{p}_{\\text{MLE}} = \\frac{1}{n}\\sum_{i=1}^nX_i\\)."
  },
  {
    "objectID": "slides/02-intro-bayes.html#maximum-a-posteriori-estimate",
    "href": "slides/02-intro-bayes.html#maximum-a-posteriori-estimate",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Maximum-a-posteriori Estimate",
    "text": "Maximum-a-posteriori Estimate\n\nThe posterior mean is not the only estimate we can obtain from the posterior.\nAnother commonly used estimator is the maximum-a-posterior (MAP) estimate \\[\n\\hat{p}_{\\text{MAP}} = \\argmax_{0 &lt; p &lt; 1} \\pi(p \\mid X_1, \\ldots, X_n).\n\\]\nSince the mode of \\(\\text{Beta}(\\alpha, \\beta)\\) is \\(\\frac{\\alpha-1}{\\alpha+\\beta-2}\\) when \\(\\alpha, \\beta &gt; 1\\), \\[\n\\hat{p}_{\\text{MAP}} = \\frac{\\alpha -1 + \\sum_{i=1}^n X_i}{\\alpha + \\beta + n - 2}.\n\\]\nIf \\(\\alpha = \\beta = 1\\) and \\(\\sum X_i &gt; 1\\), then \\(\\hat{p}_{\\text{MAP}} = \\hat{p}_{\\text{MLE}} = \\bar{X}\\)."
  },
  {
    "objectID": "slides/02-intro-bayes.html#recall-laplaces-rule-of-succession",
    "href": "slides/02-intro-bayes.html#recall-laplaces-rule-of-succession",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Recall: Laplace’s Rule of Succession",
    "text": "Recall: Laplace’s Rule of Succession\nGiven binary iid random variables \\(X_1, \\ldots, X_n\\) with \\(\\sum_{i=1}^n X_i = s\\), then \\[\\P\\left(X_{n+1}=1 \\mid X_1+\\cdots+X_n=s\\right)=\\frac{s+1}{n+2}.\\]\nDerivation:\n\nLet \\(X_1, \\ldots, X_n \\iid \\ber(p)\\) and \\(p \\sim \\text{Beta}(1,1)\\) (the uniform prior).\nThe posterior is \\(p \\mid X_1 + \\cdots + X_n = s \\sim \\text{Beta}(s + 1, n-s + 1)\\).\nAssuming \\(X_{n+1} \\sim \\ber(p)\\) and \\(X_i\\)’s are iid conditioned on \\(p\\), \\[\\begin{align*}\n\\P\\left(X_{n+1}=1 \\mid \\sum X_i=s\\right)\n& = \\int \\P(X_{n+1}=1 \\mid p)\\pi\\left(p \\mid \\sum X_i = s\\right) dp\\\\\n& = \\int p \\pi\\left(p \\mid \\sum X_i = s\\right) dp\n= \\frac{s+1}{n+2}.\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/02-intro-bayes.html#interval-estimation-credible-intervalregion",
    "href": "slides/02-intro-bayes.html#interval-estimation-credible-intervalregion",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Interval Estimation: Credible Interval/Region",
    "text": "Interval Estimation: Credible Interval/Region\n\nFor frequentists, it’s called confidence interval/region.\nLet \\([L(X), U(X)]\\) be an interval for \\(\\theta\\) based on sample \\(X\\).\n\\(100\\times(1-\\alpha)\\%\\) Bayesian Coverage: \\(\\P(L(x) \\leq \\theta \\leq U(x) \\mid {\\color{magenta}X = x}) = 1-\\alpha\\).\n\ndescribes your information about the location of the true value of \\(\\theta\\) after you have observed \\(X = x\\)\n\n\\(100\\times(1-\\alpha)\\%\\) Frequentist Coverage: \\(\\P(L(X) \\leq \\theta \\leq U(X) \\mid {\\color{magenta}\\theta}) = 1-\\alpha\\)\n\ndescribes the probability that the interval will cover the true value before the data are observed\n\nThere are many ways to construct a credible interval:\n\nQuantile-based method\nHighest posterior density (HPD) region"
  },
  {
    "objectID": "slides/02-intro-bayes.html#quantile-based-method",
    "href": "slides/02-intro-bayes.html#quantile-based-method",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Quantile-based method",
    "text": "Quantile-based method\nTo find a \\(100\\times(1-\\alpha)\\%\\) credible interval for \\(\\theta\\):\n\nFind numbers \\(\\theta_{\\alpha / 2} &lt; \\theta_{1-\\alpha / 2}\\) such that\n\n\n\\(\\P\\left(\\theta&lt;\\theta_\\alpha \\mid X = x\\right)=\\alpha / 2\\);\n\\(\\P\\left(\\theta&gt;\\theta_{1-\\alpha / 2} \\mid X = x\\right)=\\alpha / 2\\).\n\n\nThe numbers \\(\\theta_{\\alpha / 2}, \\theta_{1-\\alpha / 2}\\) are the \\(\\alpha / 2\\) and \\(1-\\alpha / 2\\) posterior quantiles of \\(\\theta\\), and so \\[\\begin{align*}\n\\P\\left(\\theta \\in\\left[\\theta_{\\alpha / 2}, \\theta_{1-\\alpha / 2}\\right] \\mid X = x\\right) & =1-\\P\\left(\\theta \\notin\\left[\\theta_{\\alpha / 2}, \\theta_{1-\\alpha / 2}\\right] \\mid X = x\\right) \\\\\n& =1-\\left[\\P\\left(\\theta&lt;\\theta_{\\alpha / 2} \\mid X=x\\right)\\right.\\\\\n& \\qquad \\left.+\\P\\left(\\theta&gt;\\theta_{1-\\alpha / 2} \\mid X = x\\right)\\right] \\\\\n& =1-\\alpha.\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/02-intro-bayes.html#binomial-example",
    "href": "slides/02-intro-bayes.html#binomial-example",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Binomial Example",
    "text": "Binomial Example\nSuppose we observed \\(X=2\\) from a \\(\\bin(10, p)\\). Assume the uniform prior \\(p\\).\n\nalpha &lt;- 1;  beta &lt;- 1 # uniform prior\nn &lt;- 10; y &lt;- 2 # data\n\nqbeta(c(0.025, 0.975), alpha + y, beta + n - y)\n\n[1] 0.06022 0.51776"
  },
  {
    "objectID": "slides/02-intro-bayes.html#highest-posterior-density-hpd-region",
    "href": "slides/02-intro-bayes.html#highest-posterior-density-hpd-region",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Highest posterior density (HPD) region",
    "text": "Highest posterior density (HPD) region\n\nDefinition 1 A \\(100 \\times(1-\\alpha) \\%\\) HPD region consists of a subset of the parameter space, \\(s(x) \\subset \\Theta\\) such that\n\n\\(\\P(\\theta \\in s(x) \\mid X = x)=1-\\alpha\\);\nIf \\(\\theta_a \\in s(x)\\), and \\(\\theta_b \\notin s(x)\\), then \\(\\pi\\left(\\theta_a \\mid X=x\\right)&gt;\\pi\\left(\\theta_b \\mid X=x\\right)\\).\n\n\n\nAn HPD region might not be an interval if the posterior density is multimodal (having multiple peaks)."
  },
  {
    "objectID": "slides/02-intro-bayes.html#binomial-example-1",
    "href": "slides/02-intro-bayes.html#binomial-example-1",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Binomial Example",
    "text": "Binomial Example\nLet’s compute the HPD for the previous example: \\(\\theta \\mid X = 2 \\sim \\text{Beta}(3, 9)\\).\n\nlibrary(HDInterval)\nhdi(qbeta, 0.95, shape1 = 3, shape2=9)\n\n  lower   upper \n0.04056 0.48372 \nattr(,\"credMass\")\n[1] 0.95\n\n\nThe HPD is narrower than the quantile-based interval.\n\n\n\n\nFig. 3.6 in Hoff’s book"
  },
  {
    "objectID": "slides/02-intro-bayes.html#wrap-up-for-the-beta-binomial-model",
    "href": "slides/02-intro-bayes.html#wrap-up-for-the-beta-binomial-model",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Wrap-up for the Beta-Binomial model",
    "text": "Wrap-up for the Beta-Binomial model\n\nThe Beta-Binomial model: \\[\\begin{align*}\nX \\mid p & \\sim \\bin(n,p)\\\\\np & \\sim \\text{Beta}(\\alpha,\\beta)\\\\\np \\mid X & \\sim \\text{Beta}(\\alpha + X, \\beta + n - X)\n\\end{align*}\\]\nNote that the posterior is in the same family of the prior: both of them are in the Beta family.\nIn this case, the Beta prior is called a conjugate prior for the Binomial model."
  },
  {
    "objectID": "slides/02-intro-bayes.html#conjugate-prior-1",
    "href": "slides/02-intro-bayes.html#conjugate-prior-1",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Conjugate prior",
    "text": "Conjugate prior\n\nDefinition 2 A class \\(\\mc{P}\\) of prior distributions for \\(\\theta\\) is called conjugate for a sampling model \\(p(x \\mid \\theta)\\) if \\[\\begin{align*}\n\\pi(\\theta) \\in \\mc{P} \\Rightarrow \\pi(\\theta \\mid x) \\in \\mc{P} .\n\\end{align*}\\]\n\n\nUsually, the class \\(\\mc{P}\\) is a parametric family with a finite number of parameters.\nExample: Beta is conjugate for the binomial model.\nConjugate priors make posterior calculations easy, but might not actually represent our prior information.\nHowever, mixtures of conjugate prior distributions are very flexible and are computationally tractable."
  },
  {
    "objectID": "slides/02-intro-bayes.html#catalog-for-conjugate-priors",
    "href": "slides/02-intro-bayes.html#catalog-for-conjugate-priors",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Catalog for conjugate priors",
    "text": "Catalog for conjugate priors\n\n\n\n\n\n\n\n\n\n\n\n\n\nSampling model\nParameter\nPrior\nPosterior\n\n\n\n\n\\(X \\sim \\bin(n, p)\\)\n\\(p\\)\n\\(\\text{Beta}(\\alpha_0, \\beta_0)\\)\n\\(\\text{Beta}(\\alpha_0 + X, \\beta_0+n-X)\\)\n\n\n\\(X \\sim N(\\mu, \\sigma^2)\\)\n\\(\\mu\\)\n\\(N(\\mu_0, \\sigma_0^2)\\)\n\\(N\\left(\\frac{1}{\\frac{1}{\\sigma_0^2}+\\frac{1}{\\sigma^2}}\\left(\\frac{\\mu_0}{\\sigma_0^2}+\\frac{ X}{\\sigma^2}\\right),\\left(\\frac{1}{\\sigma_0^2}+\\frac{1}{\\sigma^2}\\right)^{-1}\\right)\\)\n\n\n\\(X \\sim \\text{Poisson}(\\lambda)\\)\n\\(\\lambda\\)\n\\(\\text{Gamma}(\\alpha_0, \\beta_0)\\)\n\\(\\text{Gamma}(\\alpha_0 + X, \\beta+1)\\)\n\n\n\\(X \\sim \\text{Gamma}(\\alpha, \\beta)\\)\n\\(\\beta\\)\n\\(\\text{Gamma}(\\alpha_0, \\beta_0)\\)\n\\(\\text{Gamma}(\\alpha_0 + \\alpha, \\beta_0+X)\\)\n\n\n\\(X \\sim \\text{NB}(r, p)\\)\n\\(p\\)\n\\(\\text{Beta}(\\alpha_0, \\beta_0)\\)\n\\(\\text{Beta}(\\alpha_0 + r, \\beta_0+X)\\)\n\n\n\n\nMore can be found on Wiki\nExercise: Derive the posterior for the Normal-Normal model. You should do this once in your life and then you are allowed to copy the formula from Wiki."
  },
  {
    "objectID": "slides/02-intro-bayes.html#exponential-families-and-conjugate-priors",
    "href": "slides/02-intro-bayes.html#exponential-families-and-conjugate-priors",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Exponential families and conjugate priors",
    "text": "Exponential families and conjugate priors"
  },
  {
    "objectID": "slides/02-intro-bayes.html#pros-and-cons",
    "href": "slides/02-intro-bayes.html#pros-and-cons",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Pros and cons",
    "text": "Pros and cons"
  },
  {
    "objectID": "slides/02-intro-bayes.html#real-data-example",
    "href": "slides/02-intro-bayes.html#real-data-example",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Real Data Example",
    "text": "Real Data Example\n\n\n2022年，基隆市男女嬰出生數分別為856及731，性別比例為1.1711（生下男嬰機率為0.5394）。\n同年，全台灣男女嬰出生數分別為71,208及66,205，性別比例為1.076（生下男嬰機率為0.5183）\n根據統計，全球人類自然出生性別比1.052（生下男嬰機率為0.5122）。\n基隆市自然出生性別比是否高於台灣平均？\n\n\n行政院資料庫Our World in Data"
  },
  {
    "objectID": "slides/02-intro-bayes.html#real-data-example-1",
    "href": "slides/02-intro-bayes.html#real-data-example-1",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Real Data Example",
    "text": "Real Data Example\n\nlibrary(tidyverse)\nlibrary(knitr)\nm &lt;- 856; f &lt;- 731\nprior_mean &lt;- c(0.5, 0.5122, 0.5122, 0.5122, 0.5122, 0.5122)\na_b &lt;- c(2, 2, 10, 100, 1000, 10000)\nalpha &lt;- prior_mean*a_b\nbeta &lt;- a_b - alpha\ndata.frame(alpha, beta, prior_mean) |&gt; \n    mutate(a_b = alpha + beta,\n           post_mean = (alpha+m)/(alpha+beta+m+f),\n           ratio = post_mean/(1-post_mean),\n           post_int = paste0(\"[\", round(qbeta(0.025, alpha + m, beta + f), 3),\n                             \", \", round(qbeta(0.975, alpha + m, beta + f), 3), \"]\")) |&gt;\n    select(c(prior_mean, a_b, post_mean, ratio, post_int)) |&gt;\n    kable(format = \"markdown\", digits = 4,\n          col.names = c(\"Prior mean $\\\\frac{\\\\alpha}{\\\\alpha+\\\\beta}$\",\n                        \"$\\\\alpha + \\\\beta$\",\n                        \"Post. mean\", \"Gender ratio\",\n                        \"Post. 95% Interval\"))"
  },
  {
    "objectID": "slides/02-intro-bayes.html#real-data-example-1-output",
    "href": "slides/02-intro-bayes.html#real-data-example-1-output",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Real Data Example",
    "text": "Real Data Example\n\n\n\n\n\n\n\n\n\n\n\nPrior mean \\(\\frac{\\alpha}{\\alpha+\\beta}\\)\n\\(\\alpha + \\beta\\)\nPost. mean\nGender ratio\nPost. 95% Interval\n\n\n\n\n0.5000\n2\n0.5393\n1.171\n[0.515, 0.564]\n\n\n0.5122\n2\n0.5393\n1.171\n[0.515, 0.564]\n\n\n0.5122\n10\n0.5392\n1.170\n[0.515, 0.564]\n\n\n0.5122\n100\n0.5378\n1.163\n[0.514, 0.562]\n\n\n0.5122\n1000\n0.5289\n1.123\n[0.51, 0.548]\n\n\n0.5122\n10000\n0.5159\n1.066\n[0.507, 0.525]"
  },
  {
    "objectID": "slides/02-intro-bayes.html#some-follow-up-questions",
    "href": "slides/02-intro-bayes.html#some-follow-up-questions",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Some follow-up questions",
    "text": "Some follow-up questions\n\nIs the sample size (856+731=1587) large enough?\nWhich prior? Depend on how strong your prior belief is.\n\nNo prior knowledge \\(\\Rightarrow\\) Uniform prior\nReliable prior information \\(\\Rightarrow\\) prior with larger \\(\\alpha+\\beta\\)\n\nWith prior information, we can obtain better estimates when the sample size is small:\n\nFor example, think of a really small village in which only two boys and one girl were born last year.\nIf we don’t use any prior information, the gender ratio is 2.\n\nWhat if our prior information cannot be described by a Beta distribution?\n\nEx: our prior information indicates a multimodal distribution\nThe posterior will be complicated and we need some other methods for posterior inference."
  },
  {
    "objectID": "slides/02-intro-bayes.html#what-is-good-and-bad-about-bayesian",
    "href": "slides/02-intro-bayes.html#what-is-good-and-bad-about-bayesian",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "What is good and bad about Bayesian?",
    "text": "What is good and bad about Bayesian?\n\nGood:\n\nconsistent and coherent: everything (sample or parameter) is a random variable\neverything is conditional: we only care about conditional independence rather than marginal independence\nstopping rule does not matter\n\nBad:\n\nthe choice of prior is subjective\ncomputationally challenging"
  },
  {
    "objectID": "slides/02-intro-bayes.html#exchangeability",
    "href": "slides/02-intro-bayes.html#exchangeability",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Exchangeability",
    "text": "Exchangeability\n\nDefinition 3 Let \\(p\\left(x_1, \\ldots, x_n\\right)\\) be the joint density of \\(X_1\\), \\(\\ldots, X_n\\). If \\(p\\left(x_1, \\ldots, x_n\\right)=p\\left(x_{\\pi(1)}, \\ldots, x_{\\pi(n)}\\right)\\) for all permutations \\(\\pi\\) of \\(\\{1, \\ldots, n\\}\\), then \\(X_1, \\ldots, X_n\\) are exchangeable.\n\n\nRoughly speaking, \\(X_1, \\ldots, X_n\\) are exchangeable if the subscript labels convey no information about the outcomes.\nApparently, independence implies exchangeability, but the converse is false.\nWhat is the relationship between conditional independence and exchangeability?"
  },
  {
    "objectID": "slides/02-intro-bayes.html#conditional-independence-and-exchangeability",
    "href": "slides/02-intro-bayes.html#conditional-independence-and-exchangeability",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Conditional independence and exchangeability",
    "text": "Conditional independence and exchangeability\n\nProposition 1 If \\(\\theta \\sim p(\\theta)\\) and \\(X_1, \\ldots, X_n\\) are conditionally i.i.d. given \\(\\theta\\), then marginally (unconditionally on \\(\\theta\\) ) \\(, X_1, \\ldots, X_n\\) are exchangeable.\n\n\nProof. Suppose \\(X_1, \\ldots, X_n\\) are conditionally iid given some unknown parameter \\(\\theta\\). Then for any permutation \\(\\pi\\) of \\(\\{1, \\ldots, n\\}\\) and any set of values \\(\\left(x_1, \\ldots, x_n\\right) \\in\\) \\(\\mc{X}^n\\) \\[\\begin{align*}\np\\left(x_1, \\ldots, x_n\\right) & =\\int p\\left(x_1, \\ldots, x_n \\mid \\theta\\right) p(\\theta) d \\theta & & \\text { (definition of marginal probability) } \\\\\n& =\\int\\left\\{\\prod_{i=1}^n p\\left(x_i \\mid \\theta\\right)\\right\\} p(\\theta) d \\theta & & \\text { ($X$'s are conditionally i.i.d.) } \\\\\n& =\\int\\left\\{\\prod_{i=1}^n p\\left(x_{\\pi(i)} \\mid \\theta\\right)\\right\\} p(\\theta) d \\theta & & \\text { (product does not depend on order) } \\\\\n& =p\\left(x_{\\pi(1)}, \\ldots x_{\\pi(n)}\\right) & & \\text { (definition of marginal probability) } .\n\\end{align*}\\]\n\n\n\nCh. 2.7 & 2.8 in Hoff’s book."
  },
  {
    "objectID": "slides/02-intro-bayes.html#de-finettis-theorem",
    "href": "slides/02-intro-bayes.html#de-finettis-theorem",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "de Finetti’s Theorem",
    "text": "de Finetti’s Theorem\n\nWe have seen that\n\n\n\\[\\begin{align*}\n\\left.\\begin{array}{l}\nX_1, \\ldots, X_n \\mid \\theta \\text { i.i.d } \\\\\n\\theta \\sim p(\\theta)\n\\end{array}\\right\\} \\Rightarrow X_1, \\ldots, X_n \\text { are exchangeable. }\n\\end{align*}\\]\n\n\nWhat about an arrow in the other direction?\n\n\nTheorem 1 (de Finetti) Let \\(X_i \\in \\mc{X}\\) for all \\(i \\in\\{1,2, \\ldots\\}\\). Suppose that, for any \\(n\\), \\(X_1, \\ldots, X_n\\) are exchangeable. Then our model can be written as \\[\\begin{align*}\np\\left(x_1, \\ldots, x_n\\right)=\\int\\left\\{\\prod_{i=1}^n p\\left(x_i \\mid \\theta\\right)\\right\\} p(\\theta) d \\theta\n\\end{align*}\\] for some parameter \\(\\theta\\), some prior distribution on \\(\\theta\\) and some sampling model \\(p(x \\mid \\theta)\\). The prior and sampling model depend on \\(p\\left(x_1, \\ldots, x_n\\right)\\)."
  },
  {
    "objectID": "slides/02-intro-bayes.html#de-finettis-theorem-1",
    "href": "slides/02-intro-bayes.html#de-finettis-theorem-1",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "de Finetti’s Theorem",
    "text": "de Finetti’s Theorem\n\n\nThe conclusion is\n\n\\[\\begin{align*}\n\\left.\\begin{array}{l}\nX_1, \\ldots, X_n \\mid \\theta \\text { are i.i.d. } \\\\\n\\theta \\sim p(\\theta)\n\\end{array}\\right\\} \\Leftrightarrow X_1, \\ldots, X_n \\text { are exchangeable for all } n \\text {. }\n\\end{align*}\\]\n\n\nThis justifies the use of prior distributions when samples are exchangeable.\nWhen is the condition “\\(X_1, \\ldots, X_n\\) are exchangeable for all \\(n\\)” reasonable?\n\n\\(X_1, \\ldots, X_n\\) are outcomes of a repeatable experiment;\n\\(X_1, \\ldots, X_n\\) are sampled from a finite population with replacement;\n\\(X_1, \\ldots, X_n\\) are sampled from an infinite population without replacement."
  },
  {
    "objectID": "slides/02-intro-bayes.html#stopping-rule",
    "href": "slides/02-intro-bayes.html#stopping-rule",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Stopping Rule",
    "text": "Stopping Rule\nLet \\(\\theta\\) be the probability of a particular coin landing on heads, and suppose we want to test the hypotheses \\[\\begin{align*}\nH_0: \\theta=1 / 2, \\quad H_1: \\theta&gt;1 / 2\n\\end{align*}\\] at a significance level of \\(\\alpha=0.05\\). Suppose we observe the following sequence of flips: \\[\n\\text{heads, heads, heads, heads, heads, tails (5 heads, 1 tails)}\n\\]\n\nTo perform a frequentist hypothesis test, we must define a random variable to describe the data.\nThe proper way to do this depends on exactly which of the following two experiments was actually performed:\n\n\n\nExample 1.1 in Essential Bayes"
  },
  {
    "objectID": "slides/02-intro-bayes.html#stopping-rule-1",
    "href": "slides/02-intro-bayes.html#stopping-rule-1",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Stopping Rule",
    "text": "Stopping Rule\nSuppose the experiment is “Flip six times and record the results.”\n\n\\(X\\) counts the number of heads, and \\(X \\sim \\bin(6, \\theta)\\).\nThe observed data was \\(x=5\\), and the \\(p\\)-value of our hypothesis test is \\[\\begin{align*}\np\\text{-value} & =\\P_{\\theta=1 / 2}(X \\geq 5) \\\\\n& =\\P_{\\theta=1 / 2}(X=5)+\\P_{\\theta=1 / 2}(X=6) \\\\\n& =\\frac{6}{64}+\\frac{1}{64}=\\frac{7}{64}=0.109375&gt;0.05 .\n\\end{align*}\\] So we fail to reject \\(H_0\\) at \\(\\alpha=0.05\\)."
  },
  {
    "objectID": "slides/02-intro-bayes.html#stopping-rule-2",
    "href": "slides/02-intro-bayes.html#stopping-rule-2",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Stopping Rule",
    "text": "Stopping Rule\nSuppose now the experiment is “Flip until we get tails.”\n\n\\(X\\) counts the number of the flip on which the first tails occurs, and \\(X \\sim \\text{Geometric}(1-\\theta)\\).\nThe observed data was \\(x=6\\), and the p-value of our hypothesis test is \\[\\begin{align*}\np \\text{-value} & = \\P_{\\theta=1 / 2}(X \\geq 6) \\\\\n& =1-\\P_{\\theta=1 / 2}(X&lt;6) \\\\\n& =1-\\sum_{x=1}^5 \\P_{\\theta=1 / 2}(X=x) \\\\\n& =1-\\left(\\frac{1}{2}+\\frac{1}{4}+\\frac{1}{8}+\\frac{1}{16}+\\frac{1}{32}\\right)=\\frac{1}{32}=0.03125&lt;0.05\n\\end{align*}\\] So we reject \\(H_0\\) at \\(\\alpha=0.05\\)."
  },
  {
    "objectID": "slides/02-intro-bayes.html#stopping-rule-3",
    "href": "slides/02-intro-bayes.html#stopping-rule-3",
    "title": "Lecture 02: Introduction to Bayesian Statistics",
    "section": "Stopping Rule",
    "text": "Stopping Rule\n\nThe result our hypothesis test depends on whether we would have stopped flipping if we had gotten a tails sooner.\nThe tests are dependent on what we call the stopping rule.\nThe likelihood for the actual value of \\(x\\) that was observed is the same for both experiments (up to a constant): \\[\\begin{align*}\np(x \\mid \\theta) \\propto \\theta^5(1-\\theta) .\n\\end{align*}\\]\nA Bayesian approach would take the data into account only through this likelihood.\nHomework: Show that under the uniform prior, the posteriors under the two stopping rules are the same.\n\n\n\nHome"
  }
]