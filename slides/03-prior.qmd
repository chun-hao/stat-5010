---
title: "Lecture 03: Prior Information and Prior Distribution"
author: "Chun-Hao Yang"
footer: "[Home](https://chun-hao.github.io/stat-5010/)"
format: 
    revealjs:
        theme: slides.scss
        chalkboard: true
        slide-number: true
        html-math-method: mathjax
        incremental: true
execute:
    echo: false
    warning: false
    cache: true
    freeze: auto
---

## Introduction
{{< include macro.qmd >}}

-   The choice of prior distribution is the most critical and difficult step in Bayesian analysis.
-   In practice, the available prior information is often not precise enough to lead to an exact determination of the prior distribution.
-   We need to find a prior distribution that is compatible with the prior information.
-   Another problem is the determination of prior when the prior information is too vague or does not even exist.

## Maximum Entropy Priors

-   If some characteristics of the prior distribution are known,
    assuming that they can be written as prior expectations, $$
    \E_{\pi}\left[g_k(\theta)\right]=\omega_k, \quad k = 1, \ldots, K,
    $$ a way to select a prior $\pi$ satisfying these constraints is the
    *maximum entropy method*.

-   Let $\pi$ be a discrete distribution. The Shannon Entropy[^1] of
    $\pi$ is $$
    \mc{E}(\pi) =  -\E_{\pi}[\log \pi(\theta)].
    $$

[^1]: Introduced by Shannon (1948).

## Intuition behind Shannon Entropy

-   Entropy is a measure of randomness:
    \color{red}{high entropy = high randomness}

-   Entropy is the expectation of information:
    $\mc{E}(\pi) = \E_{\pi}(I(X))$, where $I(x)$ is the *information* of
    the event $\{X = x\}$.

-   Roughly speaking,
    \textcolor{cyan}{information of an event $E$ is the knowledge you obtained after the occurrence of $E$}.

## Shannon's Axioms for Information

1.  An event that always happens yields no information.
2.  The less probable an event is, the more information it yields.
3.  If two independent events are measured separately, the total amount
    of information is the sum of the information of the individual
    events.

That is, the information $I(A)$ of an event $A$ satisfies

1.  $I(A) = 0$ if $\P(A) = 1$.
2.  $I(A)$ is a decreasing function of $\P(A)$.
3.  $I(A\cap B) = I(A) + I(B)$ if $A$ and $B$ are independent.

```{=tex}
\vspace{1cm}
\begin{center}
\textcolor{cyan}{\large Shannon's solution: $I(A) = -\log \P(A)$ for $\P(A) > 0$}
\end{center}
```
## Shannon Entropy

-   Let $X$ be a discrete random variable with probability mass function
    $\pi(x)$. The Shannon information of $\pi$ is $$
    I(x) \coloneqq I(\{X = x\}) = -\log \P(X = x) = -\log \pi(x).
    $$

-   The Shannon entropy of $\pi$ is
    $\mc{E}(\pi) = \E_{\pi}(I(X)) = -\E_{\pi}(\log \pi(X))$.

-   If the support of $\pi(x)$ is a finite set, say
    $\{x_1,\ldots, x_m\}$, then $\mc{E}(\pi) \leq \log m$ and the
    equality holds when $\pi(x) = \frac{1}{m}$.

-   What if $X$ is continuous? If $\pi(x)$ is a probability density, the
    information $-\log \pi(x)$ can be negative.

## Relative Entropy

-   It does not make sense to have a negative information.
-   But \`\`relative information" can be negative.
-   Let $\pi_0$ be a reference distribution. The **relative
    information** of $\pi$ with respect to $\pi_0$ is $$
    I(x; \pi_0) \coloneqq \log \frac{1}{\pi_0(x)} - \log \frac{1}{\pi(x)} = \log \frac{\pi(x)}{\pi_0(x)}
    $$
-   Then by Jensen's inequality $$
    \E_{\pi_0}[I(X;\pi_0)] = \mc{E}(\pi_0) - \mc{E}(\pi, \pi_0) \leq 0
    $$ and the equality holds when $\pi = \pi_0$.

## Relative Entropy

-   **Cross entropy**: $\mc{E}(\pi, \pi_0) = - \E_{\pi_0}(\log \pi(X))$

-   **Relative entropy**: $$
    \mc{E}(\pi_0\| \pi) = - \E_{\pi_0}[I(X;\pi_0)] = \E_{\pi_0}\left[\log \frac{\pi_0(x)}{\pi(x)}\right] = D_{KL}(\pi_0\| \pi) \geq 0
    $$

-   It is also called the **Kullback-Liebler divergence**.

-   A common choice of the reference distribution $\pi_0$ is
    non-informative distributions, e.g., the Lebesgue measure.

## Entropy/Information

Some take-home messages:

-   Entropy $\approx$ Information $\approx$ Uncertainty $\approx$ Energy
    $\approx$ Temparature
-   There are different definitions of entropy, e.g. RÃ©nyi entropy, and
    therefore different divergences can be induced.
-   Entropy plays the role of utility functions in decision theory: an
    inference procedure is obtained by maximizing the entropy.

## Maximum Entropy Prior (MEP)

-   The maximum entropy prior (MEP) (with respect to a reference
    distribution $\pi_0$) is the solution to the optimization problem
    \begin{equation}\label{eq:MEP}
    \begin{split}
    \max_{\pi \in \Gamma} & \; \mc{E}(\pi; \pi_0)\\
    & s.t. \quad \E_{\pi}\left[g_k(\theta)\right]=\omega_k, \quad k = 1, \ldots, K
    \end{split}
    \end{equation} where $\Gamma$ is a class of candidate priors.

-   The existence of the solution depends on $g_k$'s, $\Gamma$, and
    $\pi_0$.

-   The prior $\pi$ maximizing the entropy is, in this
    information-theoretic sense, minimizing the prior information
    brought through $\pi$ about $\theta$.

## Discrete MEP

-   Let $\Theta=\{\theta_1, \ldots, \theta_m\}$ and
    $\pi_0(\theta_i) = \frac{1}{m}$ be the reference distribution.
-   The prior informations are $$
    \E_{\pi}\left[g_k(\theta)\right]=\omega_k, \quad k = 1, \ldots, K.
    $$
-   The MEP for $\theta$ is $$
    \pi^*(\theta_i)=\frac{\exp \left\{\sum_{k=1}^K \lambda_k g_k\left(\theta_i\right)\right\}}{\sum_j \exp \left\{\sum_{k=1}^K \lambda_k g_k\left(\theta_j\right)\right\}}
    $$ where the $\lambda_k$'s are obtained from \eqref{eq:MEP} as
    Lagrange multipliers.
-   Here $\Gamma$ is the class of all discrete distributions on
    $\Theta$, i.e., $\Gamma = \{(p_1, \ldots, p_m): \sum p_i = 1\}$.
-   The proof is a simple application of Lagrange multipliers.

## Continuous MEP

-   Let $\Theta = \R$ and the prior information be $$
    \E_{\pi}\left[g_k(\theta)\right]=\omega_k, \quad k = 1, \ldots, K.
    $$
-   The MEP for $\theta$ is $$
    \pi^*(\theta)=\frac{\exp \left\{\sum_{k=1}^K \lambda_k g_k(\theta)\right\} \pi_0(\theta)}{\int \exp \left\{\sum_{k=1}^K \lambda_k g_k(\eta)\right\} \pi_0(d \eta)}.
    $$
-   ??

## Examples

-   Without any prior information, the MEP is the uniform distribution.
-   With $\E_{\pi}(\theta) = \mu$, the MEP is
    $\pi^*(\theta) \propto e^{\lambda\theta}$, which can not be
    normalized to 1.
-   With $\E_{\pi}(\theta) = \mu$ and $\var_{\pi}(\theta) = \sigma^2$,
    the MEP is $N(\mu, \sigma^2)$.

## Problems with MEP

## Parametric Approximations

-   A more practical solution to incorporate the prior information is to
    use parametric approximations.
-   Choose a parametric family, e.g. normal, and find one in the family
    that matches the prior information (exactly or approximately).
-   Example: Suppose the prior informations are: median = 0, lower
    quartile = -1, and upper quartile = 1.
-   The normal distribution satisfying these constraints is
    $N(0, 2.19)$.
-   The Cauchy distribution satisfying these constraints is
    $\text{Cauchy}(0,1)$.
-   Both are reasonbale prior distributions (depending on the likelihood
    and data).


## Conjugate prior

::: {#def-conjugacy}
A class $\mc{P}$ of prior distributions for $\theta$ is called
[conjugate]{.underline} for a sampling model $p(x \mid \theta)$ if
\begin{align*}
\pi(\theta) \in \mc{P} \Rightarrow \pi(\theta \mid x) \in \mc{P} .
\end{align*}
:::

-   Usually, the class $\mc{P}$ is a parametric family with a finite
    number of parameters.
-   Example: Beta is conjugate for the binomial model.
-   Conjugate priors make posterior calculations easy, but might not
    actually represent our prior information.
-   However, mixtures of conjugate prior distributions are very flexible
    and are computationally tractable.

## Exponential families

-   Recall that a family of distributions is an **exponential family** if its pdf/pmf can be written as 
\begin{align*}
f(x;\theta) & = h(x)\exp\left(\sum_{i=1}^k w_i(\theta)T_i(x) - \psi(\theta)\right)\\
& = h(x)\exp\left(\sum_{i=1}^k \eta_iT_i(x)-\tilde{\psi}(\eta)\right), \quad \eta_i = w_i(\theta), \eta = [\eta_1, \ldots, \eta_k].
\end{align*}
    +   $\eta$ is the **natural parameter**.
    +   $T(X) = [T_1(X), \ldots, T_k(X)]$ is a **sufficient statistic**; it is **complete** if the parameter space is an open set in $\R^k$.
-   Example: Normal, Gamma, Beta, Binomial (with known $n$), Poisson, ...
-   Not an exponential family: $\text{Unif}(\theta, \theta+1)$ (if the support of the distribution depends on the parameter, then it is not an exponential family).

## Constructing a conjugate prior

-   Suppose $f(x\mid \theta) = h(x)\exp(\theta^Tx - \psi(\theta))$ (natural parametrization).
-   How to construct a conjugate prior for $f(x \mid \theta)$?
-   We need $\pi(\theta)$ and $\pi(\theta \mid x)$ to be in the same family.

:::{.fragment}
$$
\begin{align}
  \pi(\theta \mid x) = \frac{f(x \mid \theta)\pi(\theta)}{\int f(x\mid \theta^{\prime})\pi(\theta^{\prime})d\theta^{\prime}} 
  & = \frac{{\color{green}\pi(\theta)}{\color{magenta}h(x)}\exp(\theta^Tx {\color{green} - \psi(\theta)})}{{\color{magenta}m(x)}}\\
    &\class{fragment}{{} = \exp\left[x^T\theta - \left(\psi(\theta)\right)\right]}         \\[3px]
\end{align}
$$
:::


## Pros and cons



## Some practical guides for choosing priors

With prior information: - Find the MEP - Find a (conjugate) parametric
prior that approximates the prior information

Without prior information: - Use conjugate priors (if applicable) and
hyper-priors - Use noninformative priors, e.g., Jeffreys prior,
reference priors, vague priors

Finally, perform some sensitivity analysis to access priors' influence
on the result.