---
title: "Lecture 03: From Prior Information to Prior Distribution"
author: "Chun-Hao Yang"
footer: "[Home](https://chun-hao.github.io/stat-5010/)"
format: 
    revealjs:
        theme: slides.scss
        chalkboard: true
        slide-number: true
        html-math-method: mathjax
        incremental: true
execute:
    echo: false
    warning: false
    cache: true
    freeze: auto
editor: 
  markdown: 
    wrap: 72
---

## Introduction

{{< include macro.qmd >}}

-   The choice of prior distribution is the most critical and difficult
    step in Bayesian analysis.
-   In practice, the available prior information is often not precise
    enough to lead to an exact determination of the prior distribution.
-   We need to find a prior distribution that is compatible with the
    prior information.
-   Another problem is the determination of prior when the prior
    information is too vague or does not even exist.



## Prior Information

-   Suppose the prior information for $\theta$ is given through some expectations.
    $$
    \E_{\pi}\left[g_k(\theta)\right]=\omega_k, \quad k = 1, \ldots, K,
    $$
-   For example, the census records show that the sex ratio at birth is around 1.05 (ranging from 1.03 to 1.07 for most countries).
-   The prior information about the sex ratio $\theta$ can be expressed as 
    $$
        \E_{\pi}(\theta) = 1.05 \quad \text{and} \quad \var_{\pi}(\theta) = 0.1^2.
    $$
-   Obviously, there are many distributions compatible with these constraints.
-   We will discuss two approaches for choosing prior distributions compatible with these information.


# Maximum Entropy Priors


## Entropy

-   Let $\pi$ be a discrete distribution. The Shannon Entropy[^1] of
    $\pi$ is $$
    \mc{E}(\pi) =  -\E_{\pi}[\log \pi(\theta)].
    $$

[^1]: Introduced by Shannon (1948).

## Intuition behind Shannon Entropy

-   Entropy is a measure of randomness:
    <span style="color:red;">high entropy = high randomness</span>

-   Entropy is the expectation of information:
    $\mc{E}(\pi) = \E_{\pi}(I(X))$, where $I(x)$ is the *information* of
    the event $\{X = x\}$.

-   Roughly speaking,
    <span style="color:red;">information of an event $E$ is the knowledge you obtained after the occurrence of $E$ </span>.

## Shannon's Axioms for Information

1.  An event that always happens yields no information.
2.  The less probable an event is, the more information it yields.
3.  If two independent events are measured separately, the total amount
    of information is the sum of the information of the individual
    events.

:::{.fragment}
That is, the information $I(A)$ of an event $A$ satisfies
:::

1.  $I(A) = 0$ if $\P(A) = 1$.
2.  $I(A)$ is a decreasing function of $\P(A)$.
3.  $I(A\cap B) = I(A) + I(B)$ if $A$ and $B$ are independent.

:::{.fragment style="text-align: center;"}
<span style="color:red;">
Shannon's solution: $I(A) = -\log \P(A)$ for $\P(A) > 0$
</span>
:::

## Shannon Entropy

-   Let $X$ be a discrete random variable with probability mass function
    $\pi(x)$. The Shannon information of $\pi$ is $$
    I(x) \coloneqq I(\{X = x\}) = -\log \P(X = x) = -\log \pi(x).
    $$

-   The Shannon entropy of $\pi$ is
    $\mc{E}(\pi) = \E_{\pi}(I(X)) = -\E_{\pi}(\log \pi(X))$.

-   If the support of $\pi(x)$ is a finite set, say
    $\{x_1,\ldots, x_m\}$, then $\mc{E}(\pi) \leq \log m$ and the
    equality holds when $\pi(x) = \frac{1}{m}$.

-   What if $X$ is continuous? If $\pi(x)$ is a probability density, the
    information $-\log \pi(x)$ can be negative.

## Relative Entropy

-   It does not make sense to have a negative information.
-   But \`\`relative information" can be negative.
-   Let $\pi_0$ be a reference distribution. The **relative
    information** of $\pi$ with respect to $\pi_0$ is $$
    I(x; \pi_0) \coloneqq \log \frac{1}{\pi_0(x)} - \log \frac{1}{\pi(x)} = \log \frac{\pi(x)}{\pi_0(x)}
    $$
-   Then by Jensen's inequality $$
    \E_{\pi_0}[I(X;\pi_0)] = \mc{E}(\pi_0) - \mc{E}(\pi, \pi_0) \leq 0
    $$ and the equality holds when $\pi = \pi_0$.

## Relative Entropy

-   **Cross entropy**: $\mc{E}(\pi, \pi_0) = - \E_{\pi_0}(\log \pi(X))$

-   **Relative entropy**: $$
    \mc{E}(\pi_0\| \pi) = - \E_{\pi_0}[I(X;\pi_0)] = \E_{\pi_0}\left[\log \frac{\pi_0(x)}{\pi(x)}\right] = D_{KL}(\pi_0\| \pi) \geq 0
    $$

-   It is also called the **Kullback-Liebler divergence**.

-   A common choice of the reference distribution $\pi_0$ is
    non-informative distributions, e.g., the Lebesgue measure.

## Entropy/Information

Some take-home messages:

-   Entropy $\approx$ Information $\approx$ Uncertainty $\approx$ Energy
    $\approx$ Temparature
-   There are different definitions of entropy, e.g. RÃ©nyi entropy, and
    therefore different divergences can be induced.
-   Entropy plays the role of utility functions in decision theory: an
    inference procedure is obtained by maximizing the entropy.

## Maximum Entropy Prior (MEP)

-   The maximum entropy prior (MEP) (with respect to a reference
    distribution $\pi_0$) is the solution to the optimization problem
    \begin{equation}\label{eq:MEP}
    \begin{split}
    \max_{\pi \in \Gamma} & \; \mc{E}(\pi; \pi_0)\\
    & s.t. \quad \E_{\pi}\left[g_k(\theta)\right]=\omega_k, \quad k = 1, \ldots, K
    \end{split}
    \end{equation} where $\Gamma$ is a class of candidate priors.

-   The existence of the solution depends on $g_k$'s, $\Gamma$, and
    $\pi_0$.

-   The prior $\pi$ maximizing the entropy is, in this
    information-theoretic sense, minimizing the prior information
    brought through $\pi$ about $\theta$.

## Discrete MEP

-   Let $\Theta=\{\theta_1, \ldots, \theta_m\}$ and
    $\pi_0(\theta_i) = \frac{1}{m}$ be the reference distribution.
-   The prior informations are $$
    \E_{\pi}\left[g_k(\theta)\right]=\omega_k, \quad k = 1, \ldots, K.
    $$
-   The MEP for $\theta$ is $$
    \pi^*(\theta_i)=\frac{\exp \left\{\sum_{k=1}^K \lambda_k g_k\left(\theta_i\right)\right\}}{\sum_j \exp \left\{\sum_{k=1}^K \lambda_k g_k\left(\theta_j\right)\right\}}
    $$ where the $\lambda_k$'s are obtained from \eqref{eq:MEP} as
    Lagrange multipliers.
-   Here $\Gamma$ is the class of all discrete distributions on
    $\Theta$, i.e., $\Gamma = \{(p_1, \ldots, p_m): \sum p_i = 1\}$.
-   The proof is a simple application of Lagrange multipliers.

## Continuous MEP

-   Let $\Theta = \R$ and the prior information be $$
    \E_{\pi}\left[g_k(\theta)\right]=\omega_k, \quad k = 1, \ldots, K.
    $$
-   The MEP for $\theta$ is $$
    \pi^*(\theta)=\frac{\exp \left\{\sum_{k=1}^K \lambda_k g_k(\theta)\right\} \pi_0(\theta)}{\int \exp \left\{\sum_{k=1}^K \lambda_k g_k(\eta)\right\} \pi_0(d \eta)}.
    $$
-   ??

## Examples

-   Without any prior information, the MEP is the uniform distribution.
-   With $\E_{\pi}(\theta) = \mu$, the MEP is
    $\pi^*(\theta) \propto e^{\lambda\theta}$, which can not be
    normalized to 1.
-   With $\E_{\pi}(\theta) = \mu$ and $\var_{\pi}(\theta) = \sigma^2$,
    the MEP is $N(\mu, \sigma^2)$.

## Problems with MEP

## Parametric Approximations

-   A more practical solution to incorporate the prior information is to
    use parametric approximations.
-   Choose a parametric family, e.g. normal, and find one in the family
    that matches the prior information (exactly or approximately).
-   Example: Suppose the prior informations are: median = 0, lower
    quartile = -1, and upper quartile = 1.
-   The normal distribution satisfying these constraints is
    $N(0, 2.19)$.
-   The Cauchy distribution satisfying these constraints is
    $\text{Cauchy}(0,1)$.
-   Both are reasonbale prior distributions (depending on the likelihood
    and data).


# Conjugate Prior

## Conjugate prior

::: {#def-conjugacy}
A class $\mc{P}$ of prior distributions for $\theta$ is called
[conjugate]{.underline} for a sampling model $p(x \mid \theta)$ if
\begin{align*}
\pi(\theta) \in \mc{P} \Rightarrow \pi(\theta \mid x) \in \mc{P} .
\end{align*}
:::

-   Usually, the class $\mc{P}$ is a parametric family with a finite
    number of parameters.
-   Example: Beta is conjugate for the binomial model.
-   Conjugate priors make posterior calculations easy, but might not
    actually represent our prior information.
-   However, mixtures of conjugate prior distributions are very flexible
    and are computationally tractable.

## Exponential families

-   Recall that a family of distributions is an **exponential family**
    if its pdf/pmf can be written as \begin{align*}
    f(x;\theta) & = h(x)\exp\left(\sum_{i=1}^k w_i(\theta)T_i(x) - \psi(\theta)\right)\\
    & = h(x)\exp\left(\sum_{i=1}^k \eta_iT_i(x)-\tilde{\psi}(\eta)\right), \quad \eta_i = w_i(\theta), \eta = [\eta_1, \ldots, \eta_k].
    \end{align*}
    -   $\eta$ is the **natural parameter**.
    -   $T(X) = [T_1(X), \ldots, T_k(X)]$ is a **sufficient statistic**;
        it is **complete** if the parameter space is an open set in
        $\R^k$.
-   Example: Normal, Gamma, Beta, Binomial (with known $n$), Poisson,
    ...
-   Not an exponential family: $\text{Unif}(\theta, \theta+1)$ (if the
    support of the distribution depends on the parameter, then it is not
    an exponential family).

## Constructing a conjugate prior

-   Suppose $f(x\mid \theta) = h(x)\exp(\theta^Tx - \psi(\theta))$
    (natural parametrization).
-   How to construct a conjugate prior for $f(x \mid \theta)$?
-   We need $\pi(\theta)$ and $\pi(\theta \mid x)$ to be in the same
    family.

::: fragment
$$
\begin{align}
  \pi(\theta \mid x) = \frac{f(x \mid \theta)\pi(\theta)}{\int f(x\mid \theta^{\prime})\pi(\theta^{\prime})d\theta^{\prime}} 
  & = \frac{{\color{green}\pi(\theta)}{\color{magenta}h(x)}\exp(\theta^Tx {\color{green} - \psi(\theta)})}{{\color{magenta}m(x)}}\\
    &\class{fragment}{{} = \exp\left[x^T\theta - \left(\psi(\theta)\right)\right]}         \\[3px]
\end{align}
$$
:::

## Pros and cons

# Noninformative prior

## Noninformative prior

-   When there is no prior information or the prior information is unreliable, should we still use Bayesian techniques?
-   The answer is YES, due to some optimality criteria (we will discuss this in the next lecture).
-   In such cases, the priors should be derived from the sampling distribution, since that is the only information we have.
-   These priors are called **noninformative priors**, or objective/default priors.
-   We will introduce some principles for deriving noninformative priors.

## Laplace's prior

-   For the binomial model, Laplace proposed to use the uniform distribution for $p$.
-   It is called the **principle of insufficient reason**:

:::{.fragment}
>   If we are ignorant of the ways an event can occur, the event will occur equally likely in any way. [^2]
:::

-   It is also called the **equiprobability principle**.

[^2]: <https://mathworld.wolfram.com/PrincipleofInsufficientReason.html>

## Criticisms against Laplace's prior

1.  When the parameter space is not compact, this principle leads to an **improper prior**.
    -   A distribution $\pi$ is improper if $\pi(\theta) \geq 0$ but $\int \pi(\theta)d\theta = \infty$.
    -   However, it is actually possible to work with improper priors, as long as we do not try to interpret them as probability distributions.
    
2.  Laplace's prior is not **invariant under reparametrization**.
    -   Suupose we assign a unifrom prior for $\theta$: $\pi(\theta) \propto 1$.
    -   We consider a reparametrization $\eta = g(\theta)$, where $g$ is one-to-one.
    -   The corresponding prior is 
    $$
    \pi^*(\eta)=\left|\frac{d}{d \eta} g^{-1}(\eta)\right|
    $$
    which is not constant.
    
## Example

-   Suppose $\Theta = [0, 1]$ and $\pi(\theta) = 1$ for $\theta \in \Theta$.
-   Consider the odds $\eta = \frac{\theta}{1-\theta}$, which is one-to-one on $\Theta$.
-   Then $\pi^*(\eta) = \left|\frac{d}{d \eta} g^{-1}(\eta)\right| = \frac{1}{(1+\eta)^2}$, that is, the prior prefers small $\eta$ (corresponding to small $\theta$). 

:::{.fragment}
```{r}
#| echo: false
#| fig-align: center
#| fig-width: 7
#| fig-height: 4.5
plot(0, type = "n", xlim = c(0, 5), ylim = c(0,3), 
     xlab = "parameter", ylab="prior density")
abline(h = 1, col="blue")
curve(1/(1+x)^2, col="red", add = T)
```
:::

## Jeffreys prior

:::{.fragment .nonincremental}
-   Recall that the Fisher information of a pdf/pmf $f(x\mid\theta)$ is given by

$$
I(\theta)=\E_\theta\left[\left(\frac{\partial \log f(X \mid \theta)}{\partial \theta}\right)^2\right] \stackrel{(*)}{=} -\E_\theta\left[\frac{\partial^2 \log f(X \mid \theta)}{\partial \theta^2}\right].
$$
:::


-   The Jeffreys prior distribution is $\pi_J(\theta) \propto \sqrt{I(\theta)}$, 
defined up to a normalization constant when $\pi_J$ is proper.


::: aside
The equality $(*)$ holds under some regularity conditions on $f(x\mid\theta)$.
:::

## Example 
If $X \sim \bin(n, p)$,

:::{.fragment}
$$
\begin{aligned}
f(x \mid p) & =\left(\begin{array}{c}
n \\
x
\end{array}\right) p^x(1-p)^{n-x}, \\
\frac{\partial^2 \log f(x \mid p)}{\partial p^2} & = - \frac{x}{p^2} - \frac{n-x}{(1-p)^2}, \\
I(p) & =n\left[\frac{1}{p}+\frac{1}{1-p}\right]=\frac{n}{p(1-p)} .
\end{aligned}
$$
:::

:::{.fragment}
Therefore, the Jeffreys prior for this model is
$$
\pi_J(p) \propto[p(1-p)]^{-1 / 2}
$$
and is thus proper, since it is a $\text{Beta}(1/2, 1/2)$ distribution.
:::

## Example 
```{r}
#| echo: false
#| output-location: slide
#| fig-align: center
#| fig-cap: "Beta(1,1) (blue) and Beta(1/2, 1/2) (red)"
#| fig-cap-location: top
#| fig-width: 7
plot(0, type = "n", xlim = c(0, 1), ylim = c(0, 4),
     xlab = expression(theta), ylab = expression(pi(theta)))
curve(dbeta(x, 1, 1), col = "blue", add = T)
curve(dbeta(x, 1/2, 1/2), col = "red", add = T)
```

## Multidimensional parameters

-   For $\theta \in \R^k$, the **Fisher information matrix** $I(\theta)$ has the following elements,

:::{.fragment}
$$
I_{i j}(\theta)=-\mathbb{E}_\theta\left[\frac{\partial^2}{\partial \theta_i \partial \theta_j} \log f(x \mid \theta)\right] \quad(i, j=1, \ldots, k).
$$
:::

-   The Jeffreys noninformative prior is then defined by $\pi_J(\theta) \propto \sqrt{\det(I(\theta))}$.

-   If $f(x \mid \theta)$ belongs to an exponential family, $f(x \mid \theta)=h(x) \exp (\theta \cdot x-\psi(\theta))$,
the Fisher information matrix is given by $I(\theta)=\nabla \nabla^t \psi(\theta)$ and
$$
\pi_J(\theta) \propto\left(\prod_{i=1}^k \psi_{i i}^{\prime \prime}(\theta)\right)^{1 / 2}
$$
where $\psi_{i i}^{\prime \prime}(\theta)=\frac{\partial^2}{\partial \theta_i^2} \psi(\theta)$.

## Jeffreys prior

Jeffreys prior has several advantages:

-   It depends only on the sampling model, through the Fisher information.
-   The Fisher information is an indicator of the amount of information brought by the model (or the observation) about $\theta$.
-   It is **invariant under reparametrization**.


## Invariance of Jeffreys prior

-   By definition, $I(\theta) = I(h(\theta))[h^{\prime}(\theta)]^2$ for any one-to-one $h$.
-   Let $\eta = g(\theta)$ and $\theta \sim \pi_J(\theta) \propto \sqrt{I(\theta)}$.
-   Then 
    \begin{align*}
    \pi^*(\eta) & = \pi_J(g^{-1}(\eta))\left|\frac{d}{d \eta} g^{-1}(\eta)\right| 
    = \sqrt{I(g^{-1}(\eta))}\left|\frac{d}{d \eta} g^{-1}(\eta)\right| \\
    & = \sqrt{I(\eta)[h^{\prime}(\eta)]^{-2}}|h^{\prime}(\eta)| \quad \text{(Let $h = g^{-1}$)}\\
    & = \sqrt{I(\eta)} = \pi_J(\eta).
    \end{align*}

## Limitations of Jeffreys prior

-   Jeffreys prior works well for one-parameter models.
-   For multiparameter models however, Jeffreys prior leads to incoherences or even paradoxes.
-   They do not necessarily perform satisfactorily for all inferential purposes, in particular when considering subvectors of interest (see Example 3.5.9 in Bayesian Choice). 


## Example 
-   Consider $X \sim N(\mu, \sigma^2)$ with $\theta = (\mu, \sigma^2)$ unknown. 
-   In this case,
$$
I(\theta)  =\left(\begin{array}{cc}
1 / \sigma^2 & 0 \\
0 & 2 / \sigma^2
\end{array}\right).
$$
-   Thus $\pi_J(\mu, \sigma^2) \propto \sigma^{-2}$.
-   If we assume $\mu$ and $\sigma^2$ are **a priori independent**, then
$$
\pi_J(\mu, \sigma^2) = \pi_J(\mu)\pi_J(\sigma^2) \propto \sigma^{-1}.
$$
-   Theoretically, $\pi_J(\mu,\sigma^2) \propto \sigma^{-1}$ has better convergence properties than $\pi_J(\mu,\sigma^2) \propto \sigma^{-2}$.

## Example

## Reference prior

-   Bernardo (1979) proposed a modification of the Jeffreys approach called *the reference prior approach*.
-   A major difference is that this method distinguishes between **parameters of interest** and **nuisance parameters**.
-   Hence the prior depends on the [sampling model]{.underline} as well as the [inferential problem]{.underline}.
-   The main idea behind the reference approach is to a prior that maximizes the information brought by the data, i.e. the KL divergence between prior and posterior.
-   It has been shown that for one-parameter models, the reference approach gives the Jeffreys prior.


## Matching prior

## Some practical guides for choosing priors

-   With prior information: 
    + Find the MEP 
    + Find a (conjugate) parametric prior that approximates the prior information

-   Without prior information: 
    + Use conjugate priors (if applicable) and hyper-priors 
    + Use noninformative priors, e.g., Jeffreys prior, reference priors, vague priors

-   Finally, perform some sensitivity analysis to access priors' influence
on the result.
