---
title: "Lecture 06: Testing and Model Selection"
author: "Chun-Hao Yang"
footer: "[Home](https://chunhaoy.com/stat-5010/)"
format: 
    revealjs:
        theme: slides.scss
        chalkboard: true
        slide-number: true
        html-math-method: mathjax
        incremental: true
        scrollable: true
execute:
    echo: false
    warning: false
    cache: true
    freeze: auto
---


## Project Proposal

{{< include macro.qmd >}}

:::{.nonincremental}

-   A 10-15 min presentation describing your final project, including
    +   the dataset
    +   the scientific questions
    +   what statistical methods do you think can help you answer the questions
-   Upload your slides to NTU cool by 10/23.
-   This presentation weights 20% of your final grade.


:::

## Introduction

-   In the last lecture, we saw that hierarchical structures allow us to construct models with high flexibility.
-   In practice, there are more than one way to build a hierarchical models, especially when you use latent variables.
-   Is a model with more hierarchy always "better" than one with less hierarchy?
-   What does it mean by saying "a model is better than the other?"
-   You can evaluate models by
    +   how well it fits the data,
    +   how good it is at prediction,
    +   how we can interpret this model,
    +   how robust it is to the presence of outliers, etc.

# Hypothesis Testing

## Hypothesis testing 

-   The goal of a hypothesis test is to decide whether you can reject the null hypothesis $H_0$ and accept the alternative hypothesis $H_1$ or not.
-   Usually, the hypotheses are of the form
    $$
    H_0: \theta \in \Theta_0\quad \text{vs.} \quad H_1: \theta \in \Theta_1 = \Theta_0^c. 
    $$
-   The philosophy is that we first assume $H_0$ to be true, and see how strong the data is against $H_0$. If it is strong enough, we reject $H_0$ and accept $H_1$.

## Likelihood Ratio Test

-   One of the frequentist approaches to hypothesis testing is the **likelihood ratio test** (LRT).
-   Suppose $X_1, \ldots, X_n \iid f(x\mid \theta)$ and $L(\theta \mid x) = \prod_{i=1}^n f(x_i \mid \theta)$ is the likelihood function.
-   The LRT statistic is 
    $$
        \lambda = \frac{\max_{\theta \in \Theta_0} L(\theta \mid x)}{\max_{\theta \in \Theta_0 \cup \Theta_1} L(\theta \mid x)}.
    $$
-   Apparently, $0 \leq \lambda \leq 1$ and small values of $\lambda$ provide evidence [against]{.underline} $H_0$.
-   When should we reject $H_0$?

## Two types of error

-   False rejection (Type I error) and False acceptance (Type II error)
-   The decision space is $\mc{D} = \{\text{accept}, \text{reject}\} = \{0, 1\}$.
-   A typical loss is the 0-1 loss
    $$
    L(\theta, d)= \begin{cases}1 & \text { if } d = \mathbb{I}_{\Theta_0}(\theta) \\ 0 & \text { otherwise }\end{cases}.
    $$
-   The frequentist risk is 
    \begin{align*}
    R(\theta, \delta) & = \E L(\theta, \delta(X)) = \P(\delta(X) = 1 \mid \theta \in \Theta_0) + \P(\delta(X) = 0 \mid \theta \notin \Theta_0)\\
    & = \text{Type I error} + \text{Type II error}
    \end{align*}

-   Under this loss, the Bayesian solution 
    $$
    \varphi^\pi(x)= \begin{cases}1 & \text { if } \P^\pi\left(\theta \in \Theta_1 \mid x\right) > \P^\pi\left(\theta \in \Theta_0 \mid x\right) \\ 0 & \text { otherwise }\end{cases}.
    $$

## Bayesian test

-   A Bayesian test with prior $\pi$ rejects $H_0: \theta \in \Theta_0$ if 
    $$
    \P^\pi\left(\theta \in \Theta_1 \mid x\right) > \P^\pi\left(\theta \in \Theta_0 \mid x\right).
    $$
-   What are the Type I and Type II errors?
-   What is the power of the test?
-   What if we have a point null hypothesis $H_0: \theta = \theta_0$? For continuous parameter, a Bayesian test will [always reject $H_0$]{.underline}.

## Bayes Factor

## p-value (BC 5.3.4)

# Model Disagnostics

## What do we need to check?

-   A 'model' encompasses the **sampling distribution**, the **prior distribution**, any **hierarchical structure**, and issues such as which explanatory variables have been included in a regression.
-   *Sensitivity analysis*: how much do posterior inferences change when other reasonable probability models are used in place of the present model?
-   *External validation*: using the model to make predictions about future data, and then collecting those data and comparing to their predictions.
-   What do you want to predict?




# Model Selection

## Information criterion

## Cross-validation

## Bayes Factor