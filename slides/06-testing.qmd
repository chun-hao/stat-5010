---
title: "Lecture 06: Testing and Model Selection"
author: "Chun-Hao Yang"
footer: "[Home](https://chunhaoy.com/stat-5010/)"
format: 
    revealjs:
        theme: slides.scss
        chalkboard: true
        slide-number: true
        html-math-method: mathjax
        incremental: true
        scrollable: true
execute:
    echo: false
    warning: false
    cache: true
    freeze: auto
---


## Project Proposal

{{< include macro.qmd >}}

:::{.nonincremental}

-   A 10-15 min presentation describing your final project, including
    +   the dataset
    +   the scientific questions
    +   what statistical methods do you think can help you answer the questions
-   Upload your slides to NTU cool by 10/23.
-   This presentation weights 20% of your final grade.


:::

## Introduction

-   In the last lecture, we saw that hierarchical structures allow us to construct models with high flexibility.
-   In practice, there are more than one way to build a hierarchical models, especially when you use latent variables.
-   Is a model with more hierarchy always "better" than one with less hierarchy?
-   What does it mean by saying "a model is better than the other?"
-   You can evaluate models by
    +   how well it fits the data,
    +   how good it is at prediction,
    +   how we can interpret this model,
    +   how robust it is to the presence of outliers, etc.

# Hypothesis Testing

## Hypothesis testing 

-   The goal of a hypothesis test is to decide whether you can reject the null hypothesis $H_0$ and accept the alternative hypothesis $H_1$ or not.
-   Usually, the hypotheses are of the form
    $$
    H_0: \theta \in \Theta_0\quad \text{vs.} \quad H_1: \theta \in \Theta_1 = \Theta_0^c. 
    $$
-   The philosophy is that we first assume $H_0$ to be true, and see how strong the data is against $H_0$. If it is strong enough, we reject $H_0$ and accept $H_1$.

## Likelihood Ratio Test

-   One of the frequentist approaches to hypothesis testing is the **likelihood ratio test** (LRT).
-   Suppose $X_1, \ldots, X_n \iid f(x\mid \theta)$ and $L(\theta \mid x) = \prod_{i=1}^n f(x_i \mid \theta)$ is the likelihood function.
-   The LRT statistic is 
    $$
        \lambda = \frac{\max_{\theta \in \Theta_0} L(\theta \mid x)}{\max_{\theta \in \Theta_0 \cup \Theta_1} L(\theta \mid x)}.
    $$
-   Apparently, $0 \leq \lambda \leq 1$ and small values of $\lambda$ provide evidence [against]{.underline} $H_0$.
-   When should we reject $H_0$?

## Two types of error

-   False rejection (Type I error) and False acceptance (Type II error)
-   The decision space is $\mc{D} = \{\text{accept}, \text{reject}\} = \{0, 1\}$.
-   A typical loss is the 0-1 loss
    $$
    L(\theta, d)= \begin{cases}1 & \text { if } d = \mathbb{I}_{\Theta_0}(\theta) \\ 0 & \text { otherwise }\end{cases}.
    $$
-   The frequentist risk is 
    \begin{align*}
    R(\theta, \delta) & = \E L(\theta, \delta(X)) = \P(\delta(X) = 1 \mid \theta \in \Theta_0) + \P(\delta(X) = 0 \mid \theta \notin \Theta_0)\\
    & = \text{Type I error} + \text{Type II error}
    \end{align*}

-   Under this loss, the Bayesian solution 
    $$
    \varphi^\pi(x)= \begin{cases}1 & \text { if } \P^\pi\left(\theta \in \Theta_1 \mid x\right) > \P^\pi\left(\theta \in \Theta_0 \mid x\right) \\ 0 & \text { otherwise }\end{cases}.
    $$

## Bayesian test

-   A Bayesian test with prior $\pi$ rejects $H_0: \theta \in \Theta_0$ if 
    $$
    \P^\pi\left(\theta \in \Theta_1 \mid x\right) > \P^\pi\left(\theta \in \Theta_0 \mid x\right).
    $$
-   What are the Type I and Type II errors?
-   What is the power of the test?
-   What if we have a point null hypothesis $H_0: \theta = \theta_0$? For continuous parameter, a Bayesian test will [always reject $H_0$]{.underline}.

## Bayes Factor

-   The Bayes factor is the ratio of the marginal likelihoods of the two models
    $$
    \text{BF} = \frac{\int_{\Theta_0} f(x \mid \theta) \pi(\theta) \, d\theta}{\int_{\Theta_1} f(x \mid \theta) \pi(\theta) \, d\theta}.
    $$

## Type I error and Type II error for Bayesian test

-   The Type I error is
    $$
    \alpha = \P^\pi\left(\theta \in \Theta_1 \mid \theta \in \Theta_0\right) = \frac{\int_{\Theta_0} f(x \mid \theta) \pi(\theta) \, d\theta}{\int_{\Theta} f(x \mid \theta) \pi(\theta) \, d\theta}.
    $$
-   The Type II error is
    $$
    \beta = \P^\pi\left(\theta \in \Theta_0 \mid \theta \in \Theta_1\right) = \frac{\int_{\Theta_1} f(x \mid \theta) \pi(\theta) \, d\theta}{\int_{\Theta} f(x \mid \theta) \pi(\theta) \, d\theta}.
    $$
-   The power of the test is $1 - \beta$.
-   The Bayes factor is the ratio of the prior odds to the posterior odds.

## p-value (BC 5.3.4)

## Bayesian t-test
-   Suppose $X_1, \ldots, X_n \iid N(\mu, \sigma^2)$ and $\sigma^2$ is known.
-   The null hypothesis is $H_0: \mu = \mu_0$ and the alternative hypothesis is $H_1: \mu \neq \mu_0$.
-   The likelihood function is
    $$
    L(\mu \mid x) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi}\sigma} \exp\left(-\frac{(x_i - \mu)^2}{2\sigma^2}\right) = \frac{1}{(2\pi\sigma^2)^{n/2}} \exp\left(-\frac{\sum_{i=1}^n (x_i - \mu)^2}{2\sigma^2}\right).
    $$
-   The posterior distribution is
    $$
    \pi(\mu \mid x) \propto \exp\left(-\frac{\sum_{i=1}^n (x_i - \mu)^2}{2\sigma^2}\right) \cdot \mathbb{I}_{\mu \in \Theta_0}(\mu).
    $$
-   The posterior mean is
    $$
    \E(\mu \mid x) = \frac{\sum_{i=1}^n x_i + \mu_0/\sigma^2}{n + 1/\sigma^2}.
    $$
-   The posterior variance is
    $$
    \Var(\mu \mid x) = \frac{\sigma^2}{n + 1/\sigma^2}.
    $$

# Model Disagnostics

## What do we need to check?

-   A 'model' encompasses the **sampling distribution**, the **prior distribution**, any **hierarchical structure**, and issues such as which explanatory variables have been included in a regression.
-   *Sensitivity analysis*: how much do posterior inferences change when other reasonable probability models are used in place of the present model?
-   *External validation*: using the model to make predictions about future data, and then collecting those data and comparing to their predictions.
-   What do you want to predict?




# Model Selection

## Information criterion

-   The **deviance** is defined as
    $$
    D(\theta) = -2 \log f(x \mid \theta).
    $$
-   The **Bayesian information criterion** (BIC) is defined as
    $$
    \text{BIC} = -2 \log f(x \mid \hat{\theta}) + \log n \cdot \text{df},
    $$
    where $\hat{\theta}$ is the MLE and $\text{df}$ is the number of parameters in the model.
    
-   The **Akaike information criterion** (AIC) is defined as
    $$
    \text{AIC} = -2 \log f(x \mid \hat{\theta}) + 2 \cdot \text{df}.
    $$
-   The **deviance information criterion** (DIC) is defined as
    $$
    \text{DIC} = -2 \log f(x \mid \hat{\theta}) + 2 \cdot p_D,
    $$
    where $p_D = \E_{\theta \mid x} \left[D(\theta)\right] - D(\E_{\theta \mid x}[\theta])$ is the effective number of parameters.
    
    
## Model averaging

-   Model averaging is a way to combine the results from multiple models.
-   The idea is to average the posterior distributions of the parameters over all models.
-   The posterior probability of model $M_i$ is
    $$
    \P(M_i \mid x) = \frac{\P(M_i) \P(x \mid M_i)}{\sum_{j=1}^k \P(M_j) \P(x \mid M_j)}.
    $$
-   The posterior distribution of $\theta$ is
    $$
    \P(\theta \mid x) = \sum_{i=1}^k \P(M_i \mid x) \P(\theta \mid x, M_i).
    $$
    
    
    
    
## Cross-validation

-   Cross-validation is a way to evaluate the predictive performance of a model.
-   The idea is to split the data into two parts: a training set and a test set.
-   The model is fit to the training set, and then the predictive performance is evaluated on the test set.
-   The predictive performance is measured by the predictive mean squared error (PMSE)
    $$
    \text{PMSE} = \frac{1}{n} \sum_{i=1}^n \E_{\theta \mid x} \left[(y_i - \hat{y}_i)^2 \mid x_i\right],
    $$
    where $\hat{y}_i$ is the prediction of $y_i$ based on the training set.
    
-   The **leave-one-out cross-validation** (LOO-CV) is defined as
    $$
    \text{LOO-CV} = \frac{1}{n} \sum_{i=1}^n \E_{\theta \mid x} \left[(y_i - \hat{y}_i)^2 \mid x_i\right],
    $$
    where $\hat{y}_i$ is the prediction of $y_i$ based on the training set without $x_i$.
    
    
## Bayes Factor