---
title: "Lecture 06: Testing and Model Selection"
author: "Chun-Hao Yang"
footer: "[Home](https://chunhaoy.com/stat-5010/)"
format: 
    revealjs:
        theme: slides.scss
        chalkboard: true
        slide-number: true
        html-math-method: mathjax
        incremental: true
        scrollable: true
execute:
    echo: false
    warning: false
    cache: true
    freeze: auto
---


## Project Proposal

{{< include macro.qmd >}}

:::{.nonincremental}

-   A 10-15 min presentation describing your final project, including
    +   the dataset
    +   the scientific questions
    +   what statistical methods do you think can help you answer the questions
-   Upload your slides to NTU cool by 10/23.
-   This presentation weights 20% of your final grade.


:::

## Introduction

-   In the last lecture, we saw that hierarchical structures allow us to construct models with high flexibility.
-   In practice, there are more than one way to build a hierarchical models, especially when you use latent variables.
-   Is a model with more hierarchy always "better" than one with less hierarchy?
-   What does it mean by saying "a model is better than the other?"
-   You can evaluate models by
    +   how well it fits the data,
    +   how good it is at prediction,
    +   how we can interpret this model,
    +   how robust it is to the presence of outliers, etc.

# Hypothesis Testing

## Hypothesis testing 

-   The goal of a hypothesis test is to decide whether you can reject the null hypothesis $H_0$ and accept the alternative hypothesis $H_1$ or not.
-   Usually, the hypotheses are of the form
    $$
    H_0: \theta \in \Theta_0\quad \text{vs.} \quad H_1: \theta \in \Theta_1 = \Theta_0^c. 
    $$
-   The philosophy is that we first assume $H_0$ to be true, and see how strong the data is against $H_0$. If it is strong enough, we reject $H_0$ and accept $H_1$.

## Likelihood Ratio Test

-   One of the frequentist approaches to hypothesis testing is the **likelihood ratio test** (LRT).
-   Suppose $X_1, \ldots, X_n \iid f(x\mid \theta)$ and $L(\theta \mid x) = \prod_{i=1}^n f(x_i \mid \theta)$ is the likelihood function.
-   The LRT statistic is 
    $$
        \lambda = \frac{\max_{\theta \in \Theta_0} L(\theta \mid x)}{\max_{\theta \in \Theta_0 \cup \Theta_1} L(\theta \mid x)}.
    $$
-   Apparently, $0 \leq \lambda \leq 1$ and small values of $\lambda$ provide evidence [against]{.underline} $H_0$.
-   When should we reject $H_0$?

## Two types of error

-   False rejection (Type I error) and False acceptance (Type II error)
-   The decision space is $\mc{D} = \{\text{accept}, \text{reject}\} = \{0, 1\}$.
-   A typical loss is the 0-1 loss
    $$
    L(\theta, d)= \begin{cases}1 & \text { if } d = \mathbb{I}_{\Theta_0}(\theta) \\ 0 & \text { otherwise }\end{cases}.
    $$
-   The frequentist risk is 
    \begin{align*}
    R(\theta, \delta) & = \E L(\theta, \delta(X)) = \P(\delta(X) = 1 \mid \theta \in \Theta_0) + \P(\delta(X) = 0 \mid \theta \notin \Theta_0)\\
    & = \text{Type I error} + \text{Type II error}
    \end{align*}

-   Under this loss, the Bayesian solution 
    $$
    \varphi^\pi(x)= \begin{cases}1 & \text { if } \P^\pi\left(\theta \in \Theta_1 \mid x\right) > \P^\pi\left(\theta \in \Theta_0 \mid x\right) \\ 0 & \text { otherwise }\end{cases}.
    $$

## Bayesian test

-   A Bayesian test with prior $\pi$ rejects $H_0: \theta \in \Theta_0$ if 
    $$
    \P^\pi\left(\theta \in \Theta_1 \mid x\right) > \P^\pi\left(\theta \in \Theta_0 \mid x\right).
    $$
-   What are the Type I and Type II errors?
-   What is the power of the test?
-   What if we have a point null hypothesis $H_0: \theta = \theta_0$? For continuous parameter, a Bayesian test will [always reject $H_0$]{.underline}.
-   Is this rejection due to the prior or the observation?

## Bayes Factor

-   The Bayes factor is the ratio of posterior odds to the prior odds
    $$
    B_{01}^\pi(x)=\left.\frac{P\left(\theta \in \Theta_0 \mid x\right)}{P\left(\theta \in \Theta_1 \mid x\right)} \middle/ \frac{\pi\left(\theta \in \Theta_0\right)}{\pi\left(\theta \in \Theta_1\right)}\right..
    $$
-   It is an "objective" answer, since it partly eliminates the
influence of the prior modeling and emphasizes the role of the observations.
-   If the Bayes factor is 1, the observation does not provide information in favor of either hypothesis.
-   If the Bayes factor is greater than 1, the observation favors $H_0$ and hence we accept $H_0$ when the Bayes factor is large.
-   The Bayes factor offers a continuum of evidence, rather than a dichotomous decision.

## Differences between Bayes factor and $p$-value

-   $p$-value is the probability of observing a test statistic at least as extreme as the one observed, given that $H_0$ is true.
-   So small $p$-values provide evidence against $H_0$, but large $p$-values do not provide evidence in favor of $H_0$ (or against $H_1$).
-   In contrast, large Bayes factors provide evidence in favor of $H_0$, and small Bayes factors provide evidence in favor of $H_1$ (or against $H_0$).
-   The $p$-value bases rejection of $H_0$ on **unlikely events that did not occur**.

## Type I and Type II errors for Bayesian test

-   Although it is possible to define Type I and Type II errors for Bayesian tests, the concept is not compatible with Bayesian paradigm.
-   In Bayesian inference, we tend not to give a simple, dichotomous answer to the question of whether a hypothesis is true or false.
-   We compute the posterior probability of the two hypotheses and choose the hypothesis when you think the evidence (e.g., Bayes factor) is strong enough.




## Improper prior for testing

-   For Bayesian estimation, improper priors (e.g., noninformative priors) sometimes give us good estimates, although we are required to check the propriety of the posterior.
-   For testing, it is not recommended to use improper prior for a couple of reasons.
    1.   Improper priors make the Bayes factor undefined.
    2.   The testing setting is not coherent with an absolute lack of information since we need to decide our hypotheses.
-   There are still some solutions proposed to overcome the difficulties related to the
use of improper priors. 
-   Most of them are based on the idea to use part
of the data to transform the priors into proper distributions.
-   For example, partial Bayes factor uses a subset of samples (training sample) to construct a proper prior, and use the rest to compute the Bayes factor.

## Bayesian one sample t-test
-   Suppose $X_1, \ldots, X_n \iid N(\mu, \sigma^2)$ and $\sigma^2$ is known.
-   The null hypothesis is $H_0: \mu = \mu_0$ and the alternative hypothesis is $H_1: \mu \neq \mu_0$.

## Example 


# Model Disagnostics

## What do we need to check?

-   A 'model' encompasses the **sampling distribution**, the **prior distribution**, any **hierarchical structure**, and issues such as which explanatory variables have been included in a regression.
-   *Sensitivity analysis*: how much do posterior inferences change when other reasonable probability models are used in place of the present model?
-   *External validation*: using the model to make predictions about future data, and then collecting those data and comparing to their predictions.
-   What do you want to predict?




# Model Selection

## Model selection

-   Hypothesis testing is a special case of model selection. 
-   A general model selection problem is to choose between two models $M_1$ and $M_2$.
-   For example, we can choose whether we want to use a linear regression model or a quadratic regression model.
-   The choice can be based on the $R^2$ or the adjusted $R^2$, i.e., choose the one with highest adjusted $R^2$.



## Information criterion

-   The **deviance** is defined as
    $$
    D(\theta) = -2 \log f(x \mid \theta).
    $$
-   The **Bayesian information criterion** (BIC) is defined as
    $$
    \text{BIC} = -2 \log f(x \mid \hat{\theta}) + \log n \cdot \text{df},
    $$
    where $\hat{\theta}$ is the MLE and $\text{df}$ is the number of parameters in the model.
    
-   The **Akaike information criterion** (AIC) is defined as
    $$
    \text{AIC} = -2 \log f(x \mid \hat{\theta}) + 2 \cdot \text{df}.
    $$
-   The **deviance information criterion** (DIC) is defined as
    $$
    \text{DIC} = -2 \log f(x \mid \hat{\theta}) + 2 \cdot p_D,
    $$
    where $p_D = \E_{\theta \mid x} \left[D(\theta)\right] - D(\E_{\theta \mid x}[\theta])$ is the effective number of parameters.
    
    
    
    
    
    
## Cross-validation

-   Cross-validation is a way to evaluate the predictive performance of a model.
-   The idea is to split the data into two parts: a training set and a test set.
-   The model is fit to the training set, and then the predictive performance is evaluated on the test set.
-   The predictive performance is measured by the predictive mean squared error (PMSE)
    $$
    \text{PMSE} = \frac{1}{n} \sum_{i=1}^n \E_{\theta \mid x} \left[(y_i - \hat{y}_i)^2 \mid x_i\right],
    $$
    where $\hat{y}_i$ is the prediction of $y_i$ based on the training set.
    
-   The **leave-one-out cross-validation** (LOO-CV) is defined as
    $$
    \text{LOO-CV} = \frac{1}{n} \sum_{i=1}^n \E_{\theta \mid x} \left[(y_i - \hat{y}_i)^2 \mid x_i\right],
    $$
    where $\hat{y}_i$ is the prediction of $y_i$ based on the training set without $x_i$.
    
    
## Bayes Factor

-   

## Model averaging

-   Model averaging is a way to combine the results from multiple models.
-   The idea is to average the posterior distributions of the parameters over all models.
-   The posterior probability of model $M_i$ is
    $$
    \P(M_i \mid x) = \frac{\P(M_i) \P(x \mid M_i)}{\sum_{j=1}^k \P(M_j) \P(x \mid M_j)}.
    $$
-   The posterior distribution of $\theta$ is
    $$
    \P(\theta \mid x) = \sum_{i=1}^k \P(M_i \mid x) \P(\theta \mid x, M_i).
    $$