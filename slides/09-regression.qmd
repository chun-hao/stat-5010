---
title: "Lecture 09: Bayesian Regression"
author: "Chun-Hao Yang"
footer: "[Home](https://chunhaoy.com/stat-5010/)"
format: 
    revealjs:
        theme: slides.scss
        chalkboard: true
        slide-number: true
        html-math-method: mathjax
        incremental: true
        scrollable: true
        include-in-header: 
            - text: 
                <link href='https://fonts.googleapis.com/css?family=Fira Code' rel='stylesheet'>
execute:
    echo: false
    warning: false
    cache: true
    freeze: true
---

# Classical Linear regression

{{< include macro.qmd >}}

## Linear regression

-   Suppose we have samples $\{x_i, y_i\}_{i=1}^n$ where $y_i \in \R$ and $x_i \in \R^{p-1}$. We want to model the relationship between $x_i$ and $y_i$.
-   The classical linear regression model is $Y = X^T\beta$. 
-   We can fit the model by minimizing the ordinary least-square (OLS) $\sum_{i=1}^n (y_i - x_i^T\beta)^2$.
-   The solution is $\hat{\beta} = (X^TX)^{-1}X^TY$, where $Y = [y_1,\ldots, y_n]^T$ and 
    $$
    X = \left[\begin{array}{cc}
    1 & x_1^T \\
    \vdots & \vdots \\
    1 & x_n^T
    \end{array}
    \right]_{n\times p}.
    $$
-   If we further assume $Y = X^T\beta + \varepsilon$, $\varepsilon \sim N(0, \sigma^2)$, then the OLS solution $\hat{\beta} = (X^TX)^{-1}X^TY$ is the MLE of $\beta$.

## Normal linear model

-   The simple linear regression model is a **normal linear model**
    $$
    Y \mid X, \beta, \sigma^2 \sim N(X\beta, \sigma^2I_n)
    $$
    where $I_n$ is the $n \times n$ identity matrix, $X \in \R^{n \times p}$, and $\beta \in \R^p$.
-   Both $\beta$ and $\sigma^2$ are unknown parameters.
-   In practice, it is very likely that the normality assumption is violated. In such cases, we can apply some transformations to $Y$, e.g., Box-Cox transformation, to make it normal.
-   In the next lecture, we will discuss **generalized linear models (GLM)**, i.e.,
    $$
    Y \mid X, \beta \sim F_{\eta}
    $$
    where $F_\eta$ is an exponential family and $\eta = X^T\beta$.

## Standard noninformative prior

-   A convenient noninformative prior is the uniform prior on $(\beta, \log \sigma)$ or, equivalently,
$$
\pi\left(\beta, \sigma^2 \mid X\right) \propto \sigma^{-2}
$$
-   This prior is useful when there are many samples and only a few parameters, i.e., $n \gg p$.

## Posterior distributions

-   Similar to the normal model with unknown mean and variance, the joint posterior can be factorized: $\pi(\beta, \sigma^2 \mid X, Y) = \pi(\beta \mid \sigma^2, X, Y)\pi(\sigma^2 \mid X, Y)$
-   **Conditional posterior distribution of $\beta$, given $\sigma$**: 
    $$
    \beta \mid \sigma^2, y \sim N\left(\hat{\beta}, V_\beta \sigma^2\right)
    $$
    where $\hat{\beta} = (X^TX)^{-1}X^TY$ and $V_\beta = (X^TX)^{-1}$.
-   **Marginal posterior distribution of $\sigma^2$**:
    $$
    \sigma^2 \mid y \sim \text{Inv}-\chi^2\left(n-p, s^2\right)
    $$
    where $s^2 = \frac{1}{n-p}(Y - X\hat{\beta})^T(Y - X\hat{\beta})$.
-   $\text{Inv}-\chi^2(\nu, s)$ is the scaled inverse $\chi^2$ distribution.
-   Hence the posterior means of $\beta$ and $\sigma^2$ are the same as the OLS estimates.

## Posterior predictive distribution


-   Suppose we observe a new sample $X^*$ and we want to predict its corresponding  response $Y^*$.
-   The **posterior predictive distribution** is 
    $$
    p(Y^* \mid X^*, X, Y) = \int p(Y^* \mid \beta, \sigma^2, X^*) \pi(\beta, \sigma^2 \mid X, Y) \; d\beta\; d\sigma^2.
    $$
-   For a normal linear model with the noninformative prior $\pi(\beta, \sigma^2) \propto \sigma^{-2}$, we can compute the analytic form of the predictive distribution.
-   However, there is no need to do so, since once we have the posterior samples of $\beta$ and $\sigma^2$, we can easily draw samples from the posterior predictive distribution.
-   That is, with $(\beta^{(i)}, \sigma^{2,{(i)}}) \iid \pi(\beta, \sigma^2 \mid X, Y)$, we can draw 
    $$
    Y^{*(i)} \mid X^*, X, Y \sim N(X^{*T}\beta^{(i)}, \sigma^{2,(i)}).
    $$



## Computation

There are many `R` packages for fitting Bayesian regression models:

:::{.nonincremental}

-   [`rstanarm`](https://mc-stan.org/rstanarm/index.html): pre-compiled Stan regression models; easier to use; faster; less flexible
-   [`brms`](https://paul-buerkner.github.io/brms/): compile models on the fly; slower; more flexible
-   [`bayesplot`](http://mc-stan.org/bayesplot/index.html): visualization of Bayesian models
    +   MCMC diagnostics (using functions `mcmc_*`)
    +   Posterior prediction checking (PPC) (using functions `ppc_*`)
    +   Posterior prediction distribution (PPD) (using functions `ppd_*`)
-   [`loo`](https://mc-stan.org/loo/): efficient approximate leave-one-out cross-validation for fitted Bayesian models
-   [`shinystan`](https://mc-stan.org/shinystan/index.html): Interactive diagnostic tools for assessing Bayesian models

:::

## Example: Car Price Prediction[^data] 

```{r}
#| label: model_fit
#| echo: true
#| results: hide
library(rstanarm)
library(bayesplot)
library(ggplot2)

data <- read.csv("dataset/car_price/CarPrice_Assignment.csv", 
                 header = TRUE)
data$price <- data$price/1000

# fit a linear regression model
fit <- stan_glm(price ~ highwaympg + citympg + horsepower, 
               data = data, 
               family = gaussian(),
               prior = NULL,
               prior_intercept = NULL,
               prior_aux = NULL,
               chains = 4, iter = 2000)

```

[^data]: <https://www.kaggle.com/datasets/hellbuoy/car-price-prediction>

## Summary of the fitted model

```{r}
#| label: model_summary
#| echo: true
summary(fit)

```

## Prior Summary

```{r}
#| label: prior_summary
#| echo: true
prior_summary(fit)
```

## MCMC Diagnostics

```{r}
#| label: mcmc_trace
#| echo: true
color_scheme_set("brightblue")
mcmc_trace(fit, pars = c("(Intercept)", "highwaympg", 
                         "citympg", "horsepower"))
```

## MCMC Diagnostic

```{r}
#| label: mcmc_posterior
#| echo: true
plot_title <- ggtitle("Posterior distributions",
                      "with medians and 80% intervals")
mcmc_areas(fit, pars = c("highwaympg", "citympg"),
           prob = 0.8) + plot_title
```

## Posterior Prediction Checks (In-sample checking)

Compare observed data to draws from the posterior or prior predictive distribution.

```{r}
#| label: ppc_density
#| echo: true
#| fig-align: center
#| output-location: slide

# predicted vs. observed
Y_rep <- posterior_predict(fit)

ppc_dens_overlay(data$price, Y_rep[1:10,], size = 1) + 
  xlab("Price (in thousands)") +
  ylab("Density") +
  ggtitle("Posterior predictive distribution for observed samples")
```

This figure shows that this model is not a good fit for the data.


## Posterior Prediction Distributions (Out-of-sample prediction)

Compute the posterior predictive distribution for a new sample.

```{r}
#| label: ppd_intervals
#| echo: true
#| fig-align: center
#| output-location: slide
X_new <- data.frame(highwaympg = 40:50, citympg = 40, horsepower = 200)
Y_new_pred <- posterior_predict(fit, newdata = X_new) 

ppd_intervals(Y_new_pred[1:500,], prob = 0.75, linewidth = 2) + 
  ylab("Price (in thousands)") +
  ggtitle("Posterior predictive intervals for new samples") 
```

## Interactive tool: `shinystan`

```{r}
#| label: shinystan
#| echo: true
#| eval: false
library(shinystan)
launch_shinystan(fit)
```
![](images/lec09/shinystan.png){fig-align="center"}

# Regularized regression

## Regularized linear regression

-   The OLS solution $\hat{\beta} = (X^TX)^{-1}X^TY$ is obtained by minimizing the residual sum of squares (RSS), i.e., $\arg\min_{\beta\in\R^p}  \|Y - X\beta\|^2$.
-   In some cases, the matrix $X^TX$ is ill-conditioned, i.e., the inverse $(X^TX)^{-1}$ is numerically unstable.
-   Also when $p > n$, the matrix $X^TX$ is singular and the OLS solution does not exist.
-   We can consider the following penalized least-square problem:
    $$
    \arg\min_{\beta\in\R^p}  \|Y - X\beta\|^2 + \lambda \cdot\text{Penalty}(\beta) \quad \text{for some } \lambda > 0.
    $$
-   There are three commonly used penalties:
    -   Ridge regression: $\text{Penalty}(\beta) = \|\beta\|^2 = \sum_{j=1}^p \beta_j^2$
    -   LASSO: $\text{Penalty}(\beta) = \|\beta\|_1 = \sum_{j=1}^p |\beta_j|$
    -   Elastic net: $\text{Penalty}(\beta) = \alpha \|\beta\|^2 + (1-\alpha) \|\beta\|_1$ for some $\alpha \in [0, 1]$.
    
## Bayesian interpretation of regularization

-   Any penalized likelihood estimator has a Bayesian interpretation, i.e., it is the posterior mode under the prior $\pi(\beta) \propto \exp(-\lambda\cdot\text{Penalty}(\beta))$.
-   For example, 
    +   Ridge regression: normal prior $\beta_j \sim N(0, \lambda^{-1})$
    +   LASSO: Laplace prior $\beta_j \sim \text{Laplace}(0, \lambda^{-1})$
    +   Elastic net: $\pi(\boldsymbol{\beta}) \propto \exp \left\{-\lambda_1\|\boldsymbol{\beta}\|_1-\lambda_2\|\boldsymbol{\beta}\|_2^2\right\}$ 
-   Usually, the tuning parameter $\lambda$ is chosen by cross-validation.
-   Under the Bayesian framework, we can
    +   put a prior on $\lambda$ (fully Bayesian)
    +   put a hierarchical prior on $\lambda$ (hierarchical Bayes)
    +   estimate it from the data (empirical Bayes)
    
    
## Bayesian LASSO

Model (this is how `stan_glm` implements LASSO):
\begin{align*}
Y \mid X, \beta, \sigma^2 & \sim N(X\beta, \sigma^2 I_n) \\
\beta_j & \iid \text{Laplace}(0, \lambda^{-1}) \\
\lambda & \sim \chi^2_{\nu}
\end{align*}
where $\nu$ is the degrees of freedom (the default is $\nu = 1$).

## Bayesian LASSO

The following code finds the **mode of the posterior distribution** using the `optimizing` algorithm. Hence you will see exact zeros.

```{r}
#| label: bayesian_lasso
#| echo: true
#| message: false
#| results: hide
fit_lasso <- stan_glm(price ~ highwaympg + citympg + horsepower + peakrpm, 
                      data = data, 
                      prior = lasso(df = 1, location = 0, 
                                    scale = 1, autoscale = TRUE),
                      prior_intercept = normal(location = 0, scale = 1, 
                                               autoscale = TRUE),
                      prior_aux = exponential(rate = 1, autoscale = TRUE),
                      algorithm = "optimizing",
                      iter = 5000, seed = 123)
```

## Summary of the fitted model

```{r}
#| label: bayesian_lasso_summary
summary(fit_lasso)
```
Although we use `optimizing` is this example, it is recommended to use `sampling` for inference.


# High-dimensional linear regression

## High-dimensional linear regression

-   In some applications, we might have more predictors than samples, i.e., $p \gg n$.
-   In such cases, the sparsity assumption is often used, i.e., only a few predictors are relevant.
-   LASSO or Bayesian Lasso can be used to select the relevant predictors.
-   However LASSO or Bayesian LASSO have some problems:
    -   it does not handle correlated variables well
    -   it shrinks all coefficients with the same intensity 
    -   it often overshrinks large coefficients.
-   We will now introduce two priors for high-dimensional sparse linear regression that address these problems.


## Spike-and-slab prior

-   The spike-and-slab prior[^SS_prior] is
    $$
    \beta_j \mid w, \tau^2 \sim (1 - w)\delta_0 + w \cdot N(0, \tau^2).
    $$
-   Equivalently, 
    \begin{align*}
    \beta_j \mid \gamma_j = 0 & \sim \delta_0 \\
    \beta_j \mid \gamma_j = 1 & \sim N(0, \tau^2)\\
    \gamma_j & \iid \text{Ber}(w)
    \end{align*}
-   $w$ is the parameter that controls the sparsity, and $\gamma_j$ is the indicator of the $j$-th predictor being relevant.
-   This prior is considered the ``gold standard'' for sparse regression.
-   However, it is computationally expensive to update the discrete variables $\gamma_j$.
    
[^SS_prior]: Mitchell, T. J., & Beauchamp, J. J. (1988). *Bayesian variable selection in linear regression*. Journal of the American Statistical Association, 83(404), 1023-1032.    
    
## Horseshoe prior

-   The horseshoe prior[^HS_prior] is 
    \begin{align*}
    \beta_j \mid \lambda_j, \tau & \sim N\left(0, \tau^2 \lambda_j^2\right) \\
    \lambda_j & \sim C^{+}(0,1), \quad j=1, \ldots, p
    \end{align*}
    where $C^{+}(0, 1)$ is the half-Cauchy distribution.
-   This is an example of global-local shrinkage prior, where $\lambda_j$ is the local shrinkage parameter and $\tau$ is the global shrinkage parameter.


[^HS_prior]: Carvalho, C. M., Polson, N. G., & Scott, J. G. (2010). *The horseshoe estimator for sparse signals*. Biometrika, 97(2), 465-480.

## Available priors for `stan_lm`

:::{.nonincremental}
-   Student-$t$ family: cauchy, $t$, normal
-   Hierarchical shrinkage family: horseshoe, horseshoe+
-   Laplace family: laplace, lasso
-   R2 prior: This prior hinges on prior beliefs about the location of $R^2$, the proportion of variance in the outcome attributable to the predictors.

:::



## Issues in linear regression




